{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-awsim-labs","title":"Welcome to AWSIM Labs","text":"<p>AWSIM Labs is currently being developed under the Autoware Labs initiative. Main purpose of this fork is to provide faster implementation of features needed by the users of the AWSIM while also ensuring a high-performance simulation environment for the Autoware.</p> <p>This is a fork of TIER IV's AWSIM.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Many predefined components included (Vehicle dynamic models, Sensor models, Environment configuration, ROS2 communication, etc)</li> <li>Support for Ubuntu 22.04 and windows10/11</li> <li>ROS2 native communication (humble)</li> <li>Open sourced</li> <li>Made with Unity</li> </ul>"},{"location":"#feature-differences-from-the-tier-ivawsim","title":"Feature differences from the TIER IV/AWSIM","text":"<p>See Feature differences from the TIER IV/AWSIM in the Autoware Documentation.</p>"},{"location":"#try-the-simulation-demo-yourself","title":"Try the simulation demo yourself!","text":"<p>Run the AWSIM Labs demo with Autoware following the instructions in the Quick start demo.</p>"},{"location":"Components/Clock/ClockPublisher/","title":"Clock Publisher","text":""},{"location":"Components/Clock/ClockPublisher/#introduction","title":"Introduction","text":"<p><code>ClockPublisher</code> allows the publication of the simulation time from the clock operating within AWSIM. The current time is retrived from a <code>TimeSource</code> object via the <code>SimulatorROS2Node</code>. The AWSIM provides convenient method for selecting the appropriate time source type as well as the flexibility to implement custom <code>TimeSources</code> tailored to specific user requirements.</p>"},{"location":"Components/Clock/ClockPublisher/#setup","title":"Setup","text":"<p>To enable the publication of the current time during simulation execution, <code>ClockPublisher</code> must be included as a component within the scene. Moreover, to allow the <code>TimeSource</code> to be set or changed, the <code>TimeSourceSelector</code> object must also be present in the active scene.</p> <p></p>"},{"location":"Components/Clock/ClockPublisher/#selecting-time-source","title":"Selecting Time Source","text":"<p>The desired <code>TimeSource</code> can be selected in two ways:</p> <ul> <li>Inspector Selection: <code>TimeSource</code> type can be conveniently choosen directly from the editor interface.</li> </ul> <p></p> <ul> <li>JSON Configuration File: Alternatively, the <code>TimeSource</code> type can be specified in the JSON configuration file via the TimeSource field. The supported values for this field can be found in the list of available time sources in the \"String Value for JSON Config\" column.</li> </ul> <p></p>"},{"location":"Components/Clock/ClockPublisher/#list-of-time-sources","title":"List of Time Sources","text":"Type String Value for JSON Config Description UNITY unity based on the time of the Unity Engine SS2 ss2 driven by an external source, used by the scenario simulator v2 DOTNET_SYSTEM system based on system time, starting with time since UNIX epoch, progressing according to simulation timescale DOTNET_SIMULATION simulation based on system time, starting with zero value, progressing according to simulation timescale ROS2 ros2 based on ROS2 time (system time by default)"},{"location":"Components/Clock/ClockPublisher/#architecture","title":"Architecture","text":"<p>The <code>ClockPublisher</code> operates within a dedicated thread called the 'Clock' thread. This design choice offers significant advantages by freeing the publishing process from the constraints imposed by fixed update limits. As a result, <code>ClockPublisher</code> is able to consistently publish time at a high rate, ensuring stability and accuracy.</p>"},{"location":"Components/Clock/ClockPublisher/#accessing-time-source","title":"Accessing Time Source","text":"<p>Running the clock publisher in a dedicated thread introduced the challenge of accessing shared resources by different threads. In our case, the Main Thread and Clock Thread compete for <code>TimeSoruce</code> resources. The diagram below illustrates this concurrent behaviour, with two distinct threads vying for access to the <code>TimeSource</code>:</p> <ul> <li>Main Thread: included publishing message by sensors (on the diagram blueish region labeled sensor loop),</li> <li>Clock Thread: included clock publisher (on the diagram blueish region labeled clock loop).</li> </ul> <p>Given multiple sensors, each with its own publishing frequency, alongside a clock running at 100Hz, there is a notable competition for <code>TimeSource</code> resources. In such cases, it becomes imperative for the <code>TimeSource</code> class to be thread-safe.</p> <p></p>"},{"location":"Components/Clock/ClockPublisher/#thread-safe-time-source","title":"Thread-Safe Time Source","text":"<p>The <code>TimeSource</code> synchronization mechanism employs a mutex to lock the necessary resource for the current thread. The sequence of actions undertaken each time the <code>GetTime()</code> method is called involves:</p> <ul> <li>acquiring the lock,</li> <li>getting the current time, e.g. system time since epoch, (this may be different for each type of time source),</li> <li>obtaining the current simulation time-scale,</li> <li>calculating the delta time since previous call, influenced by the time scale,</li> <li>returning the current time,</li> <li>releasing the lock.</li> </ul> <p></p>"},{"location":"Components/Clock/ClockPublisher/#extensions","title":"Extensions","text":"<p>There are two additional classes used to synchronise the UnityEngine <code>TimeAsDouble</code> and <code>TimeScale</code> values between threads:</p> <ul> <li><code>TimeScaleProvider</code>: facilitates the synchronisation of the simulation time scale value across threads,</li> <li><code>TimeAsDoubleProvider</code>: provides access to the UnityEngine <code>TimeAsDouble</code> to the threads other than the main thread.</li> </ul>"},{"location":"Components/Environment/AWSIMEnvironment/","title":"AWSIM Environment","text":""},{"location":"Components/Environment/AWSIMEnvironment/#awsim-environment","title":"AWSIM Environment","text":""},{"location":"Components/Environment/AWSIMEnvironment/#introduction","title":"Introduction","text":"<p><code>Environment</code> is an object that contains all the elements visible on the scene along with components that affect how they are rendered. It contains several objects aggregating static environment objects in terms of their type. Moreover, it contains elements responsible for controlling random traffic.</p> <p></p> <p>Own Environment prefab</p> <p>If you would like to develop your own prefab <code>Environment</code> for AWSIM, we encourage you to read this tutorial.</p> <p>AutowareSimulation scene</p> <p>If you would like to see how <code>Environment</code> with random traffic works or run some tests, we encourage you to familiarize yourself with the <code>AutowareSimulation</code> scene described in this section.</p> <p>Prefab <code>Environment</code> is also used to create a point cloud (<code>*.pcd</code> file) needed to locate the <code>EgoVehicle</code> in the simulated AWSIM scene. The point cloud is created using the <code>RGL</code> plugin and then used in Autoware. We encourage you to familiarize yourself with an example scene of creating a point cloud - described here.</p> <p>Create PointCloud (*.pcd file)</p> <p>If you would like to learn how to create a point cloud in AWSIM using <code>Environment</code> prefab, we encourage you to read this tutorial.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#architecture","title":"Architecture","text":"<p>The architecture of an <code>Environment</code> - with dependencies between components - is presented on the following diagram.</p> <p></p>"},{"location":"Components/Environment/AWSIMEnvironment/#prefabs","title":"Prefabs","text":"<p>Prefabs can be found under the following path:</p> Name Description Path Nishishinjuku Only stationary visual elements, no traffic <code>Assets/AWSIM/Prefabs/Environments/Nishishinjuku.prefab</code> Nishishinjuku RandomTraffic Stationary visual elements along with random traffic <code>Assets/AWSIM/Prefabs/Environments/Nishishinjuku RandomTraffic.prefab</code> Nishishinjuku Traffic Stationary visual elements along with non-random traffic <code>Assets/AWSIM/Prefabs/Environments/Nishishinjuku Traffic.prefab</code> <p>Environment prefab</p> <p>Due to the similarity of the above prefabs, this section focuses on prefab <code>Nishishinjuku RandomTraffic</code>. The exact differences between <code>Nishishinjuku RandomTraffic</code> and <code>Nishishinjuku Traffic</code> will be described in the future.</p> <p>Environment name</p> <p>In order to standardize the documentation, the name <code>Environment</code> will be used in this section as the equivalent of the prefab named <code>Nishishinjuku RandomTraffic</code>.</p> <p><code>Nishishinjuku RandomTraffic</code> prefab has the following content:</p> <p></p> <p>As you can see it contains:</p> <ul> <li><code>SJK*</code> objects - which are aggregators for visual models.</li> <li><code>RandomTrafficSimulator</code>, <code>TrafficIntersections</code>, <code>TrafficLanes</code>, <code>StopLines</code> - which are responsible for random traffic of <code>NPCVehicles</code>.</li> <li><code>NPCPedestrians</code> - which is an aggregator of <code>NPCPedestrian</code> prefabs added to the scene.</li> <li><code>Volume</code>, <code>Directional Light</code> - which are components that affect the appearance of objects on the scene.</li> </ul> <p>All of these objects are described below in this section.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#visual-elements","title":"Visual elements","text":"<p><code>Nishishinjuku RandomTraffic</code> prefab contains many visual elements which are described here.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p><code>Nishishinjuku RandomTraffic</code> prefab is added to the <code>Environment</code> object - between which there is rotation about the <code>Oy</code> axis by 90 degrees. This rotation is added because of the differences in coordinate alignments between the <code>Nishishinjuku RandomTraffic</code> prefab objects (which have been modeled as <code>*.fbx</code> files) and the specifics of the GridZone definition (more on this is described here).</p> <p>Object <code>Environment</code> is added to <code>AutowareSimulation</code> which is added directly to the main parent of the scene - there are no transformations between these objects.</p> <p></p>"},{"location":"Components/Environment/AWSIMEnvironment/#components","title":"Components","text":"<p><code>Nishishinjuku RandomTraffic</code> (<code>Environment</code>) prefab contains only one component:</p> <ul> <li>Environment (script) - which is important for communicating with Autoware and loading elements from Lanelet2. Because it allows to define the location of the environment in relation to the world.</li> </ul>"},{"location":"Components/Environment/AWSIMEnvironment/#layers","title":"Layers","text":"<p>In order to enable the movement of vehicles around the environment, additional layers have been added to the project: <code>Ground</code> and <code>Vehicle</code>.</p> <p></p> <p>All objects that are acting as a ground for <code>NPCVehicles</code> and <code>EgoVehicle</code> to move on have been added to <code>Ground</code> layer - they cannot pass through each other and should collide for the physics engine to calculate their interactions.</p> <p></p> <p>For this purpose, <code>NPCVehicles</code> and <code>EgoVehicle</code> have been added to the <code>Vehicle</code> layer.</p> <p></p> <p>In the project physics settings, it is ensured that collisions between objects in the <code>Vehicle</code> layer are disabled (this applies to <code>EgoVehicle</code> and <code>NPCVehicles</code> - they do not collide with each other):</p> <p></p>"},{"location":"Components/Environment/AWSIMEnvironment/#traffic-components","title":"Traffic Components","text":"<p>Due to the specificity of the use of <code>RandomTrafficSimulator</code>, <code>TrafficIntersections</code>, <code>TrafficLanes</code>, <code>StopLines</code> objects, they have been described in a separate section Traffic Components - where all the elements necessary in simulated random traffic are presented.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#visual-elements-sjk","title":"Visual Elements (SJK)","text":"<p>The visuals elements have been loaded and organized using the <code>*.fbx</code> files which can be found under the path:</p> <pre><code>Assets/AWSIM/Externals/Nishishinjuku/Nishishinjuku_optimized/Models/*\n</code></pre> <p><code>Environment</code> prefab contains several objects aggregating stationary visual elements of space by their category:</p> <ul> <li> <p><code>SJK01_P01</code> - contains all objects constituting the ground of the environment, these are roads and green fields - each of them contains a <code>MeshColliders</code> and layer set as <code>Ground</code> to ensure collisions with <code>NPCVehicles</code> and <code>EgoVehicle</code>.</p> <p></p> </li> </ul> <ul> <li> <p><code>SJK01_P02</code> - contains all road surface markings on roads added to the environment. The objects of this group do not have <code>MeshColliders</code> and their layer is <code>Default</code>.</p> <p></p> </li> </ul> <ul> <li> <p><code>SJK01_P03</code> - contains all the vertical poles added to the environment, such as lamp posts, road signs and traffic light poles. Only <code>TrafficLight</code> poles and <code>PedestrianLight</code> poles have <code>MeshCollider</code> added. The layer for all objects is <code>Default</code>.</p> <p></p> </li> </ul> <ul> <li> <p><code>SJK01_P04</code> - contains all barriers added to the environment, such as barriers by sidewalks. The objects of this group do not have <code>MeshColliders</code> and their layer is <code>Default</code>.</p> <p></p> </li> </ul> <ul> <li> <p><code>SJK01_P05</code> - contains all greenery added to the environment, such as trees, shrubs, fragments of greenery next to buildings. The objects of this group do not have <code>MeshColliders</code> and their layer is <code>Default</code>.</p> <p></p> </li> </ul> <ul> <li> <p><code>SJK01_P06</code> - contains all buildings added to the environment. Objects of this category also have a <code>MeshCollider</code> added, but their layer is <code>Default</code>.</p> <p></p> </li> </ul> <p>Scene Manager</p> <p>For models (visual elements) added to the prefab to work properly with the <code>LidarSensor</code> sensor using <code>RGL</code>, make sure that the <code>SceneManager</code> component is added to the scene - more about it is described in this section.</p> <p>In the scene containing <code>Nishishinjuku RandomTraffic</code>prefab Scene Manager (script) is added as a component to the <code>AutowareSimulation</code> object containing the <code>Environment</code>.</p> <p></p>"},{"location":"Components/Environment/AWSIMEnvironment/#trafficlights","title":"TrafficLights","text":"<p><code>TrafficLights</code> are a stationary visual element belonging to the <code>SJK01_P03</code> group. The lights are divided into two types, the classic <code>TrafficLights</code> used by vehicles at intersections and the <code>PedestrianLights</code> found at crosswalks.</p> <p></p> <p>Classic traffic lights are aggregated at object <code>TrafficLightA01_Root01_ALL_GP01</code></p> <p></p> <p>while lights used by pedestrians are aggregated at object <code>TrafficLightB01_Root01_All_GP01</code>.</p> <p></p> <p><code>TrafficLights</code> and <code>PedestrianLights</code> are developed using models available in the form of <code>*.fbx</code> files, which can be found under the following path: <code>Assets/AWSIM/Externals/Nishishinjuku/Nishishinjuku_opimized/Models/TrafficLights/Models/*</code></p>"},{"location":"Components/Environment/AWSIMEnvironment/#classic-trafficlights","title":"Classic TrafficLights","text":"<p><code>TrafficLights</code> lights, outside their housing, always contain 3 signaling light sources of different colors  - from left to right: green, yellow, red. Optionally, they can have additional sources of signaling the ability to drive in a specific direction in the form of one or three signaling arrows.</p> <p></p> <p>In the environment there are many classic lights with different signaling configurations. However, each contains:</p> <ul> <li>Transform - defines the position of the lights in the environment relative to the main parent of the group (<code>SJK01_P03</code>).</li> <li>Mesh Filter - contains a reference to the <code>Mesh</code> of the object.</li> <li>Mesh Renderer - enables the rendering of <code>Mesh</code>, including its geometry, textures, and materials, giving it a visual appearance in the scene.</li> <li>Mesh Collider - allows an object to have collision detection based on <code>Mesh</code>.</li> <li>Traffic Light (script) - provides an interface to control signaling by changing the emission of materials. This script is used for simulated traffic, so it is described.</li> </ul>"},{"location":"Components/Environment/AWSIMEnvironment/#materials","title":"Materials","text":"<p>An important element that is configured in the <code>TrafficLights</code> object are the materials in the <code>Mesh Renderer</code> component. Material with index 0 always applies to the housing of the lights. Subsequent elements 1-6 correspond to successive slots of light sources (round luminous objects) - starting from the upper left corner of the object in the right direction, to the bottom and back to the left corner. These indexes are used in script Traffic Light (script) - described here.</p> <p></p> <p>Materials for lighting slots that are assigned in <code>Mesh Renderer</code> can be found in the following path: <code>Assets/AWSIM/Externals/Nishishinjuku/Nishishinjuku_opimized/Models/TrafficLights/Materials/*</code></p>"},{"location":"Components/Environment/AWSIMEnvironment/#pedestrianlights","title":"PedestrianLights","text":"<p><code>PedestrianLights</code> lights, outside their housing, always contain 2 signaling light sources of different colors - red on top and green on the bottom.</p> <p></p> <p>In the environment there are many pedestrian lights - they have the same components as classic <code>TrafficLights</code>, but the main difference is the configuration of their materials.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#materials_1","title":"Materials","text":"<p>An important element that is configured in the <code>PedestrianLights</code> object are the materials in the <code>Mesh Renderer</code> component. Material with index 0 always applies to the housing of the lights. Subsequent elements 1-2 correspond to successive slots of light sources (round luminous objects) - starting from top to bottom. These indexes are used in script Traffic Light (script) - described here.</p> <p> Materials for lighting slots that are assigned in <code>Mesh Renderer</code> can be found in the following path: <code>Assets/AWSIM/Externals/Nishishinjuku/Nishishinjuku_opimized/Models/TrafficLights/Materials/*</code></p>"},{"location":"Components/Environment/AWSIMEnvironment/#volume","title":"Volume","text":"<p><code>Volume</code> is GameObject with Volume component which is used in the High Definition Render Pipeline (HDRP). It defines a set of scene settings and properties. It can be either global, affecting the entire scene, or local, influencing specific areas within the scene. Volumes are used to interpolate between different property values based on the Camera's position, allowing for dynamic changes to environment settings such as fog color, density, and other visual effects.</p> <p>In case of prefab <code>Nishishinjuku RandomTraffic</code> volume works in global mode and has loaded Volume profile. This volume profile has a structure that overrides the default properties of Volume related to the following components: Fog, Shadows, Ambient Occlusion, Visual Environment, HDRI Sky. It can be found in the following path: <code>Assets/AWSIM/Prefabs/Environments/Nishishinjuku/Volume Profile.asset</code></p>"},{"location":"Components/Environment/AWSIMEnvironment/#directional-light","title":"Directional Light","text":"<p><code>Directional Light</code> is GameObject with <code>Light</code> component which is used in the High Definition Render Pipeline (HDRP). It controls the shape, color, and intensity of the light. It also controls whether or not the light casts shadows in scene, as well as more advanced settings.</p> <p>In case of prefab <code>Nishishinjuku RandomTraffic</code> a <code>Directional</code> type light is added. It creates effects that are similar to sunlight in scene. This light illuminates all GameObjects in the scene as if the light rays are parallel and always from the same direction. <code>Directional</code> light disregards the distance between the Light itself and the target, so the light does not diminish with distance. The strength of the Light (<code>Intensity</code>) is set to <code>73123.09 Lux</code>. In addition, a <code>Shadow Map</code> with a resolution of <code>4096</code> is enabled, which is updated in <code>Every Frame</code> of the simulation. The transform of the <code>Directional Light</code> object is set in such a way that it shines on the environment almost vertically from above.</p> <p></p>"},{"location":"Components/Environment/AWSIMEnvironment/#npcpedestrians","title":"NPCPedestrians","text":"<p><code>NPCPedestrians</code> is an aggregating object for <code>NPCPedestrian</code> objects placed in the environment. Prefab <code>Nishishinjuku RandomTraffic</code> has 7 <code>NPCPedestrian</code> (<code>humanElegant</code>) prefabs defined in selected places. More about this <code>NPCPedestrian</code> prefab you can read in this section.</p>"},{"location":"Components/Environment/AWSIMEnvironment/#environment-script","title":"Environment (script)","text":"<p>Environment (script) contains the information about how a simulated <code>Environment</code> is positioned in real world. That means it describes what is the real world position of a simulated <code>Environment</code>.</p> <p>AWSIM uses part of a Military Grid Reference System (MGRS). To understand this topic, you only need to know, that using MGRS you can specify distinct parts of the globe with different accuracy. For AWSIM the chosen accuracy is a 100x100 km square. Such a square is identified with a unique code like <code>54SUE</code> (for more information on Grid Zone please see this page).</p> <p>Inside this Grid Zone the exact location is specified with the offset calculated from the bottom-left corner of the Grid Zone. You can interpret the Grid Zone as a local coordinate system in which you position the <code>Environment</code>.</p> <p>In the <code>Nishishinjuku RandomTraffic</code> prefab, the simulated <code>Environment</code> is positioned in the Grid Zone <code>54SUE</code>. The offset if equal to <code>81655.73</code> meters in the <code>Ox</code> axis, <code>50137.43</code> meters in the <code>Oy</code> axis and <code>42.49998</code> meters in the <code>Oz</code> axis. In addition to this shift, it is also necessary to rotate the Environment in the scene by <code>90</code> degrees about the <code>Oy</code> axis - this is ensured by the transform in the prefab object.</p> <p>This means that the 3D models were created in reference to this exact point and because of that the 3D models of <code>Environment</code> align perfectly with the data from Lanelet2.</p> <p>The essence of Environment (script)</p> <p>The Environment (script) configuration is necessary at the moment of loading data from Lanelet2.</p> <p>Internally it shifts the elements from Lanelet2 by the given offset so that they align with the <code>Environment</code> that is located at the local origin with no offset.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/","title":"Add Environment","text":""},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#add-an-environment","title":"Add an Environment","text":"<p>Environment is an important part of a Scene in AWSIM. Every aspect of the simulated surrounding world needs to be included in the <code>Environment</code> prefab - in this section you will learn how to develop it. However, first Lanelet2 needs to be developed along with 3D models of the world, which will be the main elements of this prefab.</p> <p>Tip</p> <p>If you want to learn more about the Environment at AWSIM, please visit this page.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#create-a-lanelet2","title":"Create a Lanelet2","text":"<p>Before you start creating Lanelet2, we encourage you to read the documentation to find out what Lanelet2 is all about. Lanelet2 can be created using VectorMapBuilder (<code>VMP</code>) based on the PCD obtained from real-life LiDAR sensor.</p> <p>When working with the <code>VMP</code>, it is necessary to ensure the most accurate mapping of the road situation using the available elements. Especially important are <code>TrafficLanes</code> created in <code>VMB</code> as connected <code>Road Nodes</code> and <code>StopLines</code> created in VMB as <code>Road Surface Stoplines</code>.</p> <p>Lanelet2 positioning</p> <p>Lanelet2 should be created in MGRS coordinates of the real place you are recreating. Please position your Lanelet2 relative to the origin (bottom left corner) of the MGRS Grid Zone with the 100 km Square ID in which the location lays. More details can be read here.</p> <p>You can think of the Grid Zone as a local coordinate system. Instead of making global (0,0) point (crossing of Equator and Prime Median) our coordinate system origin we take a closer one. The MGRS Grid Zone with 100 km Square ID code designates a 100x100 [kmxkm] square on the map and we take its bottom left corner as our local origin.</p> <p>Example</p> <p>Lets examine one node from an example Lanelet2 map:</p> <pre><code>&lt;node id=\"4\" lat=\"35.68855194431519\" lon=\"139.69142711058254\"&gt;\n    &lt;tag k=\"mgrs_code\" v=\"54SUE815501\"/&gt;\n    &lt;tag k=\"local_x\" v=\"81596.1357\"/&gt;\n    &lt;tag k=\"local_y\" v=\"50194.0803\"/&gt;\n    &lt;tag k=\"ele\" v=\"34.137\"/&gt;\n&lt;/node&gt;\n</code></pre> <p>The node with <code>id=\"4\"</code> position is described as absolute coordinates given in the <code>&lt;node&gt;</code>. In this example the coordinates are as follows <code>lat=\"35.68855194431519\" lon=\"139.69142711058254</code>.</p> <p>It is also described as local transformation defined as a translation relative to the origin of the MGRS Grid Zone with 100 km Square ID (bottom left corner). The MGRS Grid Zone designation with 100 km Square ID in this case is equal to <code>54SUE</code>. In this example the offset in the X axis is as follows <code>k=\"local_x\" v=\"81596.1357\"</code> and the offset in the Y axis is as follows <code>k=\"local_y\" v=\"50194.0803\"</code>.</p> <p>Note that elevation information is also included.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#create-3d-models","title":"Create 3D models","text":"<p>You can create <code>3D</code> models of an <code>Environment</code> as you wish. It is advised however, to prepare the models in form of <code>.fbx</code> files. Additionally you should include materials and textures in separate directories. Many models are delivered in this format. This file format allows you to import models into Unity with materials and replace materials while importing. You can learn more about it here.</p> <p>You can see a <code>.fbx</code> model added and modified on the fly in the example of this section.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#guidelines","title":"Guidelines","text":"<p>To improve the simulation performance of a scene containing your  <code>Environment</code> prefab, please keep in mind some of these tips when creating 3D models:</p> <ol> <li> <p>Prefer more smaller models over a few big ones.</p> <p>In general it is beneficial for performance when you make one small mesh of a object like tree and reuse it on the scene placing many prefabs instead of making one giant mesh containing all trees on the given scene. It is beneficial even in situations when you are not reusing the meshes. Lets say you have a city with many buildings - and every one of those buildings is different - it is still advised to model those building individually and make them separate GameObjects.</p> </li> <li> <p>Choose texture resolution appropriately.</p> <p>Always have in mind what is the target usage of your texture. Avoid making a high resolution texture for a small object or the one that will always be far away from the camera. This way you can save some computing power by not calculating the details that will not be seen because of the screen resolution.</p> <p>Practical advice</p> <p>You can follow these simple rules when deciding on texture quality (texel density):</p> <ul> <li>For general objects choose 512px/m (so the minimum size of texture is 512/512)</li> <li>For important objects that are close to the camera choose 1024px/m (so the minimum size of texture is 1024/1024)</li> </ul> </li> <li> <p>(optional) Add animation.</p> <p>Add animations to correct objects. If some element in the 3D model are interactive they should be divided into separate parts.</p> </li> </ol> <p>What's more, consider these tips related directly to the use of 3D models in AWSIM:</p> <ol> <li>Creating a 3D model based on actual point cloud data makes it more realistic.</li> <li>AWSIM is created using HDRP (High Definition Rendering Pipeline) which performs better when object meshes are merged.</li> <li>Occlusion culling and flutter culling cannot be used because the sensors detection target will disappear.</li> <li>Each traffic light should have a separate <code>GameObject</code>. Also, each light in the traffic light should be split into separate materials.</li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#create-an-environment-prefab","title":"Create an Environment prefab","text":"<p>In this part, you will learn how to create a <code>Environment</code> prefab - that is, develop a GameObject containing all the necessary elements and save it as a prefab.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#1-add-3d-models","title":"1. Add 3D models","text":"<p>In this section we will add roads, buildings, greenery, signs, road markings etc. to our scene.</p> <p>Most often your models will be saved in the <code>.fbx</code> format. If so, you can customize the materials in the imported model just before importing it. Sometimes it is necessary as models come with placeholder materials. You can either</p> <ul> <li>replace materials for every added GameObject into the Scene,</li> <li>or replace materials for one GameObject and save this object as a prefab to easily load it later.</li> </ul> <p>In order to add 3D models from the <code>.fbx</code> file to the Scene please do the following steps:</p> <ol> <li>In the Project view navigate to the directory where the model is located and click on the model file.</li> <li>Now you can customize the materials used in the model in the Inspector view.</li> <li>Drag the model into the Scene where you want to position it.</li> <li>Move the Object in the Hierarchy tree appropriately.</li> <li>(optional) Now you can save this model configuration as a prefab to easily reuse it.     Do this by dragging the Object from the Scene into the Project view.     When you get a warning make sure to select you want to create an original new prefab.</li> </ol> <p>Example</p> <p>An example video of the full process of importing a model, changing the materials, saving new model as a prefab and importing the new prefab.  </p> <p>When creating a complex Environment with many elements you should group them appropriately in the Hierarchy view. This depends on the individual style you like more, but it is a good practice to add all repeating elements into one common Object. E.g. all identical traffic lights grouped in TrafficLights Object. The same can be done with trees, buildings, signs etc. You can group Objects as you like.</p> <p>Object hierarchy</p> <p>When adding elements to the Environment that are part of the static world (like 3D models of buildings, traffic lights etc.) it is good practice to collect them in one parent GameObject called <code>Map</code> or something similar.</p> <p>By doing this you can set a transformation of the parent GameObject <code>Map</code> to adjust the world pose in reference to e.g. loaded objects from Lanelet2. </p> <p>Remember to unpack</p> <p>Please remember to unpack all Object added into the scene. If you don't they will change materials together with the <code>.fbx</code> model file as demonstrated in the example below.</p> <p>This is unwanted behavior. When you import a model and change some materials, but leave the rest default and don't unpack the model, then your instances of this model on the scene may change when you change the original fbx model settings.</p> <p>See the example below to visualize what is the problem.</p> Example <p>In this example we will</p> <ul> <li>Place the model on the Scene.</li> <li>Then intentionally not unpack the model</li> <li>Only then change the materials of the original fbx model, not the instance on the scene</li> </ul> <p></p> <p>Watch what happens, the instance on the Scene changes the materials together with the model. This only happens if you don't unpack the model.</p> <p></p> <p>Example Environment after adding 3D models</p> <p>After completing this step you should have an <code>Environment</code> Object that looks similar to the one presented below.</p> <p></p> <p>The <code>Environment</code> with 3D models can look similar to the one presented below.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#2-add-an-environment-script","title":"2. Add an Environment Script","text":"<p>Add an <code>Environment Script</code> as component in the <code>Environment</code> object (see the last example in section before). It does not change the appearance of the Environment, but is necessary for the simulation to work correctly.</p> <ol> <li> <p>Click on the Add Component button in the <code>Environment</code> object.</p> <p></p> </li> <li> <p>Search for <code>Environment</code> and select it.</p> <p></p> </li> <li> <p>Set the <code>MGRS</code> to the offset of your Environment as explained in this section.</p> <p></p> </li> </ol> <p>Info</p> <p>Due to the differences between VectorMapBuilder and Unity, it may be necessary to set the transform of the <code>Environment</code> object. The transform in <code>Environment</code> should be set in such a way that the <code>TrafficLanes</code> match the modeled roads. Most often it is necessary to set the positive <code>90</code> degree rotation over <code>Y</code> axis.</p> <p>This step should be done after importing items from lanelet2. Only then will you know if you have Environment misaligned with items from lanelet2.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#3-add-a-directional-light","title":"3. Add a Directional Light","text":"<ol> <li> <p>Create a new child Object of the Environment and name it <code>Directional Light</code>.</p> <p></p> </li> <li> <p>Click <code>Add Component</code> button, search for <code>Light</code> and select it.</p> <p></p> </li> <li> <p>Change light Type to <code>Directional</code>.</p> </li> <li> <p>Now you can configure the directional light as you wish. E.g. change the intensity or orientation.</p> <p></p> </li> </ol> <p>Tip</p> <p>For more details on lighting check out official Unity documentation.</p> <p>Example Environment after adding Directional Light</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#4-add-a-volume","title":"4. Add a Volume","text":"<ol> <li> <p>Create a new child object of the Environment and name it <code>Volume</code>.</p> <p></p> </li> <li> <p>Click <code>Add Component</code> search for <code>Volume</code> and select it.</p> <p></p> </li> <li> <p>Change the Profile to <code>Volume Profile</code> and wait for changes to take effect.</p> <p></p> </li> <li> <p>Now you can configure the Volume individually as you wish.</p> </li> </ol> <p>Tip</p> <p>For more details on volumes checkout official Unity documentation.</p> <p>Example Environment after adding Volume</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#5-add-npcpedestrians","title":"5. Add NPCPedestrians","text":"<ol> <li> <p>Make <code>NPCPedestrians</code> parent object.</p> <p></p> </li> <li> <p>Open <code>Assets/AWSIM/Prefabs/NPCs/Pedestrians</code> in Project view and drag a <code>humanElegant</code> into the <code>NPCPedestrians</code> parent object.</p> <p></p> <p></p> </li> <li> <p>Click <code>Add Component</code> in the humanElegant object and search for <code>Simple Pedestrian Walker Controller</code> Script and select it.</p> <p></p> <p></p> <p>This is a simple Script that makes the pedestrian walk straight and turn around indefinitely. You can configure pedestrian behavior with 2 parameters.</p> <ul> <li>Duration - how long will the pedestrian walk straight</li> <li>Speed - how fast will the pedestrian walk straight</li> </ul> <p>Tip</p> <p>The <code>Simple Pedestrian Walker Controller</code> Script is best suited to be used on pavements.</p> </li> <li> <p>Finally position the <code>NPCPedestrian</code> on the scene where you want it to start walking.</p> <p>Warning</p> <p>Remember to set correct orientation, as the <code>NPCPedestrian</code> will walk straight from the starting position with the starting orientation.</p> </li> </ol> <p>Example Environment after adding NPC Pedestrians</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddEnvironment/#6-save-an-environment-prefab","title":"6. Save an Environment prefab","text":"<p>After doing all the previous steps and having your Environment finished you can save it to prefab format.</p> <ol> <li>Find an Environments directory in the Project view (<code>Assets/AWSIM/Prefabs/Environments</code>).</li> <li>Drag the <code>Environment</code> Object into the Project view.</li> <li>(optional) Change the prefab name to recognize it easily later.</li> </ol> <p></p> <p>Success</p> <p>Once you've added the <code>Environment</code>, you need to add and configure <code>TrafficLights</code>. For details please visit this tutorial.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddRandomTraffic/","title":"Add Random Traffic","text":"<p>To add a Random Traffic to your scene you need the Random Traffic Simulator Script.</p> <ol> <li> <p>Create a new Game Object as a child of <code>Environment</code> and call it <code>RandomTrafficSimulator</code>.</p> <p></p> </li> <li> <p>Click a button <code>Add Component</code> in the <code>Inspector</code> to add a script.</p> <p></p> </li> <li> <p>A small window should pop-up. Search for <code>RandomTrafficSimulator</code> script and add it by double clicking it or by pressing enter.</p> <p></p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddRandomTraffic/#basic-configuration","title":"Basic Configuration","text":"<p>After clicking on the newly created <code>RandomTrafficSimulator</code> object in the Scene tree you should see something like this in the <code>Inspector</code> view.</p> <p></p> <p>Random Traffic Simulator, as the name suggests, generates traffic based on random numbers. To replicate situations you can set a specific seed.</p> <p>You can also set <code>Vehicle Layer Mask</code> and <code>Ground Layer Mask</code>. It is important to set these layers correctly, as they are a base for vehicle physics. If set incorrectly the vehicles may fall through the ground into the infinity.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddRandomTraffic/#add-npcvehicles","title":"Add NPCVehicles","text":"<p>Random Traffic Simulator Script moves, spawns and despawns vehicles based on the configuration. These settings can to be adjusted to your preference.</p> <ol> <li> <p>Setting Max Vehicle Count.</p> <p>This parameter sets a limit on how many vehicles can be added to the scene at one time.</p> </li> <li> <p>NPC Prefabs</p> <p>These are models of vehicles that should be spawned on the scene, to add NPC Prefabs please follow these steps:</p> <ol> <li> <p>To do this click on the \"+\" sign and in the new list element at the bottom and click on the small icon on the right to select a prefab.</p> </li> <li> <p>Change to the <code>Assets</code> tab in the small windows that popped-up.</p> </li> <li> <p>Search for the Vehicle prefab you want to add, e.g. <code>Hatchback</code>.</p> </li> </ol> <p></p> <p>Available NPC prefabs are shown in the NPC Vehicle section.</p> <p>Control NPC Vehicle spawning</p> <p>Random Traffic Simulator Script will on random select one prefab from <code>Npc Prefabs</code> list every time when there  are not enough vehicles on the scene (the number of vehicles on the scene is smaller than the number specified in the <code>Max Vehicle Count</code> field).</p> <p>You can control the odds of selecting one vehicle prefab over another by adding more than one instance of the same prefab to this list.</p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddRandomTraffic/#add-spawnable-lanes","title":"Add spawnable lanes","text":"<p>Spawnable lanes are the lanes on which new vehicles can be spawned by the Random Traffic Simulator Script. Best practice is to use beginnings of the lanes on the edges of the map as spawnable lanes.</p> <p>Warning</p> <p>Make sure you have a lanelet added into your scene. The full tutorial on this topic can be found here.</p> <p>Adding spawnable lanes is similar to Adding NPC Prefabs.</p> <ol> <li> <p>Add an element to the <code>Spawnable Lanes</code> list by clicking on the \"+\" symbol or by selecting number of lanes directly.</p> <p></p> </li> <li> <p>Now you can click on the small icon on the right of the list element and select a Traffic Lane you are interested in.</p> <p></p> <p>Unfortunately all Traffic Lanes have the same names so it can be difficult to know which one to use. Alternatively you can do the following to add a traffic lane by visually selecting it in the editor:</p> <ul> <li> <p>Lock <code>RandomTrafficSimulator</code> in the Inspector view.</p> <p></p> </li> </ul> <ul> <li> <p>Select the Traffic Lane you are interested in on the Scene and as it gets highlighted in the Hierarchy view you can now drag and drop this Traffic Lane into the appropriate list element.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddRandomTraffic/#vehicles-configuration","title":"Vehicles configuration","text":"<p>The last thing to configure is the behavior of NPCVehicles. You can specify acceleration rate  of vehicles and three values of deceleration.</p> <p></p> <ul> <li> <p>Acceleration</p> <p>This value is used for every acceleration the vehicle performs (after stop line or traffic lights).</p> </li> </ul> <ul> <li> <p>Deceleration</p> <p>This deceleration value is used for most casual traffic situations like slowing down before stop line.</p> </li> </ul> <ul> <li> <p>Sudden Deceleration</p> <p>This deceleration rate is used for emergency situations - when using standard deceleration rate is not enough to prevent some accident from happening (e.g. vehicle on the intersection didn't give way when it was supposed to).</p> </li> </ul> <ul> <li> <p>Absolute Deceleration</p> <p>This deceleration rate is a last resort for preventing a crash from happening. When no other deceleration is enough to prevent an accident this value is used. This should be set to the highest value achievable by a vehicle.</p> </li> </ul> <p>Question</p> <p>This configuration is common for all vehicles managed by the <code>Random Traffic Simulator Script</code>.</p> <p>Success</p> <p>The last thing that needs to be done for <code>RandomTraffic</code> to work properly is to add intersections with traffic lights and configure their sequences. Details here.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/","title":"Add Traffic Intersection","text":"<p>Every <code>TrafficIntersection</code> on the scene needs to be added as a GameObject. Best practice is to create a parent object <code>TrafficIntersections</code> and add all instances of <code>TrafficIntersection</code> as its children. You can do this the same as with Random Traffic Simulator.</p> <p>Traffic Lights configuration</p> <p>Before performing this step, check all <code>TrafficLights</code> for correct configuration and make sure that <code>TrafficLights</code> have added scripts. If you want to learn how to add and configure it check out this tutorial.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#add-a-box-collider","title":"Add a Box Collider","text":"<ol> <li> <p><code>TrafficIntersection</code> needs to be marked with a box collider. First click on the <code>Add Component</code> button.</p> <p></p> </li> <li> <p>In the window that popped up search for <code>Box Collider</code> and select it.</p> <p></p> </li> <li> <p>Then set the position and orientation and size of the Box Collider. You can do this by manipulating Box Collider properties <code>Center</code> and <code>Size</code> in the Inspector view.</p> <p></p> <p>Traffic Intersection Box Collider guidelines</p> <p>When adding a Box Collider marking your Traffic Intersection please make sure that</p> <ul> <li>It is not floating over the ground - there is no gap between the Box Collider and The Traffic Intersection</li> <li>It is high enough to cover all Vehicles that will be passing through the Intersection</li> <li>It accurately represents the shape, position and orientation of the Traffic Intersection</li> </ul> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#add-a-traffic-intersection-script","title":"Add a Traffic Intersection Script","text":"<ol> <li> <p>Click on the <code>Add Component</code> button.</p> <p></p> </li> <li> <p>In the window that popped up search for <code>Traffic Intersection</code> and select it.</p> <p></p> </li> <li> <p>You need to set a proper Collider Mask in order for the script to work.</p> <p></p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#create-traffic-light-groups","title":"Create traffic light groups","text":"<p>Traffic Light Groups are groups of traffic lights that are synchronized, meaning they light up with the same color and pattern at all times.</p> <p>Traffic lights are divided into groups to simplify the process of creating a lighting sequence. By default you will see 4 Traffic Light Groups, you can add and remove them to suit your needs.</p> <ol> <li> <p>First choose from the drop-down menu called <code>Group</code> the Traffic Light Group name you want to assign to your Traffic Light Group.</p> <p> </p> </li> <li> <p>Then add as many Traffic Lights as you want your group to have. From the drop-down menu select the Traffic Lights you want to add.</p> <p> </p> <p>Select Traffic Lights visually</p> <p>If you have a lot of Traffic Lights it can be challenging to add them from the list. You can select them visually from the Scene the same as you had selected Traffic Lanes in the Random Traffic Simulator.</p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#create-lighting-sequences","title":"Create lighting sequences","text":"<p>Lighting Sequences is a list of commands based on which the Traffic Lights will operate on an intersection. The elements in the Lighting Sequences list are changes (commands) that will be executed on the Traffic Light Groups.</p> <p>Group Lighting Order should be interpreted as a command (or order) given to all Traffic Lights in selected Traffic Light Group. In Group Lighting Orders you can set different traffic light status for every Traffic Light Group (in separate elements). Lighting sequences list is processed in an infinite loop.</p> <p>It should be noted that changes applied to one Traffic Light Group will remain the same until the next Group Lighting Order is given to this Traffic Light Group. This means that if in one Group Lighting Order no command is sent to a Traffic Light Group then this Group will remain its current lighting pattern (color, bulb and status).</p> <p>For every Lighting Sequences Element you have to specify the following</p> <ol> <li> <p>Interval Sec</p> <p>This is the time for which the sequence should wait until executing next order, so how long this state will be active.</p> </li> <li> <p>For every element in Group Lighting Orders there needs to be specified</p> <ol> <li>Group to which this order will be applied</li> <li> <p>List of orders (Bulb Data)</p> <p>In other words - what bulbs should be turned on, their color and pattern.</p> <ul> <li>Type - What type of bulb should be turned on</li> <li>Color - What color this bulb should have (in most cases this will be the same as color of the bulb if specified)</li> <li>Status - How the bulb should light up (selecting <code>SOLID_OFF</code> is necessary only when you want to turn the Traffic Light completely off, meaning no bulb will light up)</li> </ul> <p>Note</p> <p>When applying the change to a Traffic Light</p> <ul> <li>First all bulbs are turned off</li> <li>Only after that changes made in the order are applied</li> </ul> <p>This means it is only necessary to supply the data about what bulbs should be turned on. E.g. you don't have to turn off a red bulb when turning on the green one.</p> </li> </ol> </li> </ol> <p>Warning</p> <p>The first Element in the Lighting Sequences (in most cases) should contain bulb data for every Traffic Light Group. Traffic Light Groups not specified in the first Element will not light up at the beginning of the scene.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#example","title":"Example","text":"<p>Lets consider the following lighting sequence element.</p> <p></p> <p>In the Lighting Sequence Element 5 we tell all Traffic Lights in the Vehicle Traffic Light Group 2 to light up their Green Bulb with the color Green and status Solid On which means that they will be turned on all the time. We also implicitly tell them to turn all other Bulbs off.</p> <p>In the same time we tell all Traffic Lights in the Pedestrian Traffic Light Group 2 to do the very same thing.</p> <p>This state will be active for the next 15 seconds, and after that Traffic Intersection will move to the next Element in the Sequence.</p> <p>Now lets consider the following Lighting Sequences Element 6.</p> <p></p> <p>Here we order the Traffic Lights in the Pedestrian Traffic Light Group 2 to light up their Green Bulb with the color Green and status Flashing. We also implicitly tell them to turn all other bulbs off, which were already off from the implicit change in Element 5, so this effectively does nothing.</p> <p>Note that Lighting Sequences Element 6 has no orders for Vehicle Traffic Light Group 2. This means that Traffic Lights in the Vehicle Traffic Light Group 2 will hold on to their earlier orders.</p> <p>This state will be active for 5 seconds, which means that Traffic Lights in the Vehicle Traffic Light Group 2 will be lighting solid green for the total of 20 seconds.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/AddTrafficIntersection/#how-to-test","title":"How to test","text":"<p>To test how your Traffic Intersection behaves simply run the Scene as shown here (but don't launch Autoware). To take a better look at the Traffic Lights you can change to the Scene view by pressing <code>ctrl + 1</code> - now you can move the camera freely (to go back to the Game view simply press <code>ctrl + 2</code>).</p> <p>As the time passes you can examine whether your Traffic Intersection is configured correctly.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/","title":"Load Items From Lanelet","text":""},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#load-items-from-lanelet2","title":"Load items from lanelet2","text":"<p>To add <code>RandomTraffic</code> to the <code>Environment</code>, it is necessary to load elements from the lanelet2. As a result of loading, <code>TrafficLanes</code> and <code>StopLines</code> will be added to the scene. Details of these components can be found here.</p> <p>Warning</p> <p>Before following this tutorial make sure you have added an Environment Script and set a proper <code>MGRS</code> offset position. This position is used when loading elements from the lanelet2!</p> <ol> <li> <p>Click on the <code>AWSIM</code> button in the top menu of the Unity editor and navigate to <code>AWSIM -&gt; Random Traffic -&gt; Load Lanelet</code>.</p> <p></p> </li> <li> <p>In the window that pops-up select your osm file, change some Waypoint Settings to suit your needs and click <code>Load</code>.</p> <p></p> <p>Waypoint Settings explanation</p> <ul> <li>Resolution - resolution of resampling. Lower values provide better accuracy at the cost of processing time</li> <li>Min Delta Length - minimum length(m) between adjacent points</li> <li>Min Delta Angle - minimum angle(deg) between adjacent edges. Lowering this value produces a smoother curve</li> </ul> </li> <li> <p>Traffic Lanes and Stop Lanes should occur in the Hierarchy view. If they appear somewhere else in your Hierarchy tree, then move them into the <code>Environment</code> object.</p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#complete-loaded-trafficlanes","title":"Complete loaded TrafficLanes","text":"<p>The Traffic Lanes that were loaded should be configures accordingly to the road situation. The aspects you can configure</p> <ul> <li> <p>Right of way</p> <p>The right of way has to be configured so that Vehicles know how ot behave in the traffic. To configure this please visit a dedicated section in Add a Traffic Lane. After you have set right of way to all traffic lanes please follow this final step.</p> </li> </ul> <ul> <li> <p>Stop Line</p> <p>Assuming you have all Stop Lines loaded from the lanelet you have to add them to the Traffic Lanes. For detailed instruction please visit a dedicated section in Add a Traffic Lane.</p> <p>If - for any reason - you don't have all the Stop Lines added, please follow this dedicated section.</p> </li> </ul>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#how-to-test","title":"How to test","text":"<p>If you want to test your Traffic Lanes you have to try running a Random Traffic. To verify one particular Traffic Lane or Traffic Lane connection you can make a new spawnable lane next to the Traffic Lane you want to test. This way you can be sure NPCVehicles will start driving on the Traffic Lane you are interested in at the beginning.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#add-a-stopline-manually","title":"Add a StopLine manually","text":"<p>When something goes wrong when loading data from lanelet2 or you just want to add another StopLine manually please do the following</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#1-add-a-gameobject","title":"1. Add a GameObject","text":"<p>Add a new GameObject StopLine in the StopLines parent object.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#2-add-a-stopline-script","title":"2. Add a StopLine Script","text":"<p>Add a StopLine Script by clicking 'Add Component' and searching for <code>Stop Line</code>.</p> <p></p> <p></p> <p>Example</p> <p>So far your Stop Line should look like the following</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#3-set-points","title":"3. Set points","text":"<p>Set the position of points <code>Element 0</code> and <code>Element 1</code>. These Elements are the two end points of a Stop Line. The Stop Line will span between these points.</p> <p>You don't need to set any data in the 'Transform' section as it is not used anyway.</p> <p>StopLine coordinate system</p> <p>Please note that the Stop Line Script operates in the global coordinate system. The transformations of StopLine Object and its parent Objects won't affect the Stop Line.</p> Example <p>In this example you can see that the Position of the Game Object does not affect the position and orientation of the Stop Line.</p> <p>For a Game Object in the center of the coordinate system.</p> <p></p> <p>The stop Line is in the specified position.</p> <p></p> <p>However with the Game Object shifted in X axis.</p> <p></p> <p>The Stop Line stays in the same position as before, not affected by any transformations happening to the Game Object.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#4-has-stop-sign","title":"4. Has Stop Sign","text":"<p>Select whether there is a Stop Sign.</p> <p>Select the <code>Has Stop Sign</code> tick-box confirming that this Stop Line has a Stop Sign. The Stop Sign can be either vertical or horizontal.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#5-select-a-traffic-light","title":"5. Select a Traffic Light","text":"<p>Select from the drop-down menu the Traffic Light that is on the Traffic Intersection and is facing the vehicle that would be driving on the Traffic Lane connected with the Stop Line you are configuring.</p> <p>In other words select the right Traffic Light for the Lane on which your Stop Line is placed.</p> <p></p> <p>Select Traffic Lights visually</p> <p>If you have a lot of Traffic Lights it can be challenging to add them from the list. You can select them visually from the Scene the same as you had selected Traffic Lanes in the Random Traffic Simulator.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#6-configure-the-traffic-lane","title":"6. Configure the Traffic Lane","text":"<p>Every Stop Line has to be connected to a Traffic Lane. This is done in the Traffic Lane configuration. For this reason please check the Traffic Lane section for more details.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#add-a-trafficlane-manually","title":"Add a TrafficLane manually","text":"<p>It is possible that something may go wring when reading a lanelet2 and you need to add an additional Traffic Lane or you just want to add it. To add a Traffic Lane manually please follow the steps below.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#1-add-a-gameobject_1","title":"1. Add a GameObject","text":"<p>Add a new Game Object called <code>TrafficLane</code> into the <code>TrafficLanes</code> parent Object.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#2-add-a-traffic-lane-script","title":"2. Add a Traffic Lane Script","text":"<p>Click the 'Add Component' button and search for the <code>Traffic lane</code> script and select it.</p> <p></p> <p></p> <p>Example</p> <p>So far your Traffic Lane should look like the following.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#3-configure-waypoints","title":"3. Configure Waypoints","text":"<p>Now we will configure the 'Waypoints' list. This list is an ordered list of nest points defining the Traffic Lane. When you want to add a waypoint to a Traffic Lane just click on the <code>+</code> button or specify the number of waypoints on the list in the field with number to the right from 'Waypoints' identifier.</p> <p>The order of elements on this list determines how waypoints are connected.</p> <p></p> <p>Traffic Lane coordinate system</p> <p>Please note that the Traffic Lane waypoints are located in the global coordinate system, any transformations set to a Game Object or paren Objects will be ignored.</p> <p>This behavior is the same as with the Stop Line. You can see the example provided in the Stop Line tutorial.</p> <p>General advice</p> <ul> <li>Two waypoints should not be too far away from each other.</li> <li>When creating a turn that is a curvature please keep in mind the angle that is created between two next waypoints connected.     The angles should be fairly small - this will translate to a smooth motion of vehicles.</li> </ul>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#4-select-the-turn-direction","title":"4. Select the Turn Direction","text":"<p>You also need to select the Turn Direction. This field describes what are the vehicles traveling on ths Traffic Lane doing in reference to other Traffic Lanes. You need to select whether the vehicles are</p> <ul> <li>Driving straight (<code>STRAIGHT</code>)</li> <li>Turning right (<code>RIGHT</code>)</li> <li>Turning left (<code>LEFT</code>)</li> </ul> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#5-configure-next-lanes","title":"5. Configure Next Lanes","text":"<p>You need to add all Traffic Lanes that have their beginning in the end of this Traffic Lane into the Next Lanes list. In other words if the vehicle can choose where he wants to drive (e.g. drive straight or drive left with choice of two different Traffic Lines).</p> <p>To do this click the <code>+</code> sign in the Next Lanes list and in the element that appeared select the correct Traffic Lane.</p> <p></p> Next Lane example <p>Lets consider the following Traffic Intersection.</p> <p></p> <p>In this example we will consider the Traffic Lane driving from the bottom of the screen and turning right. After finishing driving in this Traffic Lane the vehicle has a choice of 4 different Traffic Lanes each turning into different lane on the parallel road.</p> <p>All 4 Traffic Lanes are connected to the considered Traffic Lane. This situation is reflected in the Traffic Lane configuration shown below.</p> <p></p> <p>Select Traffic Lanes visually</p> <p>If you have a lot of Traffic Lanes it can be challenging to add them from the list. You can select them visually from the Scene the same as you had selected Traffic Lanes in the Random Traffic Simulator.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#6-configure-previous-lanes","title":"6. Configure Previous Lanes","text":"<p>Traffic Lane has to have previous Traffic Lanes configured. This is done in the exact same way as configuring next lanes which was shown in the previous step. Please do the same, but add Traffic Lanes that are before the configured one instead of the ones after into the Prev Lanes list.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#7-configure-right-of-way","title":"7. Configure Right Of Way","text":"<p>Now we will configure the Right Of Way Lanes. The Right Of Way Lanes is a list of Traffic Lanes that have a priority over the configured one. The process of adding the Right Of Way Lanes is the same as with adding Next Lanes. For this reason we ask you to see the aforementioned step for detailed description on how to do this (the only difference is that you add Traffic Lanes to the Right Of Way Lanes list).</p> Right of way example <p>In this example lets consider the Traffic Lane highlighted in blue from the Traffic Intersection below.</p> <p></p> <p>This Traffic Lane has to give way to all Traffic Lanes highlighted on yellow. This means all of the yellow Traffic Lanes have to be added to the 'Right Of Way Lanes' list which is reflected on the configuration shown below.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#8-add-stop-line","title":"8. Add Stop Line","text":"<p>Adding a Stop Line is necessary only when at the end of the configured Traffic Lane the Stop Line is present. If so, please select the correct Stop Line from the drop-down list.</p> <p></p> <p>Select Stop Line visually</p> <p>If you have a lot of Stop Lines it can be challenging to add them from the list. You can select them visually from the Scene the same as you had selected Traffic Lanes in the Random Traffic Simulator.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#9-add-speed-limit","title":"9. Add Speed Limit.","text":"<p>In the field called <code>Speed Limit</code> simply write the speed limit that is in effect on the configured Traffic Lane.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddRandomTraffic/LoadItemsFromLanelet/#10-set-the-right-of-ways","title":"10. Set the Right of Ways","text":"<p>To make the Right Of Ways list you configured earlier take effect simply click the 'Set RightOfWays' button.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/","title":"Add TrafficLights","text":""},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#add-trafficlights","title":"Add TrafficLights","text":"<p>To add <code>TrafficLights</code> into your <code>Environment</code> follow steps below.</p> <p>Tip</p> <p>In the <code>Environment</code> you are creating there will most likely be many <code>TrafficLights</code> that should look and work the same way. To simplify the process of creating an environment it is advised to create one <code>TrafficLight</code> of each type with this tutorial and then save them as prefabs that you will be able to reuse.</p>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#1-add-trafficlight-object","title":"1. Add TrafficLight Object","text":"<p>Into your <code>Map</code> object in the Hierarchy view add a new Child Object and name it appropriately.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#2-add-a-mesh-filter-and-select-meshes","title":"2. Add a Mesh Filter and select meshes","text":"<ol> <li> <p>Click on the <code>Add Component</code> button.</p> <p></p> </li> <li> <p>Search for <code>Mesh filter</code> and select it by clicking on it.</p> <p></p> </li> <li> <p>For each <code>TrafficLight</code> specify the mesh you want to use.</p> <p></p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#3add-a-mesh-renderer-and-specify-materials","title":"3.Add a Mesh Renderer and specify materials","text":"<ol> <li> <p>The same way as above search for <code>Mesh Renderer</code> and select it.</p> <p></p> </li> <li> <p>Now you need to specify individual component materials.</p> <p>For example in the <code>Traffic.Lights.001</code> mesh there are four sub-meshes that need their individual materials.</p> <p>To specify a material click on the selection button on <code>Materials</code> element and search for the material you want to use and select it.</p> <p></p> <p>Repeat this process until you specify all materials. When you add one material more than there are sub-meshes you will see this warning. Then just remove the last material and the <code>TrafficLight</code> is prepared.</p> <p></p> <p>Info</p> <p>Different material for every bulb is necessary for the color changing behavior that we expect from traffic lights. Even though in most cases you will use the same material for every <code>Bulb</code>, having them as different elements is necessary. Please only use models of <code>TrafficLights</code> that have different <code>Materials Elements</code> for every <code>Bulb</code>.</p> <p>Materials order</p> <p>When specifying materials remember the order in which they are used in the mesh. Especially remember what <code>Materials Elements</code> are associated with every <code>Bulb</code> in the <code>TrafficLight</code>. This information will be needed later.</p> <p>Example</p> <p>In the case of <code>Traffic.Lights.001</code> the bulb materials are ordered starting from the left side with index 1 and increasing to the right.</p> <p></p> <p></p> <p></p> </li> </ol>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#4-add-a-mesh-collider","title":"4. Add a Mesh Collider","text":"<p>The same way as above search for <code>Mesh Collider</code> and select it. Collider may not seem useful, as the <code>TrafficLight</code> in many cases will be out of reach of vehicles. It is however used for LiDAR simulation, so it is advised to always add colliders to Objects that should be detected by LiDARs.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#5-position-trafficlight-in-the-environment","title":"5. Position TrafficLight in the Environment","text":"<p>Finally after configuring all visual aspects of the <code>TrafficLight</code> you can position it in the environment. Do this by dragging a <code>TrafficLight</code> with a square representing a plane or with an arrow representing one axis.</p> <p></p>"},{"location":"Components/Environment/AddNewEnvironment/AddTrafficLights/#6-add-trafficlight-script","title":"6. Add TrafficLight Script","text":"<p>The <code>Traffic Light</code> Script will enable you to control how the <code>TrafficLight</code> lights up and create sequences.</p> <ol> <li> <p>Click on <code>Add Component</code>, search for the <code>Traffic Light</code> script and select it.</p> <p></p> </li> <li> <p>You should see the <code>Bulb Emission config</code> already configured. These are the colors that will be used to light up the Bulbs in <code>TrafficLight</code>. You may adjust them to suit your needs.</p> <p></p> </li> <li> <p>You will have to specify <code>Bulb material config</code>, in which you should add elements with fields:</p> <ul> <li><code>Bulb Type</code> - One of the predefined Bulb types that describes the Bulb (its color and pattern).</li> </ul> <ul> <li><code>Material Index</code> - Index of the material that you want to be associated with the Bulb Type. This is where you need to use the knowledge from earlier where we said you have to remember what Materials Element corresponds to which bulb sub-mesh.</li> </ul> <p>Bulb configuration example</p> <p>Here we specify an element Type as <code>RED_BULB</code> and associate it with Material that has an index 3. This will result in associating the right most bulb with the name <code>RED_BULB</code>. This information will be of use to us when specifying <code>TrafficLights</code> sequences.</p> <p></p> </li> </ol> <p>Success</p> <p>Once you have added <code>TrafficLights</code> to your <code>Environment</code>, you can start configuring <code>RandomTraffic</code> which will add moving vehicles to it! Details here.</p>"},{"location":"Components/Environment/CreatePCD/","title":"Create PCD","text":""},{"location":"Components/Environment/CreatePCD/#create-pcd","title":"Create PCD","text":"<p>This section</p> <p>This section is still under development!</p>"},{"location":"Components/Environment/CreatePCD/#pointcloudmapper","title":"PointCloudMapper","text":""},{"location":"Components/Environment/CreatePCD/#description","title":"Description","text":"<p><code>PointCloudMapper</code> is a tool for a vehicle based point cloud mapping in a simulation environment. It is very useful when you need a point cloud based on some location, but don't have the possibility to physically map the real place. Instead you can map the simulated environment.</p>"},{"location":"Components/Environment/CreatePCD/#required-data","title":"Required Data","text":"<p>To properly perform the mapping, make sure you have the following files downloaded and configured:</p> <ul> <li>Lanelet2 format OSM data (<code>*.osm</code> file)</li> <li> <p>3D model map of the area</p> <p>How to obtain a map</p> <p>You can obtain the 3D model of the area by using a <code>Environment</code> prefab prepared for AWSIM or by creating your own. You can learn how to create you own <code>Environment</code> prefab in this tutorial.</p> </li> </ul> <ul> <li> <p>Configured in-simulation vehicle object with sensors attached (only the LiDAR is necessary)</p> <p>Vehicle model</p> <p>For the sake of creating a PCD the vehicle model doesn't have to be accurate. It will be just a carrier for LiDAR. The model can even be a simple box as shown earlier in this tutorial. Make sure it is not visible to the LiDAR, so it does not break the sensor readings.</p> </li> </ul>"},{"location":"Components/Environment/CreatePCD/#import-osm","title":"Import OSM","text":"<ol> <li> <p>Drag and drop an OSM file into Unity project.</p> <p></p> </li> <li> <p>OSM file will be imported as <code>OsmDataContainer</code>.</p> </li> </ol>"},{"location":"Components/Environment/CreatePCD/#setup-an-environment","title":"Setup an Environment","text":"<p>For mapping an <code>Environment</code> prefab is needed. The easiest way is to create a new Scene and import the <code>Environment</code> prefab into it. Details on how to do this can be found on this tutorial page.</p>"},{"location":"Components/Environment/CreatePCD/#setup-a-vehicle","title":"Setup a Vehicle","text":"<p>Create a <code>Vehicle</code> GameObject in the Hierarchy view.</p> <p></p>"},{"location":"Components/Environment/CreatePCD/#add-visual-elements-optional","title":"Add visual elements (optional)","text":"<p>Add vehicle model by adding a <code>Geometry</code> Object as a child of <code>Vehicle</code> and adding all visual elements as children.</p> <p></p> <p>Visual elements</p> <p>You can learn how to add visual elements and required components like Mesh Filter or Mesh Renderer in this tutorial.</p>"},{"location":"Components/Environment/CreatePCD/#add-a-camera-optional","title":"Add a Camera (optional)","text":"<p>Add a Camera component for enhanced visuals by adding a <code>Main Camera</code> Object as a child of <code>Vehicle</code> Object and attaching a <code>Camera</code> Component to it.</p> <ol> <li> <p>Add a <code>Main Camera</code> Object.</p> <p></p> </li> <li> <p>Add a <code>Camera</code> Component by clicking 'Add Component' button, searching for it and selecting it.</p> <p></p> </li> <li> <p>Change the <code>Transform</code> for an even better visual experience.</p> <p>Camera preview</p> <p>Observe how the Camera preview changes when adjusting the transformation.</p> <p></p> </li> </ol>"},{"location":"Components/Environment/CreatePCD/#setup-vehicle-sensors-rgl","title":"Setup Vehicle Sensors (RGL)","text":"<p>This part of the tutorial shows how to add a LiDAR sensor using RGL.</p> <p>RGL Scene Manager</p> <p>Please make sure that <code>RGLSceneManager</code> is added to the scene. For more details and instruction how to do it please visit this tutorial page.</p> <ol> <li> <p>Create an empty <code>Sensors</code> GameObject as a child of the <code>Vehicle</code> Object.</p> <p></p> </li> <li> <p>Create a <code>Lidar</code> GameObject as a child of the <code>Sensors</code> Object.</p> <p></p> </li> <li> <p>Attach Lidar Sensor (script) to previously created <code>Lidar</code> Object by clicking on the 'Add Component' button, searching for the script and selecting it.</p> <p>Point Cloud Visualization</p> <p>Please note that Point Cloud Visualization (script) will be added automatically with the Lidar Sensor (script).</p> <p></p> <p></p> </li> <li> <p>Configure LiDAR pattern, e.g. by selecting one of the available presets.</p> <p>Example Lidar Sensor configuration</p> <p></p> <p>Gaussian noise</p> <p>Gaussian noise should be disabled to achieve a more accurate map.</p> </li> <li> <p>Attach RGL Mapping Adapter (script) to previously created <code>Lidar</code> Object by clicking on the 'Add Component' button, searching for the script and selecting it.</p> <p></p> <p></p> </li> <li> <p>Configure <code>RGL Mapping Adapter</code> - e.g. set <code>Leaf Size</code> for filtering.</p> <p>Example RGL Mapping Adapter configuration</p> <p></p> <p>Downsampling</p> <p>Please note that downsampling is applied on the single LiDAR scans only. If you would like to filter merged scans use the external tool described below.</p> </li> </ol>"},{"location":"Components/Environment/CreatePCD/#effect-of-leaf-size-to-point-cloud-data-pcd-generation","title":"Effect of <code>Leaf Size</code> to Point Cloud Data (PCD) generation","text":"<p>Downsampling aims to reduce PCD size which for large point clouds may achieve gigabytes in exchange for map details. It is essential to find the best balance between the size and acceptable details level.</p> <p>A small <code>Leaf Size</code> results in a more detailed PCD, while a large <code>Leaf Size</code> could result in excessive filtering such that objects like buildings are not recorded in the PCD.</p> <p>In the following examples, it can be observed that when a <code>Leaf Size</code> is 1.0, point cloud is very detailed. When a <code>Leaf Size</code> is 100.0, buildings are filtered out and results in an empty PCD. A <code>Leaf Size</code> of 10.0 results in a reasonable PCD in the given example.</p> Leaf Size = 1.0 Leaf Size = 10.0 Leaf Size = 100.0"},{"location":"Components/Environment/CreatePCD/#setup-pointcloudmapper","title":"Setup PointCloudMapper","text":"<ol> <li> <p>Create a <code>PointCloudMapper</code> GameObject in the Hierarchy view.</p> <p></p> </li> <li> <p>Attach <code>Point Cloud Mapper</code> script to previously created <code>Point Cloud Mapper</code> Object by clicking on the 'Add Component' button, searching for the script and selecting it.</p> <p></p> <p></p> </li> <li> <p>Configure the <code>Point Cloud Mapper</code> fields:</p> <ul> <li><code>Osm Container</code> - the OSM file you imported earlier</li> <li> <p><code>World Origin</code> - MGRS position of the origin of the scene</p> <p>World Origin coordinate system</p> <p>Use ROS coordinate system for World Origin, not Unity.</p> </li> </ul> <ul> <li><code>Capture Location Interval</code> - Distance between consecutive capture points along lanelet centerline</li> <li><code>Output Pcd File Path</code> - Output relative path from <code>Assets</code> folder</li> <li><code>Target Vehicle</code> - The vehicle you want to use for point cloud capturing that you created earlier</li> </ul> <p>Example Point Cloud Mapper configuration</p> <p></p> <p>Lanelet visualization</p> <p> It is recommended to disable Lanelet Visualizer by setting <code>Material</code> to <code>None</code> and <code>Width</code> equal to zero. Rendered Lanelet is not ignored by the LiDAR so it would be captured in the PCD.</p> </li> </ol>"},{"location":"Components/Environment/CreatePCD/#effect-of-capture-location-interval-to-pcd-generation","title":"Effect of <code>Capture Location Interval</code> to PCD generation","text":"<p>If the <code>Capture Location Interval</code> is too small, it could result in a sparse PCD where some region of the map is captured well but the other regions aren't captured at all.</p> <p>In the below example, <code>Leaf Size</code> of 0.2 was used. Please note that using a different combination of <code>leaf size</code> and <code>Capture Location Interval</code> may result in a different PCD.</p> Capture Location Interval = 6 Capture Location Interval = 20 Capture Location Interval = 100"},{"location":"Components/Environment/CreatePCD/#capture-and-generate-pcd","title":"Capture and Generate PCD","text":"<p>If you play simulation with a scene prepared with the steps above, <code>PointCloudMapper</code> will automatically start mapping. The vehicle will warp along centerlines by intervals of <code>CaptureLocationInterval</code> and capture point cloud data. PCD file will be written when you stop your scene or all locations in the route are captured.</p> <p>If the Vehicle stops moving for longer and you see the following message in the bottom left corner - you can safely stop the scene.</p> <p></p> <p>The Point cloud <code>*.pcd</code> file is saved to the location you specified in the Point Cloud Mapper.</p>"},{"location":"Components/Environment/CreatePCD/#pcd-postprocessing","title":"PCD postprocessing","text":"<p>Install required tool</p> <p>The tool (<code>DownsampleLargePCD</code>) required for PCD conversion can be found under the link. README contains building instruction and usage.</p> <p>The generated PCD file is typically too large. Therefore you need to downsample it. Also, it should be converted to ASCII format because <code>Autoware</code> accepts only this format. <code>PointCloudMapper</code> returns PCD in binary format.</p> <ol> <li>Change the working directory to the location with <code>DownsampleLargePCD</code> tool.</li> <li> <p>Use this tool to downsample and save PCD in ASCII format.     </p><pre><code>./DownsampleLargePCD -in &lt;PATH_TO_INPUT_PCD&gt; -out &lt;PATH_TO_OUTPUT_PCD&gt; -leaf 0.2,0.2,0.2\n</code></pre> <ul> <li>Assuming input PCD is in your working directory and named <code>in_cloud.pcd</code> and output PCD is to be named <code>out_cloud.pcd</code> the command will be:     <pre><code>./DownsampleLargePCD -in in_cloud.pcd -out out_cloud.pcd -leaf 0.2,0.2,0.2\n</code></pre></li> <li>You can also save PCD in binary format by adding <code>-binary 1</code> option.</li> </ul> </li> <li> <p>Your PCD is ready to use.</p> </li> </ol> <p>Converting PCD format without downsampling</p> <p>If you don't want to downsample your PCD you can convert PCD file to ASCII format with <code>pcl_convert_pcd_ascii_binary</code> tool. This tool is available in the <code>pcl-tools</code> package and can be installed on Ubuntu with the following command: </p><pre><code>sudo apt install pcl-tools\n</code></pre> To convert your PCD use command: <pre><code>pcl_convert_pcd_ascii_binary &lt;PATH_TO_INPUT_PCD&gt; &lt;PATH_TO_OUTPUT_PCD&gt; 0\n</code></pre>"},{"location":"Components/Environment/CreatePCD/#verify-the-pcd","title":"Verify the PCD","text":"<p>To verify your PCD you can launch the Autoware with the PCD file specified.</p> <ol> <li> <p>Copy your PCD from the AWSIM project directory to the Autoware map directory.</p> <pre><code>cp &lt;PATH_TO_PCD_FILE&gt; &lt;PATH_TO_AUTOWARE_MAP&gt;/\n</code></pre> </li> <li> <p>Source the ROS and Autoware</p> <pre><code>source /opt/ros/humble/setup.bash\nsource &lt;PATH_TO_AUTOWARE&gt;/install/setup.bash\n</code></pre> </li> <li> <p>Launch the planning simulation with the map directory path (<code>map_path</code>) and PCD file (<code>pointcloud_map_file</code>) specified.</p> <p>PCD file location</p> <p>The PCD file needs to be located in the Autoware map directory and as a <code>pointcloud_map_file</code> parameter you only supply the file name, not the path.</p> <p>Absolute path</p> <p>When launching Autoware never use <code>~/</code> to specify the home directory. Either write the full absolute path ot use <code>$HOME</code> environmental variable.</p> <pre><code>ros2 launch autoware_launch planning_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit map_path:=&lt;ABSOLUTE_PATH_TO_AUTOWARE_MAP&gt; pointcloud_map_file:=&lt;PCD_FILE_NAME&gt;\n</code></pre> </li> <li> <p>Wait for the Autoware to finish loading and inspect the PCD visually given the Effect of Leaf Size and Effect of Capture Location Interval.</p> <p></p> </li> </ol>"},{"location":"Components/Environment/CreatePCD/#sample-scene","title":"Sample Scene","text":"<p><code>PointCloudMapping.unity</code> is a sample scene for <code>PointCloudMapper</code> showcase. It requires setup of OSM data and 3D model map of the area according to the steps above.</p> <p>Sample Mapping Scene</p> <p>In this example you can see a correctly configured Point Cloud Mapping Scene.</p> <p> </p>"},{"location":"Components/Environment/LaneletBoundsVisualizer/","title":"LaneletBoundsVisualizer","text":""},{"location":"Components/Environment/LaneletBoundsVisualizer/#lanelet-bounds-visualizer","title":"Lanelet Bounds Visualizer","text":"<p>Lanelet Bounds Visualizer is an Unity Editor extension allowing the user to load the left and right bounds of Lanelet to the Unity scene.</p>"},{"location":"Components/Environment/LaneletBoundsVisualizer/#usage","title":"Usage","text":"<p>The lanelet bounds load process can be performed by opening <code>AWSIM -&gt; Visualize -&gt; Load Lanelet Bounds</code> at the top toolbar of Unity Editor.</p> <p></p> <p>A window shown below will pop up. Select your <code>Osm Data Container</code> to specify which OSM data to load the Lanelet from.</p> <p></p> <p>The user can select whether to load the raw Lanelet or to adjust the resolution of the Lanelet by specifying the waypoint settings.</p> <p>To load the raw Lanelet, simply click the <code>Load Raw Lanelet</code> button.</p> <p>If the user wishes to change the resolution of the Lanelet, adjust the parameters of the <code>Waypoint Settings</code> as described below, and click the <code>Load with Waypoint Settings</code> button.</p> <ul> <li><code>Resolution</code> : resolution of resampling. Lower values provide better accuracy at the cost of processing time.</li> <li><code>Min Delta Length</code> : minimum length(m) between adjacent points.</li> <li><code>Min Delta Angle</code> : minimum angle(deg) between adjacent edges. Lowering this value produces a smoother curve.</li> </ul> <p>Once the Lanelet is successfully loaded, Lanelet bounds will be generated as a new GameObject named <code>LaneletBounds</code>.</p> <p>To visualize the <code>LaneletBounds</code>, make sure Gizmos is turned on and select the <code>LaneletBounds</code> GameObject. </p>"},{"location":"Components/Environment/LaneletBoundsVisualizer/#important-notes","title":"Important Notes","text":"<p>Generally speaking, visualizing Lanelet Bounds will result in a very laggy simulation. Therefore, it is recommended to hide the <code>LaneletBounds</code> GameObject when not used. The lag of the simulation becomes worse as you set the resolution of the Lanelet Bounds higher, so it is also recommended to set the resolution within a reasonable range.</p> <p>It is also important to note that no matter how high you set the resolution to be, it will not be any better than the original Lanelet (i.e. the raw data). Rather, the computational load will increase and the simulation will become more laggy. If the user wishes to get the highest quality of Lanelet Bounds, it is recommended to use the <code>Load Raw Lanelet</code> button.</p> <p>In short, <code>Waypoint Setting</code> parameters should be thought of as parameters to decrease the resolution from the original Lanelet to decrease the computational load and thus, reducing the lag of the simulation.</p> Higher Resolution Raw Lanelet Lower Resolution"},{"location":"Components/Environment/SmokeSimulator/","title":"SmokeSimulator","text":""},{"location":"Components/Environment/SmokeSimulator/#smoke-simulator","title":"Smoke Simulator","text":"<p> Simulating smoke in AWSIM may be useful when one wants to simulate exhaust gases from vehicles, smoke from emergency flare, etc.</p> <p>In Unity, it is common to use Particle System to simulate smokes. However, smoke simulated by Particle System cannot be sensed by RGL in AWSIM although in reality, smokes are detected by LiDAR.</p> <p><code>Smoke Simulator</code> was developed to simulate smokes that can be detected by RGL in Unity. <code>Smoke Simulator</code> works by instantiating many small cubic GameObjects called <code>Smoke Particles</code> and allows each particle to be detected by RGL.</p> <p>This document describes how to use the <code>Smoke Simulator</code>.</p>"},{"location":"Components/Environment/SmokeSimulator/#setting-smoke-simulator","title":"Setting Smoke Simulator","text":"<p>1.Create an empty GameObject</p> <p>2.Attach <code>SmokeGenerator.cs</code> to the previously created GameObject.</p> <p></p> <p>3.Adjust the parameters of the <code>SmokeGenerator</code> as described below:</p> <ul> <li><code>Max Particle</code>: Specifies the maximum number of particles created by the <code>Smoke Generator</code></li> <li><code>Particle Range Radius</code>: Specifies the radius of a circle, centered at the GameObject, which defines the region in which <code>Smoke Particles</code> are generated in</li> <li><code>Particle Size</code>: Specifies the edge of a <code>Smoke Particle</code></li> <li><code>Average Lifetime</code>: Specifies the average lifetime of a <code>Smoke Particle</code></li> <li> <p><code>Variation Lifetime</code>: Specifies the variation of lifetime of <code>Smoke Particles</code>.</p> <p>The lifetime of a <code>Smoke Particle</code> is calculated as follows:</p> <pre><code>`lifetime` = `Average Lifetime` + Random.Range(-`Variation Lifetime`, `Variation Lifetime`)\n</code></pre> </li> </ul> <ul> <li><code>Physics</code>: These parameters can be adjusted to specify the behavior of the smoke particles.<ul> <li><code>Initial Plane Velocity</code>: Specifies the velocity of a <code>SmokeParticle</code> in the x-z plane</li> <li><code>Initial Vertical Velocity</code>: Specifies the velocity of a <code>SmokeParticle</code> in the vertical direction</li> <li><code>Plane Acceleration</code>: Specifies the acceleration of a <code>SmokeParticle</code> in the x-z plane</li> <li><code>Vertical Acceleration</code>: Specifies the acceleration of a <code>SmokeParticle</code> in the vertical direction</li> </ul> </li> </ul> <p>4.(Optional): You may also specify the <code>Material</code> of <code>Smoke Particles</code>. If this field is unspecified, a default material is used</p> <p></p>"},{"location":"Components/Environment/SmokeSimulator/#example-of-different-smokes","title":"Example of Different Smokes","text":""},{"location":"Components/Environment/SmokeSimulator/#thin-smoke","title":"Thin Smoke","text":""},{"location":"Components/Environment/SmokeSimulator/#heavy-smoke","title":"Heavy Smoke","text":""},{"location":"Components/Environment/V2I/","title":"V2I","text":""},{"location":"Components/Environment/V2I/#v2i-vehicle-to-infrastructure","title":"V2I (Vehicle-to-Infrastructure)","text":"<p><code>V2I</code> is a component that simulates V2I communication protocol which allows to exchange data between vehicles and road infrastructure. In the current version of AWSIM, the <code>V2I</code> component publishes information about traffic lights.</p>"},{"location":"Components/Environment/V2I/#how-to-add-v2i-to-the-environment","title":"How to add V2I to the environment","text":""},{"location":"Components/Environment/V2I/#assign-lanelet2-wayid-and-relationid-to-trafficlight-object","title":"Assign Lanelet2 WayID and RelationID to TrafficLight object","text":"<ol> <li> <p>Load items from lanelet2 following the instruction</p> </li> <li> <p>Verify if <code>Traffic Light Lanelet ID</code> component has been added to <code>Traffic Light</code> game objects. </p> </li> <li> <p>Verify if <code>WayID</code> and <code>RelationID</code> has been correctly assigned. You can use Vector Map Builder as presented below </p> </li> </ol>"},{"location":"Components/Environment/V2I/#add-manually-traffic-light-lanelet-id-component-alternatively","title":"Add manually <code>Traffic Light Lanelet ID</code> component (alternatively)","text":"<p>If for some reason, <code>Traffic Light Lanelet ID</code> component is not added to <code>Traffic Light</code> object.</p> <ol> <li> <p>Add component manually</p> <p></p> </li> <li> <p>Fill Way ID</p> <p></p> </li> <li> <p>Fill Relation ID</p> <p></p> </li> </ol>"},{"location":"Components/Environment/V2I/#add-v2i-prefab","title":"Add V2I prefab","text":""},{"location":"Components/Environment/V2I/#select-ego-transform","title":"Select EGO transform","text":""},{"location":"Components/Environment/V2I/#parameters","title":"Parameters","text":"Name Type Description Output Hz int Topic publication frequency Ego Vehicle Transform transform Ego Vehicle object transform Ego Distance To Traffic Signals double Maximum distance between Traffic Light and Ego Traffic Signal ID enum Possibility to select if as <code>traffic_signal_id</code> field in msg is <code>Relation ID</code> or <code>Way ID</code> Traffic Signals Topic string Topic name <p>Note</p> <p>V2I feature can be used as Traffic Light ground truth information, and for that usage <code>Way ID</code> is supposed to be selected.</p>"},{"location":"Components/ROS2/AddACustomROS2Message/","title":"Add custom ROS2 msg","text":""},{"location":"Components/ROS2/AddACustomROS2Message/#add-a-custom-ros2-message","title":"Add a custom ROS2 message","text":"<p>If you want to use custom message in AWSIM, you need to generate the appropriate files, to do this you have to build <code>ROS2ForUnity</code> yourself - please follow the steps below. Remember to start with prerequisities though.</p> <p>ROS2ForUnity role</p> <p>For a better understanding of the role of <code>ROS2ForUnity</code> and the messages used, we encourage you to read this section.</p> <p>custom_msgs</p> <p>In order to simplify this tutorial, the name of the package containing the custom message is assumed to be <code>custom_msgs</code> - remember to replace it with the name of your package.</p>"},{"location":"Components/ROS2/AddACustomROS2Message/#prerequisites","title":"Prerequisites","text":"<p><code>ROS2ForUnity</code> depends on a ros2cs - a C# .NET library for ROS2. This library is already included so you don't need to install it, but there are a few prerequisites that must be resolved first.</p> <p>Please select your system and resolve all prerequisites:</p> UbuntuWindows <ul> <li><code>ros2cs</code> prerequisites for Ubuntu</li> <li>ROS2 version is <code>humble</code> and is located in <code>/opt/ros/humble</code></li> <li>Your package with custom message is located in the home directory <code>~/custom_msgs</code> or is hosted on git repository.</li> <li>Shell - commands have to be executed from the <code>bash</code> shell</li> </ul> <ul> <li> <p><code>ros2cs</code> prerequisites for Windows</p> <p>Question</p> <p>Tests are not working (<code>'charmap'</code> codec can't decode byte) on Windows - look at troubleshooting here</p> </li> </ul> <ul> <li>ROS2 version is <code>humble</code> and is located in  <code>C:\\ros2_humble</code></li> <li>Your package with custom message package is located in the home directory <code>C:\\custom_msgs</code> or is hosted on git repository.</li> <li>Shell - commands should be executed from the <code>powershell</code> shell</li> </ul>"},{"location":"Components/ROS2/AddACustomROS2Message/#1-workspace-preparation","title":"1. Workspace preparation","text":"<ol> <li> <p>Clone <code>ROS2ForUnity</code> repository by execute command:</p> UbuntuWindows <pre><code>git clone https://github.com/RobotecAI/ros2-for-unity ~/\n</code></pre> <p>Warning</p> <p>The cloned <code>ROS 2 For Unity</code> repository must be located in the home directory <code>~/</code>.</p> <pre><code>git clone https://github.com/RobotecAI/ros2-for-unity /C\n</code></pre> <p>Warning</p> <p>The cloned <code>ROS 2 For Unity</code> repository must be located in the home directory <code>C:\\</code>.</p> </li> <li> <p>Pull dependent repositories by execute commands:</p> UbuntuWindows <pre><code>cd ~/ros2-for-unity\n. /opt/ros/humble/setup.bash\n./pull_repositories.sh\n</code></pre> <pre><code>cd C:\\ros2-for-unity\nC:\\ros2_humble\\local_setup.ps1\n.\\pull_repositories.ps1\n</code></pre> </li> </ol>"},{"location":"Components/ROS2/AddACustomROS2Message/#2-setup-custom_msgs-package","title":"2. Setup <code>custom_msgs</code> package","text":"<p>The method to add a custom package to build depends on where it is located. The package can be on your local machine or just be hosted on a git repository. Please, choose the appropriate option and follow the instructions.</p>"},{"location":"Components/ROS2/AddACustomROS2Message/#21-package-contained-on-local-machine","title":"2.1. Package contained on local machine","text":"<ol> <li> <p>Copy the <code>custom_msgs</code> package with custom message to the folder to <code>src/ros2cs/custom_messages</code> directory</p> UbuntuWindows <pre><code>cp -r ~/custom_msgs ~/ros2-for-unity/src/ros2cs/custom_messages/\n</code></pre> <pre><code>Copy-Item 'C:\\custom_msgs' -Destination 'C:\\ros2-for-unity\\src\\custom_messages'\n</code></pre> </li> </ol>"},{"location":"Components/ROS2/AddACustomROS2Message/#22-package-hosted-on-git-repository","title":"2.2. Package hosted on git repository","text":"<ol> <li>Open <code>ros2-for-unity/ros2_for_unity_custom_messages.repos</code> file in editor.</li> <li> <p>Modify the contents of the file shown below, uncomment and set:</p> <ul> <li><code>&lt;package_name&gt;</code> - to your package name - so in this case <code>custom_msgs</code>,</li> <li><code>&lt;repo_url&gt;</code> - to repository address,  </li> <li><code>&lt;repo_branch&gt;</code> - to desired branch. <pre><code>repositories:\n#  src/ros2cs/custom_messages/&lt;package_name&gt;:\n#    type: git\n#    url: &lt;repo_url&gt;\n#    version: &lt;repo_branch&gt;\n</code></pre></li> </ul> <p>Example</p> <p>Below is an example of a file configured to pull 2 packages (<code>custom_msgs</code>,<code>autoware_msgs</code>) of messages hosted on a git repository. </p><pre><code># NOTE: Use this file if you want to build with custom messages that reside in a separate remote repo.\n# NOTE: use the following format\n\nrepositories:\n    src/ros2cs/custom_messages/custom_msgs:\n        type: git\n        url: https://github.com/tier4/custom_msgs.git\n        version: main\n    src/ros2cs/custom_messages/autoware_msgs:\n        type: git\n        url: https://github.com/tier4/autoware_msgs.git\n        version: tier4/main\n</code></pre> </li> <li> <p>Now pull the repositories again (also the <code>custom_msgs</code> package repository)</p> UbuntuWindows <pre><code>cd ~/ros2-for-unity\n./pull_repositories.sh\n</code></pre> <pre><code>cd C:\\ros2-for-unity\n.\\pull_repositories.ps1\n</code></pre> </li> </ol>"},{"location":"Components/ROS2/AddACustomROS2Message/#3-build-ros-2-for-unity","title":"3. Build ROS 2 For Unity","text":"<p>Build <code>ROS2ForUnity</code> with custom message packages using the following commands:</p> UbuntuWindows <pre><code>cd ~/ros2-for-unity\n./build.sh --standalone\n</code></pre> <pre><code>cd C:\\ros2-for-unity\n.\\build.ps1 -standalone\n</code></pre>"},{"location":"Components/ROS2/AddACustomROS2Message/#4-install-custom_msgs-to-awsim","title":"4. Install <code>custom_msgs</code> to AWSIM","text":"<p>New <code>ROS2ForUnity</code> build, which you just made in step 3, contains multiple libraries that already exist in the AWSIM. To install <code>custom_msgs</code> and not copy all other unnecessary files, you should get the <code>custom_msgs</code> related libraries only.</p> <p>You can find them in following directories and simply copy to the analogous directories in <code>AWSIM/Assets/Ros2ForUnity</code> folder, or use the script described here.</p> UbuntuWindows <ul> <li><code>ros2-for-unity/install/asset/Ros2ForUnity/Plugins</code> which names matches <code>custom_msgs_*</code></li> <li><code>ros2-for-unity/install/asset/Ros2ForUnity/Plugins/Linux/x86_64/</code> which names matches <code>libcustom_msgs_*</code></li> </ul> <ul> <li><code>ros2-for-unity/install/asset/Ros2ForUnity/Plugins</code> which names matches <code>custom_msgs_*</code></li> <li><code>ros2-for-unity/install/asset/Ros2ForUnity/Plugins/Windows/x86_64/</code> which names matches <code>custom_msgs_*</code></li> </ul>"},{"location":"Components/ROS2/AddACustomROS2Message/#automation-of-copying-message-files","title":"Automation of copying message files","text":"UbuntuWindows <p>To automate the process, you can use a script that copies all files related to your <code>custom_msgs</code> package.</p> <ol> <li>Create a file named <code>copy_custom_msgs.sh</code> in directory <code>~/ros2-for-unity/</code> and paste the following content into it. <pre><code>#!/bin/bash\necho \"CUSTOM_MSGS_PACKAGE_NAME: $1\"\necho \"AWSIM_DIR_PATH: $2\"\nfind ./install/asset/Ros2ForUnity/Plugins -maxdepth 1 -name \"$1*\"    -type f -exec cp {} $2/Assets/Ros2ForUnity/Plugins \\;\nfind ./install/asset/Ros2ForUnity/Plugins/Linux/x86_64 -maxdepth 1 -name     \"lib$1*\" -type f -exec cp {} $2/Assets/Ros2ForUnity/Plugins/Linux/x86_64 \\;\n</code></pre></li> <li>Save the file and give it executable rights with the command: <pre><code>chmod a+x copy_msgs.sh\n</code></pre></li> <li>Run the script with two arguments: <pre><code>./copy_custom_msgs.sh &lt;CUSTOM_MSGS_PACKAGE_NAME&gt; &lt;AWSIM_DIR_PATH&gt;\n</code></pre></li> </ol> <ul> <li><code>&lt;CUSTOM_MSGS_PACKAGE_NAME&gt;</code> - the first one which is the name of the package with messages - in this case <code>custom_msgs</code>,</li> <li><code>&lt;AWSIM_DIR_PATH&gt;</code> - the second which is the path to the cloned AWSIM repository.</li> </ul> <p>Example</p> <pre><code>./copy_custom_msgs.sh custom_msgs ~/unity/AWSIM/\n</code></pre> <p>To automate the process, you can use these commands with changed:</p> <ul> <li><code>&lt;CUSTOM_MSGS_PACKAGE_NAME&gt;</code> - the name of your package with messages - in this case <code>custom_msgs</code>,</li> <li><code>&lt;AWSIM_DIR_PATH&gt;</code> - to path to the cloned AWSIM repository <pre><code>Get-ChildItem C:\\ros2-for-unity\\install\\asset\\Ros2ForUnity\\Plugins\\* -Include @('&lt;CUSTOM_MSGS_PACKAGE_NAME&gt;*') | Copy-Item -Destination &lt;AWSIM_DIR_PATH&gt;\\Assets\\Ros2ForUnity\\Plugins\nGet-ChildItem C:\\ros2-for-unity\\install\\asset\\Ros2ForUnity\\Plugins\\Windows\\x86_64\\* -Include @('&lt;CUSTOM_MSGS_PACKAGE_NAME&gt;*') | Copy-Item -Destination &lt;AWSIM_DIR_PATH&gt;\\Assets\\Ros2ForUnity\\Plugins\\Windows\\x86_64\n</code></pre></li> </ul> <p>Example</p> <pre><code>Get-ChildItem C:\\ros2-for-unity\\install\\asset\\Ros2ForUnity\\Plugins\\* -Include @('custom_msgs*') | Copy-Item -Destination C:\\unity\\AWSIM\\Assets\\Ros2ForUnity\\Plugins\nGet-ChildItem C:\\ros2-for-unity\\install\\asset\\Ros2ForUnity\\Plugins\\Windows\\x86_64\\* -Include @('custom_msgs*') | Copy-Item -Destination C:\\unity\\AWSIM\\Assets\\Ros2ForUnity\\Plugins\\Windows\\x86_64\n</code></pre>"},{"location":"Components/ROS2/AddACustomROS2Message/#5-test","title":"5. Test","text":"<p>Make sure that the package files <code>custom_msgs</code> have been properly copied to the <code>AWSIM/Assets/Ros2ForUnity</code>. Then try to create a message object as described in this section and check in the console of Unity Editor if it compiles without errors.</p>"},{"location":"Components/ROS2/ROS2ForUnity/","title":"ROS2 For Unity","text":""},{"location":"Components/ROS2/ROS2ForUnity/#ros2-for-unity","title":"ROS2 For Unity","text":"<p>Ros2ForUnity (<code>R2FU</code>) module is a communication solution that effectively connects Unity and the ROS2 ecosystem, maintaining a strong integration. Unlike other solutions, it doesn't rely on bridging communication but rather utilizes the ROS2 middleware stack (specifically the <code>rcl</code> layer and below), enabling the inclusion of ROS2 nodes within Unity simulations.</p> <p><code>R2FU</code> is used in AWSIM for many reasons. First of all, because it offers high-performance integration between Unity and ROS2, with improved throughput and lower latencies compared to bridging solutions. It provides real ROS2 functionality for simulation entities in Unity, supports standard and custom messages, and includes convenient abstractions and tools, all wrapped as a Unity asset. For a detailed description, please see README.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#prerequisites","title":"Prerequisites","text":"<p>This asset can be prepared in two flavours:</p> <ul> <li>standalone mode - where no ROS2 installation is required on target machine, e.g., your Unity simulation server. All required dependencies are installed and can be used e.g. as a complete set of Unity plugins.</li> <li>overlay mode - where the ROS2 installation is required on target machine. Only asset libraries and generated messages are installed therefore ROS2 instance must be sourced.</li> </ul> <p>By default, asset <code>R2FU</code> in AWSIM is prepared in standalone mode.</p> <p>Warning</p> <p>To avoid internal conflicts between the standalone libraries, and sourced ones, ROS2 instance shouldn't be sourced before running AWSIM or the Unity Editor.</p> <p>Can't see topics</p> <p>There are no errors but I can't see topics published by <code>R2FU</code></p> <ul> <li>Make sure your DDS (Localhost settings) config is correct.</li> <li>Sometimes ROS2 daemon brakes up when changing network interfaces or ROS2 version.</li> </ul> <p>Try to stop it forcefully (<code>pkill -9 ros2_daemon</code>) and restart (<code>ros2 daemon start</code>).</p>"},{"location":"Components/ROS2/ROS2ForUnity/#concept","title":"Concept","text":"<p>Describing the concept of using <code>R2FU</code> in AWSIM, we distinguish:</p> <ul> <li>ROS2Node - it is the equivalent of a node in ROS2, it has its own name, it can have any number of subscribers, publishers, service servers and service clients. In the current AWSIM implementation, there is only one main node.</li> <li>ROS2Clock - it is responsible for generating the simulation time using the selected source.</li> <li>SimulatorROS2Node - it is a class that is directly responsible for AWSIM&lt;-&gt;ROS2 communication, and contains one instance each of ROS2Node and ROS2Clock, so it creates the main AWSIM node in ROS2 and simulates the time from the selected source (currently UnityTimeSource is used). It is initialized at the moment of running the scene in Unity - thanks to the <code>RuntimeInitializeOnLoadMethod</code> mark.</li> <li>Publisher - it is the equivalent of the publisher in ROS2, it uses a single topic on which it can publish a selected type of message, and it has a selected QoS profile. Each publisher in AWSIM is created in ROS2Node object of class SimulatorROS2Node.</li> <li>Subscriber - it is the equivalent of the subscriber in ROS2, it uses a single topic from which it subscribes to the selected type of message, and it has a selected QoS profile. Each subscriber in AWSIM is created in ROS2Node object of class SimulatorROS2Node.</li> </ul> <p>The SimulatorROS2Node implementation, thanks to the use of <code>R2FU</code>, allows you to add communication via ROS2 to any Unity component. For example, we can receive control commands from any other ROS2 node and publish the current state of Ego, such as its position in the environment.</p> <p>Simulation time</p> <p>If you want to use system time (ROS2 time) instead of Unity time, use <code>ROS2TimeSource</code> instead of <code>UnityTimeSource</code> in the <code>SimulatorROS2Node</code> class.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#package-structure","title":"Package structure","text":"<p><code>Ros2ForUnity</code> asset contains:</p> <ul> <li>Plugins - dynamically loaded libraries for Windows and Linux (<code>*.dll</code> and <code>*.so</code> files). In addition to the necessary libraries, here are the libraries created as a result of generation the types of ROS2 messages that are used in communication.</li> <li>Scripts - scripts for using <code>R2FU</code>  in Unity - details below.</li> <li>Extension Scripts - scripts for using <code>R2FU</code> in AWSIM, provide abstractions of a single main Node and simplify the interface  - details below. These scripts are not in the library itself, but directly in the directory <code>Assets/AWSIM/Scripts/ROS/**</code>.</li> </ul>"},{"location":"Components/ROS2/ROS2ForUnity/#scripts","title":"Scripts","text":"<ul> <li><code>ROS2UnityCore</code> - the principal class for handling ROS2 nodes and executables. Spins and executes actions (e.g. clock, sensor publish triggers) in a dedicated thread.</li> <li><code>ROS2UnityComponent</code> - <code>ROS2UnityCore</code> adapted to work as a Unity component.</li> <li><code>ROS2Node</code> - a class representing a ROS2 node, it should be constructed through <code>ROS2UnityComponent</code> class, which also handles spinning.</li> <li><code>ROS2ForUnity</code> - an internal class responsible for handling checking, proper initialization and shutdown of ROS2 communication,</li> <li><code>ROS2ListenerExample</code> - an example class provided for testing of basic ROS2-&gt;Unity communication.</li> <li><code>ROS2TalkerExample</code> - an example class provided for testing of basic Unity-&gt;ROS2 communication.</li> <li><code>ROS2PerformanceTest</code> - an example class provided for performance testing of ROS2&lt;-&gt;Unity communication.</li> <li><code>Sensor</code> - an abstract base class for ROS2-enabled sensor.</li> <li><code>Transformations</code> - a set of transformation functions between coordinate systems of Unity and ROS2.</li> <li><code>PostInstall</code> - an internal class responsible for installing <code>R2FU</code> metadata files.</li> <li><code>Time</code> scripts - a set of classes that provide  the ability to use different time sources:<ul> <li><code>ROS2Clock</code> - ROS2 clock class that for interfacing between a time source (Unity or ROS2 system time) and ROS2 messages.</li> <li><code>ROS2TimeSource</code> - acquires ROS2 time (system time by default).</li> <li><code>UnityTimeSource</code> - acquires Unity time.</li> <li><code>DotnetTimeSource</code> - acquires Unity <code>DateTime</code> based clock that has resolution increased using <code>Stopwatch</code>.</li> <li><code>ITimeSource</code> - interface for general time extraction from any source.</li> <li><code>TimeUtils</code> - utils for time conversion.</li> </ul> </li> </ul>"},{"location":"Components/ROS2/ROS2ForUnity/#extension-scripts","title":"Extension Scripts","text":"<p>Additionally, in order to adapt AWSIM to the use of <code>R2FU</code>, the following scripts are used:</p> <ul> <li><code>SimulatorROS2Node</code> - it is a class that is directly responsible for AWSIM&lt;-&gt;ROS2 communication.</li> <li> <p><code>ClockPublisher</code> - allows the publication of the simulation time from the clock running in the SimulatorROS2Node. It must be added as a component to the scene in order to publish the current time when the scene is run.</p> <p></p> </li> </ul> <ul> <li><code>QoSSettings</code> - it is the equivalent of ROS2 QoS, which allows to specify the QoS for subscribers and publishers in AWSIM. It uses the <code>QualityOfServiceProfile</code> implementation from the Ros2cs library.</li> <li><code>ROS2Utility</code> - it is a class with utils that allow, for example, to convert positions in the ROS2 coordinate system to the AWSIM coordinate system.</li> <li><code>DiagnosticsManager</code> - prints diagnostics for desired elements described in <code>*.yaml</code> config file.</li> </ul>"},{"location":"Components/ROS2/ROS2ForUnity/#default-message-types","title":"Default message types","text":"<p>The basic ROS2 msgs types that are supported in AWSIM by default include:</p> <ul> <li>common_interfaces:<ul> <li><code>std_msgs</code>.</li> <li><code>geometry_msgs</code>,</li> <li><code>sensor_msgs</code>,</li> <li><code>nav_msgs</code>,</li> <li><code>diagnostic_msgs</code>,</li> </ul> </li> <li>rcl_interfaces:<ul> <li><code>builtin_interfaces</code>,</li> <li><code>action_msgs</code>,</li> <li><code>rosgraph_msgs</code>,</li> <li><code>test_msgs</code>.</li> </ul> </li> <li>autoware_msgs:<ul> <li><code>autoware_common_msgs</code>,</li> <li><code>autoware_control_msgs</code>,</li> <li><code>autoware_localization_msgs</code>,</li> <li><code>autoware_map_msgs</code>,</li> <li><code>autoware_perception_msgs</code>,</li> <li><code>autoware_planning_msgs</code>,</li> <li><code>autoware_sensing_msgs</code>,</li> <li><code>autoware_system_msgs</code>,</li> <li><code>autoware_vehicle_msgs</code>.</li> </ul> </li> <li>tier4_autoware_msgs:<ul> <li><code>tier4_control_msgs</code>,</li> <li><code>tier4_vehicle_msgs</code>.</li> </ul> </li> <li>Others:<ul> <li><code>tf2_msgs</code>,</li> <li><code>unique_identifier_msgs</code>.</li> </ul> </li> </ul> <p>In order for the message package to be used in Unity, its <code>*.dll</code> and <code>*.so</code> libraries must be generated using <code>R2FU</code>.</p> <p>Custom message</p> <p>If you want to generate a custom message to allow it to be used in AWSIM please read this tutorial.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#use-of-generated-messages-in-unity","title":"Use of generated messages in Unity","text":"<p>Each message type is composed of other types - which can also be a complex type. All of them are based on built-in C# types. The most common built-in types in messages are <code>bool</code>, <code>int</code>, <code>double</code> and <code>string</code>. These types have their communication equivalents using ROS2.</p> <p>A good example of a complex type that is added to other complex types in order to specify a reference - in the form of a timestamp and a frame - is std_msgs/Header. This message has the following form:</p> <pre><code>builtin_interfaces/msg/Time stamp\nstring frame_id\n</code></pre> <p>ROS2 directive</p> <p>In order to work with ROS2 in Unity, remember to add the directive <code>using ROS2;</code> at the top of the file to import types from this namespace.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#create-an-object","title":"Create an object","text":"<p>The simplest way to create an object of <code>Header</code> type is:</p> <pre><code>var header = new std_msgs.msg.Header()\n{\n    Frame_id = \"map\"\n}\n</code></pre> <p>It is not required to define the value of each field. As you can see, it creates an object, filling only <code>frame_id</code> field - and left the field of complex <code>builtin_interfaces/msg/Time</code> type initialized by default. Time is an important element of any message, how to fill it is written here.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#accessing-and-filling-in-message-fields","title":"Accessing and filling in message fields","text":"<p>As you might have noticed in the previous example, a ROS2 message in Unity is just a structure containing the same fields - keep the same names and types. Access to its fields for reading and filling is the same as for any C# structure.</p> <pre><code>var header2 = new std_msgs.msg.Header();\nheader2.Frame_id = \"map\";\nheader2.Stamp.sec = \"1234567\";\nDebug.Log($\"StampSec: {header2.Stamp.sec} and Frame: {header2.Frame_id}\");\n</code></pre> <p>Field names</p> <p>There is one always-present difference in field names. The first letter of each message field in Unity is always uppercase - even if the base ROS2 message from which it is generated is lowercase.</p>"},{"location":"Components/ROS2/ROS2ForUnity/#filling-a-time","title":"Filling a time","text":"<p>In order to complete the time field of the <code>Header</code> message, we recommend the following methods in AWSIM:</p> <ol> <li> <p>When the message has no <code>Header</code> but only the <code>Time</code> type:</p> <pre><code>var header2 = new std_msgs.msg.Header();\nheader2.Stamp = SimulatorROS2Node.GetCurrentRosTime();\n</code></pre> </li> <li> <p>When the message has a <code>Header</code> - like for example  autoware_vehicle_msgs/VelocityReport:</p> <pre><code>velocityReportMsg = new autoware_vehicle_msgs.msg.VelocityReport()\n{\n    Header = new std_msgs.msg.Header()\n    {\n        Frame_id = \"map\",\n    }\n};\nvar velocityReportMsgHeader = velocityReportMsg as MessageWithHeader;\nSimulatorROS2Node.UpdateROSTimestamp(ref velocityReportMsgHeader);\n</code></pre> </li> </ol> <p>These methods allow to fill the <code>Time</code> field in the message object with the simulation time - from ROS2Clock</p>"},{"location":"Components/ROS2/ROS2ForUnity/#create-a-message-with-array","title":"Create a message with array","text":"<p>Some message types contain an array of some type. An example of such a message is <code>nav_msgs/Path</code>, which has a <code>PoseStamped</code> array. In order to fill such an array, you must first create a <code>List&lt;T&gt;</code>, fill it and then convert it to a raw array.</p> <pre><code>var posesList = new List&lt;geometry_msgs.msg.PoseStamped&gt;();\nfor(int i=0; i&lt;=5;++i)\n{\n    var poseStampedMsg = new geometry_msgs.msg.PoseStamped();\n    poseStampedMsg.Pose.Position.X = i;\n    poseStampedMsg.Pose.Position.Y = 5-i;\n    var poseStampedMsgHeader = poseStampedMsg as MessageWithHeader;\n    SimulatorROS2Node.UpdateROSTimestamp(ref poseStampedMsgHeader);\n    posesList.Add(poseStampedMsg);\n}\nvar pathMsg = new nav_msgs.msg.Path(){Poses=posesList.ToArray()};\nvar pathMsgHeader = pathMsg as MessageWithHeader;\nSimulatorROS2Node.UpdateROSTimestamp(ref pathMsgHeader);\n// pathMsg is ready\n</code></pre>"},{"location":"Components/ROS2/ROS2ForUnity/#publish-on-the-topic","title":"Publish on the topic","text":"<p>In order to publish messages, a publisher object must be created. The static method <code>CreatePublisher</code> of the <code>SimulatorROS2Node</code> makes it easy. You must specify the type of message, the topic on which it will be published and the QoS profile. Below is an example of <code>autoware_vehicle_msgs.msg.VelocityReport</code> type message publication with a frequency of <code>30Hz</code> on <code>/vehicle/status/velocity_status</code> topic, the QoS profile is <code>(Reliability=Reliable, Durability=Volatile, History=Keep last, Depth=1</code>):</p> <pre><code>using UnityEngine;\nusing ROS2;\n\nnamespace AWSIM\n{\n    public class VehicleReportRos2Publisher : MonoBehaviour\n    {\n        float timer = 0;\n        int publishHz = 30;\n        QoSSettings qosSettings = new QoSSettings()\n        {\n            ReliabilityPolicy = ReliabilityPolicy.QOS_POLICY_RELIABILITY_RELIABLE,\n            DurabilityPolicy = DurabilityPolicy.QOS_POLICY_DURABILITY_VOLATILE,\n            HistoryPolicy = HistoryPolicy.QOS_POLICY_HISTORY_KEEP_LAST,\n            Depth = 1,\n        };\n        string velocityReportTopic = \"/vehicle/status/velocity_status\";\n        autoware_vehicle_msgs.msg.VelocityReport velocityReportMsg;\n        IPublisher&lt;autoware_vehicle_msgs.msg.VelocityReport&gt; velocityReportPublisher;\n\n        void Start()\n        {\n            // Create a message object and fill in the constant fields\n            velocityReportMsg = new autoware_vehicle_msgs.msg.VelocityReport()\n            {\n                Header = new std_msgs.msg.Header()\n                {\n                    Frame_id = \"map\",\n                }\n            };\n\n            // Create publisher with specific topic and QoS profile\n            velocityReportPublisher = SimulatorROS2Node.CreatePublisher&lt;autoware_vehicle_msgs.msg.VelocityReport&gt;(velocityReportTopic, qosSettings.GetQoSProfile());\n        }\n\n         bool NeedToPublish()\n        {\n            timer += Time.deltaTime;\n            var interval = 1.0f / publishHz;\n            interval -= 0.00001f;\n            if (timer &lt; interval)\n                return false;\n            timer = 0;\n            return true;\n        }\n\n        void FixedUpdate()\n        {\n            // Provide publications with a given frequency\n            if (NeedToPublish())\n            {\n                // Fill in non-constant fields\n                velocityReportMsg.Longitudinal_velocity = 1.00f;\n                velocityReportMsg.Lateral_velocity = 0.00f;\n                velocityReportMsg.Heading_rate = 0.00f;\n\n                // Update Stamp\n                var velocityReportMsgHeader = velocityReportMsg as MessageWithHeader;\n                SimulatorROS2Node.UpdateROSTimestamp(ref velocityReportMsgHeader);\n\n                // Publish\n                velocityReportPublisher.Publish(velocityReportMsg);\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Components/ROS2/ROS2ForUnity/#upper-limit-to-publish-rate","title":"Upper limit to publish rate","text":"<p>The above example demonstrates the implementation of the <code>'publish'</code> method within the <code>FixedUpdate</code> Unity event method. However, this approach has certain limitations. The maximum output frequency is directly tied to the current value of <code>Fixed TimeStep</code> specified in the <code>Project Settings</code>. Considering that the AWSIM is targeting a frame rate of 60 frames per second (FPS), the current <code>Fixed TimeStep</code> is set to 1/60s. And this impose 60Hz as a limitation on the publish rate for any sensor, which is implemented within <code>FixedUpdate</code> method. In case a higher output frequency be necessary, an alternative implementation must be considered or adjustments made to the <code>Fixed TimeStep</code> setting in the <code>Editor-&gt;Project Settings-&gt;Time</code>.</p> <p>The table provided below presents a list of sensors along with examples of topics that are constrained by the <code>Fixed TimeStep</code> limitation.</p> Object Topic GNSS Sensor /sensing/gnss/pose IMU Sensor /sensing/imu/tamagawa/imu_raw Traffic Camera /sensing/camera/traffic_light/image_raw Pose Sensor /awsim/ground_truth/vehicle/pose OdometrySensor /awsim/ground_truth/localization/kinematic_state LIDAR /sensing/lidar/top/pointcloud_raw Vehicle Status /vehicle/status/velocity_status <p>If the sensor or any other publishing object within AWSIM does not have any direct correlation with physics (i.e., does not require synchronization with physics), it can be implemented without using the <code>FixedUpdate</code> method. Consequently, this allows the bypass of upper limits imposed by the <code>Fixed TimeStep</code>.</p> <p>The table presented below shows a list of objects that are not constrained by the <code>Fixed TimeStep</code> limitation.</p> Object Topic Clock /clock"},{"location":"Components/ROS2/ROS2ForUnity/#subscribe-to-the-topic","title":"Subscribe to the topic","text":"<p>In order to subscribe messages, a subscriber object must be created. The static method <code>CreateSubscription</code> of the <code>SimulatorROS2Node</code> makes it easy. You must specify the type of message, the topic from which it will be subscribed and the QoS profile. In addition, the callback must be defined, which will be called when the message is received - in particular, it can be defined as a lambda expression. Below is an example of <code>std_msgs.msg.Bool</code> type message subscription on <code>/vehicle/is_vehicle_stopped</code> topic, the QoS profile is <code>\u201csystem default\u201d</code>: </p><pre><code>using UnityEngine;\nusing ROS2;\n\nnamespace AWSIM\n{\n    public class VehicleStoppedSubscriber : MonoBehaviour\n    {\n        QoSSettings qosSettings = new QoSSettings();\n        string isVehicleStoppedTopic = \"/vehicle/is_vehicle_stopped\";\n        bool isVehicleStopped = false;\n        ISubscription&lt;std_msgs.msg.Bool&gt; isVehicleStoppedSubscriber;\n\n        void Start()\n        {\n            isVehicleStoppedSubscriber = SimulatorROS2Node.CreateSubscription&lt;std_msgs.msg.Bool&gt;(isVehicleStoppedTopic, VehicleStoppedCallback, qosSettings.GetQoSProfile());\n        }\n\n        void VehicleStoppedCallback(std_msgs.msg.Bool msg)\n        {\n            isVehicleStopped = msg.Data;\n        }\n\n        void OnDestroy()\n        {\n            SimulatorROS2Node.RemoveSubscription&lt;std_msgs.msg.Bool&gt;(isVehicleStoppedSubscriber);\n        }\n    }\n}\n</code></pre>"},{"location":"Components/ROS2/ROS2TopicList/","title":"ROS2 topic list","text":""},{"location":"Components/ROS2/ROS2TopicList/#ros2-topic-list","title":"ROS2 topic list","text":"<p>The following is a summary of the ROS2 topics that the AWSIM node subscribes to and publishes on.</p> <p>Ros2ForUnity</p> <p>AWSIM works with ROS2 thanks to the use of <code>Ros2ForUnity</code> - read the details here. If you want to generate a custom message to allow it to be used in AWSIM please read this tutorial.</p>"},{"location":"Components/ROS2/ROS2TopicList/#list-of-subscribers","title":"List of subscribers","text":"Category Topic Message type <code>frame_id</code> <code>Hz</code> <code>QoS</code> <p>Control</p> Ackermann Control <code>/control/command/control_cmd</code> <code>autoware_control_msgs/msg/Control</code> - <code>60</code> <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code> Gear <code>/control/command/gear_cmd</code> <code>autoware_vehicle_msgs/msg/GearCommand</code> - <code>10</code> <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code> Turn Indicators <code>/control/command/turn_indicators_cmd</code> <code>autoware_vehicle_msgs/TurnIndicatorsCommand</code> - <code>10</code> <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code> Hazard Lights <code>/control/command/hazard_lights_cmd</code> <code>autoware_vehicle_msgs/HazardLightsCommand</code> - <code>10</code> <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code> Emergency <code>/control/command/emergency_cmd</code> <code>tier4_vehicle_msgs/msg/VehicleEmergencyStamped</code> - <code>60</code> <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code>"},{"location":"Components/ROS2/ROS2TopicList/#list-of-publishers","title":"List of publishers","text":"Category Topic Message type <code>frame_id</code> <code>Hz</code> <code>QoS</code> <p>Clock</p> <code>/clock</code> <code>rosgraph_msgs/Clock</code> - <code>100</code> <code>Best effort</code>,<code>Volatile</code>,<code>Keep last/1</code> <p>Sensors</p> Camera <code>/sensing/camera/traffic_light/camera_info</code> <code>sensor_msgs/CameraInfo</code> <code>traffic_light_left_camera/camera_link</code> <code>10</code> <code>Best effort</code>,<code>Volatile</code>,<code>Keep last/1</code> Camera <code>/sensing/camera/traffic_light/image_raw</code> <code>sensor_msgs/Image</code> <code>traffic_light_left_camera/camera_link</code> <code>10</code> <code>Best effort</code>,<code>Volatile</code>,<code>Keep last/1</code> GNSS <code>/sensing/gnss/pose</code> <code>geometry_msgs/Pose</code> <code>gnss_link</code> <code>1</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> GNSS <code>/sensing/gnss/pose_with_covariance</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> <code>gnss_link</code> <code>1</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> IMU <code>/sensing/imu/tamagawa/imu_raw</code> <code>sensor_msgs/Imu</code> <code>tamagawa/imu_link</code> <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1000</code> Top LiDAR <code>/sensing/lidar/top/pointcloud_raw</code> <code>sensor_msgs/PointCloud2</code> <code>sensor_kit_base_link</code> <code>10</code> <code>Best effort</code>,<code>Volatile</code>,<code>Keep last/5</code> Top LiDAR <code>/sensing/lidar/top/pointcloud_raw_ex</code> <code>sensor_msgs/PointCloud2</code> <code>sensor_kit_base_link</code> <code>10</code> <code>Best effort</code>,<code>Volatile</code>,<code>Keep last/5</code> <p>Vehicle Status</p> Velocity <code>/vehicle/status/velocity_status</code> <code>autoware_vehicle_msgs/VelocityReport</code> <code>base_line</code> <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> Steering <code>/vehicle/status/steering_status</code> <code>autoware_vehicle_msgs/SteeringReport</code> - <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> Control Mode <code>/vehicle/status/control_mode</code> <code>autowar_vehicle_msgs/ControlModeReport</code> - <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> Gear <code>/vehicle/status/gear_status</code> <code>autoware_vehicle_msgs/GearReport</code> - <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> Turn Indicators <code>/vehicle/status/turn_indicators_status</code> <code>autoware_vehicle_msgs/TurnIndicatorsReport</code> - <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> Hazard Lights <code>/vehicle/status/hazard_lights_status</code> <code>autoware_vehicle_msgs/HazardLightsReport</code> - <code>30</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> <p>Ground Truth</p> Pose <code>/awsim/ground_truth/vehicle/pose</code> <code>geometry_msgs/PoseStamped</code> <code>base_link</code> <code>100</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code> V2I <code>/perception/traffic_light_recognition/external/traffic_signals</code> <code>autoware_perception_msgs/msg/TrafficLightGroupArray</code> - <code>10</code> <code>Reliable</code>,<code>Volatile</code>,<code>Keep last/1</code>"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/","title":"Preparing the connection between AWSIM and scenario_simulator_v2","text":""},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#preparing-the-connection-between-awsim-and-scenario_simulator_v2","title":"Preparing the connection between AWSIM and scenario_simulator_v2","text":"<p>This tutorial describes: - how to modify scenario to work with AWSIM - how to prepare the AWSIM scene to work with <code>scenario_simulator_v2</code></p>"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#scenario-preparation-to-work-with-awsim","title":"Scenario preparation to work with AWSIM","text":"<p>To prepare the scenario to work with AWSIM add <code>model3d</code> field to entity specification</p> <p>It is utilized as an asset key to identify the proper prefab.</p> <p></p> <p>Match the parameters of the configured vehicle to match the entities parameters in AWSIM as close as it is required. Especially the bounding box is crucial to validate the collisions correctly.</p>"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#default-awsim-asset-catalog","title":"Default AWSIM asset catalog","text":"<p>AWSIM currently supports the following asset key values.</p> <p>The list can be extended if required. Appropriate values should be added to asst key list in the <code>ScenarioSimulatorConnector</code> component and the vehicle parameters in scenario simulator should match them.</p>"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#ego-vehicle-entity-with-sensor","title":"Ego Vehicle Entity (with sensor)","text":"model3d boundingbox size (m) wheel base(m) front tread(m) rear tread(m) tier diameter(m) max steer(deg) lexus_rx450h width : 1.920  height : 1.700  length : 4.890 2.105 1.640 1.630 0.766 35"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#npc-vehicle-entity","title":"NPC Vehicle Entity","text":"model3d boundingbox size (m) wheel base(m) front tread(m) rear tread(m) tier diameter(m) max steer(deg) taxi width : 1.695  height : 1.515  length : 4.590 2.680 1.460 1.400 0.635 35 truck_2t width : 1.695  height : 1.960  length : 4.685 2.490 1.395 1.240 0.673 40 hatchback width : 1.695  height 1.515  length : 3.940 2.550 1.480 1.475 0.600 35 van width : 1.880  height : 2.285  length : 4.695 2.570 1.655 1.650 0.600 35 small_car width : 1.475  height 1.800  length : 3.395 2.520 1.305 1.305 0.557 35"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#npc-pedestrian-entity","title":"NPC Pedestrian Entity","text":"model3d boundingbox size (m) human width : 0.400  height : 1.800  length : 0.300"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#misc-object-entity","title":"Misc Object Entity","text":"model3d boundingbox size (m) sign_board width : 0.31  height : 0.58  length : 0.21"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#scenarios-limitations","title":"Scenarios limitations","text":"<p>Vast majority of features supported by <code>scenario_simulator_v2</code> are supported with AWSIM as well. Currently supported features are described in the scenario_simulator_v2's documentation.</p> <p>Features which are not supported when connected with AWSIM are listed below.</p> <ol> <li>Controller properties used by <code>attach_*_sensor</code><ul> <li><code>pointcloudPublishingDelay</code></li> <li><code>isClairvoyant</code></li> <li><code>detectedObjectPublishingDelay</code></li> <li><code>detectedObjectPositionStandardDeviation</code></li> <li><code>detectedObjectMissingProbability</code></li> <li><code>randomSeed</code></li> </ul> </li> </ol> <p>If those features are curcial for the scenario's execution, the scenario might not work properly.</p>"},{"location":"Components/ScenarioSimulation/PreparingTheConnectionBetweenAWSIMAndScenarioSimulator/#awsim-scene-preparation-to-work-with-scenario_simulator_v2","title":"AWSIM scene preparation to work with <code>scenario_simulator_v2</code>","text":"<ol> <li>Disable or remove random traffic and any pre-spawned NPCs</li> <li>Disable or remove V2I traffic lights publishing</li> <li>Disable or remove the clock publisher</li> </ol> <ol> <li>Add <code>ScenarioSimulatorConnector</code> prefab to the scene - located in <code>Assets/ScenarioSimulatorConnector</code></li> </ol> <ol> <li>Add Ego Follow Camera object - most likely Main Camera</li> </ol> <ol> <li>If necessary update the asset_id to prefab mapping - key in the map can be used in the scenario</li> </ol> <ol> <li>Add TimeSourceSelector prefab to the scene - located in <code>Assets/AWSIM/Scripts/Clock/Prefabs</code></li> </ol> <ol> <li>Configure Type in the TimeSourceSelector component to SS2</li> </ol>"},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/","title":"Setup Unity project for scenario simulation","text":""},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#running-awsim-from-unity-editor-with-scenario_simulator_v2","title":"Running AWSIM from Unity Editor with <code>scenario_simulator_v2</code>","text":"<p>Below you can find instructions on how to setup the scenario execution using <code>scenario_simulator_v2</code> with AWSIM run from Unity Editor as a simulator The instruction assumes using the Ubuntu OS.</p>"},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Build Autoware by following \"Build Autoware with <code>scenario_simulator_v2</code>\" section from the scenario simulator and AWSIM quick start guide</p> </li> <li> <p>Follow Setup Unity Project tutorial</p> </li> </ol>"},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#running-the-demo","title":"Running the demo","text":"<ol> <li>Open AutowareSimulationScenarioSimulator.unity scene placed under <code>Assets/AWSIM/Scenes/Main</code> directory</li> <li>Run the simulation by clicking <code>Play</code> button placed at the top section of Editor.</li> <li>Launch <code>scenario_test_runner</code>.    <pre><code>source install/setup.bash\nros2 launch scenario_test_runner scenario_test_runner.launch.py                        \\\narchitecture_type:=awf/universe  record:=false                                         \\\nscenario:='$(find-pkg-share scenario_test_runner)/scenario/sample_awsim.yaml'          \\\nsensor_model:=awsim_sensor_kit  vehicle_model:=sample_vehicle                          \\\nlaunch_simple_sensor_simulator:=false autoware_launch_file:=\"e2e_simulator.launch.xml\" \\\ninitialize_duration:=260 port:=8080\n</code></pre></li> </ol>"},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#other-sample-scenarios","title":"Other sample scenarios","text":""},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#conventional-traffic-lights-demo","title":"Conventional traffic lights demo","text":"<p>This scenario controls traffic signals in the scene based on OpenSCENARIO. It can be used to verify whether traffic light recognition pipeline works well in Autoware.</p> <pre><code>ros2 launch scenario_test_runner scenario_test_runner.launch.py                                           \\\narchitecture_type:=awf/universe  record:=false                                                            \\\nscenario:='$(find-pkg-share scenario_test_runner)/scenario/sample_awsim_conventional_traffic_lights.yaml' \\\nsensor_model:=awsim_sensor_kit  vehicle_model:=sample_vehicle                                             \\\nlaunch_simple_sensor_simulator:=false autoware_launch_file:=\"e2e_simulator.launch.xml\"                    \\\ninitialize_duration:=260 port:=8080\n</code></pre>"},{"location":"Components/ScenarioSimulation/SetupUnityProjectForScenarioSimulation/#v2i-traffic-lights-demo","title":"V2I traffic lights demo","text":"<p>This scenario publishes V2I traffic signals information based on OpenSCENARIO. It can be used to verify Autoware responds to V2I traffic lights information correctly.</p> <pre><code>ros2 launch scenario_test_runner scenario_test_runner.launch.py                                  \\\narchitecture_type:=awf/universe  record:=false                                                   \\\nscenario:='$(find-pkg-share scenario_test_runner)/scenario/sample_awsim_v2i_traffic_lights.yaml' \\\nsensor_model:=awsim_sensor_kit  vehicle_model:=sample_vehicle                                    \\\nlaunch_simple_sensor_simulator:=false autoware_launch_file:=\"e2e_simulator.launch.xml\"           \\\ninitialize_duration:=260 port:=8080\n</code></pre>"},{"location":"Components/Sensors/CameraSensor/","title":"Camera Sensor","text":""},{"location":"Components/Sensors/CameraSensor/#camerasensor","title":"CameraSensor","text":""},{"location":"Components/Sensors/CameraSensor/#introduction","title":"Introduction","text":"<p><code>CameraSensor</code> is a component that simulates an RGB camera. Autonomous vehicles can be equipped with many cameras used for various purposes. In the current version of AWSIM, the camera is used primarily to provide the image to the traffic light recognition module in Autoware.</p>"},{"location":"Components/Sensors/CameraSensor/#prefab","title":"Prefab","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/CameraSensor.prefab\n</code></pre>"},{"location":"Components/Sensors/CameraSensor/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p>The mentioned single <code>CameraSensor</code> has its own frame <code>traffic_light_left_camera/camera_link</code> in which its data is published. The sensor prefab is added to this frame. The <code>traffic_light_left_camera/camera_link</code> link is added to the <code>base_link</code> object located in the <code>URDF</code>.</p> <p></p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p>"},{"location":"Components/Sensors/CameraSensor/#camerasensorholder-script","title":"CameraSensorHolder (script)","text":"<p>CameraSensorHolder (script) allows the sequential rendering of multiple camera sensors. To utilize it, each <code>CameraSensor</code> object should be attached as a child object of the <code>CameraSensorHolder</code>. </p>"},{"location":"Components/Sensors/CameraSensor/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>Camere Sensors</code> - a collection of camera sensors used for rendering</li> <li><code>Publish Hz</code> - the frequency at which camera rendering, image processing and callbacks are executed</li> <li><code>Render In Queue</code> - camera sensors rendering sequence type: in queue (one after another) or all at the same frame</li> </ul>"},{"location":"Components/Sensors/CameraSensor/#camerasensor-components","title":"CameraSensor Components","text":"<p>For the <code>CameraSensor</code> to work properly, the GameObject to which the scripts are added must also have:</p> <ul> <li>Camera component - the basic component that ensures the functionality of the camera as a device in Unity that capture and display the world to the player.</li> <li>HD Additional Camera Data (script) - additional component that holds HDRP specific parameters for camera. This Component should be added automatically together with Camera component.</li> </ul> <p>TrafficLights recognition</p> <p>In case of problems with the recognition of traffic lights in Autoware, it may help to increase the image resolution and focal length of the camera in AWSIM.</p> <p>Camera settings</p> <p>If you would like to adjust the image captured by the camera, we encourage you to read this manual.</p> <p>The <code>CameraSensor</code> functionality is split into two scripts:</p> <ul> <li>Camera Sensor (script) - acquires the image from the Unity camera, transforms it and saves to the  BGR8 format, this format along with the camera parameters is its script output - script also calls the callback for it.</li> <li>Camera Ros2 Publisher (script) - provides the ability to publish <code>CameraSensor</code> output as Image and CameraInfo messages type published on a specific ROS2 topics.</li> </ul> <p>Scripts can be found under the following path:</p> <pre><code>Assets/AWSIM/Scripts/Sensors/CameraSensor/*\n</code></pre> <p>In the same location there are also <code>*.compute</code> files containing used <code>ComputeShaders</code>.</p>"},{"location":"Components/Sensors/CameraSensor/#camerasensor-script","title":"CameraSensor (script)","text":"<p>Camera Sensor (script) is a core camera sensor component. It is responsible for applying OpenCV distortion and encoding to BGR8 format. The distortion model is assumed to be Plumb Bob. The script renders the image from the camera to <code>Texture2D</code> and transforms it using the distortion parameters. This image is displayed in the GUI and further processed to obtain the list of bytes in BGR8 format on the script output.</p> <p>The script uses two <code>ComputeShaders</code>, they are located in the same location as the scripts:</p> <ul> <li><code>CameraDistortion</code> - to correct the image using the camera distortion parameters,</li> <li> <p><code>RosImageShader</code> - to encode two pixels color (bgr8 - 3 bytes) into one (uint32 - 4 bytes) in order to produce ROS Image BGR8 buffer.</p> <p></p> </li> </ul> API type feature DoRender void Renders the Unity camera, applies OpenCV distortion to rendered image and update output data."},{"location":"Components/Sensors/CameraSensor/#elements-configurable-from-the-editor-level_1","title":"Elements configurable from the editor level","text":"<ul> <li><code>Output Hz</code> - frequency of output calculation and callback (default: <code>10Hz</code>)</li> <li>Image On GUI:<ul> <li><code>Show</code> - if camera image should be show on GUI (default: <code>true</code>)</li> <li><code>Scale</code> - scale of reducing the image from the camera, <code>1</code> - will give an image of real size, <code>2</code> - twice smaller, etc. (default: <code>4</code>)</li> <li><code>X Axis</code> - position of the upper left corner of the displayed image in the X axis, <code>0</code>is the left edge (default: <code>0</code>)</li> <li><code>Y Axis</code> - position of the upper left corner of the  displayed image in the Y axis, <code>0</code> is the upper edge (default: <code>0</code>)</li> </ul> </li> <li>Camera parameters<ul> <li><code>Width</code> - image width (default: <code>1920</code>)</li> <li><code>Height</code> - image height (default: <code>1080</code>)</li> <li><code>K1, K2, P1, P2, K3</code> - camera distortion coefficients for Plum Bob model(default: <code>0, 0, 0, 0, 0</code>)</li> </ul> </li> <li><code>Camera Object</code> -  reference to the basic Camera component (default: <code>None</code>)</li> <li><code>Distortion Shader</code> - reference to ComputeShader asset about Distortion Shader functionality (default: <code>None</code>)</li> <li><code>Ros Image Shader</code> - reference to ComputeShader asset about Ros Image Shader functionality (default: <code>None</code>)</li> </ul>"},{"location":"Components/Sensors/CameraSensor/#output-data","title":"Output Data","text":"<p>The sensor computation output format is presented below:</p> Category Type Description ImageDataBuffer byte[ ] Buffer with image data. CameraParameters CameraParameters Set of the camera parameters."},{"location":"Components/Sensors/CameraSensor/#cameraros2publisher-script","title":"CameraRos2Publisher (script)","text":"<p>Converts the data output from <code>CameraSensor</code> to ROS2 Image and CameraInfo type messages and publishes them. The conversion and publication is performed using the <code>Publish(CameraSensor.OutputData outputData)</code> method, which is the <code>callback</code> triggered by Camera Sensor (script) for the current output.</p> <p>Due to the fact that the entire image is always published, the <code>ROI</code> field of the message is always filled with zeros. The script also ensures that <code>binning</code> is assumed to be zero and the rectification matrix is the identity matrix.</p> <p>Warning</p> <p>The script uses the camera parameters set in the CameraSensor script - remember to configure them depending on the camera you are using.</p>"},{"location":"Components/Sensors/CameraSensor/#elements-configurable-from-the-editor-level_2","title":"Elements configurable from the editor level","text":"<ul> <li><code>Image Topic</code> - the ROS2 topic on which the <code>Image</code> message is published(default: <code>\"/sensing/camera/traffic_light/image_raw\"</code>)</li> <li><code>Camera Info Topic</code> - the ROS2 topic on which the <code>CameraInfo</code> message is published(default: <code>\"/sensing/camera/traffic_light/camera_info\"</code>)</li> <li><code>Frame id</code> - frame in which data is published, used in <code>Header</code>(default: <code>\"traffic_light_left_camera/camera_link\"</code>)</li> <li><code>Qos Settings</code> - Quality of service profile used in the publication(default: <code>Best effort</code>, <code>Volatile</code>, <code>Keep last</code>, <code>1</code>)</li> </ul>"},{"location":"Components/Sensors/CameraSensor/#published-topics","title":"Published Topics","text":"<ul> <li>Frequency: <code>10Hz</code></li> <li>QoS: <code>Best effort</code>, <code>Volatile</code>, <code>Keep last/1</code></li> </ul> Category Topic Message type <code>frame_id</code> Camera info <code>/sensing/camera/traffic_light/camera_info</code> <code>sensor_msgs/CameraInfo</code> <code>traffic_light_left_camera/camera_link</code> Camera image <code>/sensing/camera/traffic_light/image_raw</code> <code>sensor_msgs/Image</code> <code>traffic_light_left_camera/camera_link</code>"},{"location":"Components/Sensors/GNSSSensor/","title":"GNSS Sensor","text":""},{"location":"Components/Sensors/GNSSSensor/#gnsssensor","title":"GnssSensor","text":""},{"location":"Components/Sensors/GNSSSensor/#introduction","title":"Introduction","text":"<p><code>GnssSensor</code> is a component which simulates the position of vehicle computed by the Global Navigation Satellite System based on the transformation of the GameObject to which this component is attached. The <code>GnssSensor</code> outputs the position in the MGRS coordinate system.</p>"},{"location":"Components/Sensors/GNSSSensor/#prefab","title":"Prefab","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/GnssSensor.prefab\n</code></pre>"},{"location":"Components/Sensors/GNSSSensor/#link","title":"Link","text":"<p><code>GnssSensor</code> has its own frame <code>gnss_link</code> in which its data is published. The sensor prefab is added to this frame. The <code>gnss_link</code> frame is added to the <code>sensor_kit_base_link</code> in the <code>base_link</code> object located in the <code>URDF</code>.</p> <p></p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p>"},{"location":"Components/Sensors/GNSSSensor/#components","title":"Components","text":"<p>The <code>GnssSensor</code> functionality is split into two components:</p> <ul> <li>Gnss Sensor (script) - it calculates the position as its output and calls the callback for it.</li> <li>Gnss Ros2 Publisher (script) - provides the ability to publish <code>GnssSensor</code> output as <code>PoseStamped</code> and PoseWithCovarianceStamped published on a specific ROS2 topics.</li> </ul> <p>Scripts can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/Gnss/*\n</code></pre>"},{"location":"Components/Sensors/GNSSSensor/#gnss-sensor-script","title":"Gnss Sensor (script)","text":"<p>This is the main script in which all calculations are performed:</p> <ol> <li>the position of the Object in Unity is read,</li> <li>this position is transformed to the ROS2 coordinate system (MGRS offset is added here),</li> <li>the result of the transformation is saved as the output of the component,</li> <li>for the current output a <code>callback</code> is called (which can be assigned externally).</li> </ol>"},{"location":"Components/Sensors/GNSSSensor/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>Output Hz</code> - frequency of output calculation and callback (default: <code>100Hz</code>)</li> </ul>"},{"location":"Components/Sensors/GNSSSensor/#output-data","title":"Output Data","text":"Category Type Description Position Vector3 Position in the MGRS coordinate system."},{"location":"Components/Sensors/GNSSSensor/#gnss-ros2-publisher-script","title":"Gnss Ros2 Publisher (script)","text":"<p>Converts the data output from <code>GnssSensor</code> to ROS2 <code>PoseStamped</code> and PoseWithCovarianceStamped messages. These messages are published on two separate topics for each type. The conversion and publication is performed using the <code>Publish(GnssSensor.OutputData outputData)</code> method, which is the <code>callback</code> triggered by Gnss Sensor (script) for the current output update.</p> <p>Covariance matrix</p> <p>The row-major representation of the 6x6 covariance matrix is filled with <code>0</code> and does not change during the script run.</p>"},{"location":"Components/Sensors/GNSSSensor/#elements-configurable-from-the-editor-level_1","title":"Elements configurable from the editor level","text":"<ul> <li><code>Pose Topic</code> - the ROS2 topic on which the message <code>PoseStamped</code> type is published(default: <code>\"/sensing/gnss/pose\"</code>)</li> <li><code>Pose With Covariance Stamped Topic</code> - the ROS2 topic on which the message PoseWithCovarianceStamped type is published(default: <code>\"/sensing/gnss/pose_with_covariance\"</code>)</li> <li><code>Frame id</code> - frame in which data are published, used in <code>Header</code>(default: <code>\"gnss_link\"</code>)</li> <li><code>Qos Settings</code> - Quality of service profile used in the publication(default is assumed as <code>\"system_default\"</code>: <code>Reliable</code>, <code>Volatile</code>, <code>Keep last</code>, <code>1</code>)</li> </ul>"},{"location":"Components/Sensors/GNSSSensor/#published-topics","title":"Published Topics","text":"<ul> <li>Frequency: <code>1Hz</code></li> <li>QoS: <code>Reliable</code>, <code>Volatile</code>, <code>Keep last/1</code></li> </ul> Category Topic Message type <code>frame_id</code> Pose <code>/sensing/gnss/pose</code> <code>geometry_msgs/Pose</code> <code>gnss_link</code> Pose with Covariance <code>/sensing/gnss/pose_with_covariance</code> <code>geometry_msgs/PoseWithCovarianceStamped</code> <code>gnss_link</code>"},{"location":"Components/Sensors/IMUSensor/","title":"IMU Sensor","text":""},{"location":"Components/Sensors/IMUSensor/#imusensor","title":"IMUSensor","text":""},{"location":"Components/Sensors/IMUSensor/#introduction","title":"Introduction","text":"<p><code>IMUSensor</code> is a component that simulates an IMU (Inertial Measurement Unit) sensor. Measures acceleration (\\({m}/{s^2}\\)) and angular velocity (\\({rad}/{s}\\)) based on the transformation of the GameObject to which this component is attached.</p>"},{"location":"Components/Sensors/IMUSensor/#prefab","title":"Prefab","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/IMUSensor.prefab\n</code></pre>"},{"location":"Components/Sensors/IMUSensor/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p><code>IMUSensor</code> has its own frame <code>tamagawa/imu_link</code> in which its data is published. The sensor prefab is added to this frame. The <code>tamagawa/imu_link</code> link is added to the <code>sensor_kit_base_link</code> in the <code>base_link</code> object located in the <code>URDF</code>.</p> <p></p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p>"},{"location":"Components/Sensors/IMUSensor/#components","title":"Components","text":"<p>The <code>IMUSensor</code> functionality is split into two scripts:</p> <ul> <li>IMU Sensor (script) - it calculates the acceleration and angular velocity as its output and calls the callback for it.</li> <li>Imu Ros2 Publisher (script) - provides the ability to publish <code>IMUSensor</code> output as Imu message type published on a specific ROS2 topics.</li> </ul> <p>Scripts can be found under the following path:</p> <pre><code>Assets/AWSIM/Scripts/Sensors/Imu/*\n</code></pre>"},{"location":"Components/Sensors/IMUSensor/#imu-sensor-script","title":"IMU Sensor (script)","text":"<p>This is the main script in which all calculations are performed:</p> <ul> <li>the angular velocity is calculated as the derivative of the orientation with respect to time,</li> <li>acceleration is calculated as the second derivative of position with respect to time,</li> <li>in the calculation of acceleration, the gravitational vector is considered - which is added.</li> </ul> <p>Warning</p> <p>If the angular velocity about any axis is <code>NaN</code> (infinite), then  angular velocity is published as vector zero.</p>"},{"location":"Components/Sensors/IMUSensor/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>Output Hz</code> - frequency of output calculation and callback (default: <code>30Hz</code>)</li> </ul>"},{"location":"Components/Sensors/IMUSensor/#output-data","title":"Output Data","text":"Category Type Description LinearAcceleration Vector3 Measured acceleration (m/s^2) AngularVelocity Vector3 Measured angular velocity (rad/s)"},{"location":"Components/Sensors/IMUSensor/#imu-ros2-publisher-script","title":"Imu Ros2 Publisher (script)","text":"<p>Converts the data output from <code>IMUSensor</code> to ROS2 Imu type message and publishes it. The conversion and publication is performed using the <code>Publish(IMUSensor.OutputData outputData)</code> method, which is the <code>callback</code> triggered by IMU Sensor (script) for the current output.</p> <p>Warning</p> <p>In each 3x3 covariance matrices the row-major representation is filled with <code>0</code> and does not change during the script run. In addition, the field <code>orientation</code> is assumed to be <code>{1,0,0,0}</code> and also does not change.</p>"},{"location":"Components/Sensors/IMUSensor/#elements-configurable-from-the-editor-level_1","title":"Elements configurable from the editor level","text":"<ul> <li><code>Topic</code> - the ROS2 topic on which the message is published(default: <code>\"/sensing/imu/tamagawa/imu_raw\"</code>)</li> <li><code>Frame id</code> - frame in which data is published, used in <code>Header</code>(default: <code>tamagawa/imu_link\"</code>)</li> <li><code>Qos Settings</code> - Quality of service profile used in the publication(default: <code>Reliable</code>, <code>Volatile</code>, <code>Keep last</code>, <code>1000</code>)</li> </ul>"},{"location":"Components/Sensors/IMUSensor/#published-topics","title":"Published Topics","text":"<ul> <li>Frequency: <code>30Hz</code></li> <li>QoS:  <code>Reliable</code>, <code>Volatile</code>, <code>Keep last/1000</code></li> </ul> Category Topic Message type <code>frame_id</code> IMU data <code>/sensing/imu/tamagawa/imu_raw</code> <code>sensor_msgs/Imu</code> <code>tamagawa/imu_link</code>"},{"location":"Components/Sensors/LiDARSensor/AddNewLiDAR/","title":"Add New LiDAR","text":""},{"location":"Components/Sensors/LiDARSensor/AddNewLiDAR/#add-a-new-lidar","title":"Add a new LiDAR","text":"<p><code>RGLUnityPlugin</code> (<code>RGL</code>) comes with a number of the most popular LiDARs model definitions and ready-to-use prefabs. However, there is a way to create your custom LiDAR. This section describes how to add a new LiDAR model that works with <code>RGL</code>, then create a prefab for it and add it to the scene.</p> <p>Supported LiDARs</p> <p>Not all lidar types are supported by <code>RGL</code>. Unfortunately, in the case of <code>MEMs</code> LiDARs, there is a non-repetitive phenomenon - for this reason, the current implementation is not able to reproduce their work.</p>"},{"location":"Components/Sensors/LiDARSensor/AddNewLiDAR/#1-add-a-new-lidar-model","title":"1. Add a new LiDAR model","text":"<p>The example shows the addition of a LiDAR named <code>NewLidarModel</code>.</p> <p>To add a new LiDAR model, perform the following steps:</p> <ol> <li> <p>Navigate to <code>Assets/RGLUnityPlugin/Scripts/LidarModels</code>.</p> </li> <li> <p>Add its name to the <code>LidarModels.cs</code> at the end of the enumeration. The order of enums must not be changed to keep existing prefabs working.</p> <p></p> </li> <li> <p>Now, it is time to define the laser (also called a channel) distribution of the LiDAR.</p> <p>Info</p> <p>If your LiDAR:</p> <pre><code>- has a uniform laser distribution\n- has the equal range for all of the lasers\n- fire all of the rays (beams) at the same time\n</code></pre> <p>You can skip this step and use our helper method to generate a simple uniform laser array definition (more information in the next step).</p> <ol> <li> <p>Laser distribution is represented by <code>LaserArray</code> consists of:</p> <ul> <li> <p><code>centerOfMeasurementLinearOffsetMm</code> - 3D translation from the game object's origin to LiDAR's origin. Preview in 2D:</p> <p></p> </li> </ul> <ul> <li> <p><code>focalDistanceMm</code> - Distance from the sensor center to the focal point where all laser beams intersect.</p> <p></p> </li> </ul> <ul> <li> <p><code>lasers</code> - array of lasers (channels) with a number of parameters:</p> <ul> <li><code>horizontalAngularOffsetDeg</code> - horizontal angle offset of the laser (Azimuth)</li> <li><code>verticalAngularOffsetDeg</code> - vertical angle offset of the laser (Elevation)</li> <li><code>verticalLinearOffsetMm</code> - vertical offset of the laser (translation from origin)</li> <li><code>ringId</code> - Id of the ring (in most cases laser Id)</li> <li><code>timeOffset</code> - time offset of the laser firing in milliseconds (with reference to the first laser in the array)</li> <li><code>minRange</code> - minimum range of the laser (set if lasers have different ranges)</li> <li><code>maxRange</code> - maximum range of the laser (set if lasers have different ranges)</li> </ul> </li> </ul> </li> <li> <p>To define a new laser distribution create a new class in the <code>LaserArrayLibrary.cs</code></p> <p></p> <ul> <li>Add a new public static instance of <code>LaserArray</code> with the definition.</li> </ul> <p>In this example, <code>NewLidarModel</code> laser distribution consists of 5 lasers with</p> <pre><code>- elevations: 15, 10, 0, -10, -15 degrees\n- azimuths: 1.4, -1.4, 1.4, -1.4, 1.4 degrees\n- ring Ids: 1, 2, 3, 4, 5\n- time offsets: 0, 0.01, 0.02, 0.03, 0.04 milliseconds\n- an equal range that will be defined later\n</code></pre> <p>Coordinate system</p> <p>Keep in mind that Unity has a left-handed coordinate system, while most of the LiDAR's manuals use a right-handed coordinate system. In that case, reverse sign of the values of the angles.</p> </li> </ol> </li> <li> <p>The last step is to create a LiDAR configuration by adding an entry to <code>LidarConfigurationLibrary.cs</code></p> <p></p> <p>Add a new item to the <code>ByModel</code> dictionary that collects LiDAR model enumerations with their <code>BaseLidarConfiguration</code> choosing one of the implementations:</p> <ul> <li><code>UniformRangeLidarConfiguration</code> - lidar configuration for uniformly distributed rays along the horizontal axis with a uniform range for all the rays (it contains <code>minRange</code> and <code>maxRange</code> parameters additionally)</li> <li><code>LaserBasedRangeLidarConfiguration</code> - lidar configuration for uniformly distributed rays along the horizontal axis with ranges retrieved from lasers description</li> <li>Or create your custom implementations in <code>LidarConfiguration.cs</code> like:<ul> <li><code>HesaiAT128LidarConfiguration</code></li> <li><code>HesaiQT128C2XLidarConfiguration</code></li> <li><code>HesaiPandar128E4XLidarConfiguration</code></li> </ul> </li> </ul> <p>Lidar configuration parameters descrition</p> <p>Please refer to this section for the detailed description of all configuration parameters.</p> </li> <li> <p>Done. New LiDAR preset should be available via Unity Inspector.</p> <p></p> <p>Frame rate of the LiDAR can be set in the <code>Automatic Capture Hz</code> parameter.</p> <p>Note: In the real-world LiDARs, frame rate affects horizontal resolution. Current implementation separates these two parameters. Keep in mind to change it manually.</p> </li> </ol>"},{"location":"Components/Sensors/LiDARSensor/AddNewLiDAR/#2-create-new-lidar-prefab","title":"2. Create new LiDAR prefab","text":"<ol> <li>Create an empty object and name it appropriately according to the LiDAR model.</li> <li>Attach script <code>LidarSensor.cs</code> to created object.</li> <li>Set the new added LiDAR model in <code>Model Preset</code> field, check if the configuration loads correctly. You can now customize it however you like.</li> <li>(Optional) Attach script <code>PointCloudVisualization.cs</code> for visualization purposes.</li> <li>For publishing point cloud via ROS2 attach script <code>RglLidarPublisher.cs</code> script to created object.</li> <li>Set the topics on which you want the data to be published and their frame.</li> <li>Save the prefab in the project.</li> </ol>"},{"location":"Components/Sensors/LiDARSensor/AddNewLiDAR/#3-test-your-prefab","title":"3. Test your prefab","text":"<ol> <li>Create a new scene (remember to add the <code>SceneManager</code>) or use one of the existing sample scenes.</li> <li> <p>Add the prepared LiDAR prefab by drag the prefab file and drop it into a scene.</p> <p></p> </li> <li> <p>A LiDAR GameObject should be instantiated automatically</p> <p></p> </li> <li> <p>Now you can run the scene and check how your LiDAR works.</p> </li> </ol> <p>Success</p> <p>We encourage you to develop a vehicle using the new LiDAR you have added - learn how to do this here.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/","title":"LiDAR Sensor","text":""},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#lidarsensor","title":"LidarSensor","text":""},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#introduction","title":"Introduction","text":"<p><code>LidarSensor</code> is the component that simulates the LiDAR (Light Detection and Ranging) sensor. LiDAR works by emitting laser beams that bounce off objects in the environment, and then measuring the time it takes for the reflected beams to return, allowing the sensor to create a 3D map of the surroundings. This data is used for object detection, localization, and mapping.</p> <p>LiDAR in an autonomous vehicle can be used for many purposes. The ones mounted on the top of autonomous vehicles are primarily used</p> <ul> <li>to scan the environment for localization in space</li> <li>to detect and identify obstacles such as approaching vehicles, pedestrians or other objects in the driving path.</li> </ul> <p>LiDARs placed on the left and right sides of the vehicle are mainly used to monitor the traffic lane and detect vehicles moving in adjacent lanes, enabling safe maneuvers such as lane changing or turning.</p> <p><code>LidarSensor</code> component is a part of <code>RGLUnityPlugin</code> that integrates the external RobotecGPULidar (<code>RGL</code>) library with Unity. <code>RGL</code> also allows to provide additional information about objects, more about it here.</p> <p>Use RGL in your scene</p> <p>If you want to use <code>RGL</code> in your scene, make sure the scene has an <code>SceneManager</code> component added and all objects meet the usage requirements.</p> <p>RGL default scenes</p> <p>If you would like to see how <code>LidarSensor</code> works using <code>RGL</code> or run some tests, we encourage you to familiarize yourself with the <code>RGL</code> test scenes section.</p> <p>Supported LiDARs</p> <p>The current scripts implementation allows you to configure the prefab for any mechanical LiDAR. You can read about how to do it here. MEMS-based LiDARs due to their different design are not yet fully supported.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#prefabs","title":"Prefabs","text":"<p>Prefabs can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/RobotecGPULidars/*\n</code></pre> <p>The table of available prefabs can be found below:</p> LiDAR Path Appearance HESAI Pandar40P <code>HesaiPandar40P.prefab</code> HESAI PandarQT64 <code>HesaiPandarQT64.prefab</code> HESAI PandarXT32 <code>HesaiPandarXT32.prefab</code> HESAI QT128C2X <code>HesaiQT128C2X.prefab</code> HESAI Pandar128E4X <code>HesaiPandar128E4X.prefab</code> HESAI AT128 E2X <code>HesaiAT128E2X.prefab</code> Ouster OS1-64 <code>OusterOS1-64.prefab</code> Velodyne VLP-16 <code>VelodyneVLP16.prefab</code> Velodyne VLC-32C <code>VelodyneVLP32C.prefab</code> Velodyne VLS-128-AP <code>VelodyneVLS128.prefab</code>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p><code>LidarSensor</code> is configured in default vehicle <code>EgoVehicle</code> prefab. It is added to <code>URDF</code> object as a child of <code>sensor_kit_base_link</code>. <code>LidarSensor</code> placed in this way does not have its own frame, and the data is published relative to <code>sensor_kit_base_link</code>. More details about the location of the sensors in the vehicle can be found here.</p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p> <p>Additional LiDARs</p> <p>For a LiDAR placed on the left side, right side or rear, an additional link should be defined.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#components-and-resources","title":"Components and Resources","text":"<p>The LiDAR sensor simulation functionality is split into three components:</p> <ul> <li>Lidar Sensor (script) - provides lidar configuration, creates RGL pipeline to simulate lidar, and performs native RGL raytrace calls,</li> <li>Rgl Lidar Publisher (script)  - extends RGL pipeline with nodes to publish ROS2 messages.</li> <li>Point Cloud Visualization (script) - visualizes point cloud collected by sensor.</li> </ul> <p>Moreover, the scripts use <code>Resources</code> to provide configuration for prefabs of supported lidar models:</p> <ul> <li>LaserModels - provides a list of supported models,</li> <li>LaserArrayLibrary - provides data related to laser array construction for supported models,</li> <li>LaserConfigurationLibrary - provides full configuration, with ranges and noise for supported models.</li> </ul> <p>These are elements of the <code>RGLUnityPlugin</code>, you can read more here.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#lidar-sensor-script","title":"Lidar Sensor (script)","text":"<p>This is the main component that creates the <code>RGL</code> node pipeline for the LiDAR simulation. The pipeline consists of:</p> <ul> <li>setting ray pattern,</li> <li>transforming rays to represent pose of the sensor on the scene,</li> <li>applying Gaussian noise,</li> <li>performing raytracing,</li> <li>removing non-hits from the result point cloud,</li> <li>transforming point cloud to sensor frame coordinate system.</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>Automatic Capture Hz</code> - the rate of sensor processing (default: <code>10Hz</code>)</li> <li><code>Model Preset</code> - allows selecting one of the built-in LiDAR models (default: <code>RangeMeter</code>)</li> <li><code>Apply Distance Gaussian Noise</code> - enable/disable distance Gaussian noise (default: <code>true</code>)</li> <li><code>Apply Angular Gaussian Noise</code> - enable/disable angular Gaussian noise (default: <code>true</code>)</li> <li><code>Apply Velocity Distortion</code> - enable/disable velocity distortion (default: <code>false</code>)</li> <li>Configuration:<ul> <li><code>Laser Array</code> - geometry description of lidar's array of lasers, should be prepared on the basis of the manual for a given model of LiDAR (default: loaded from <code>LaserArrayLibrary</code>)</li> <li><code>Horizontal Resolution</code> - the horiontal resolution of laser array firings</li> <li><code>Min H Angle</code> - minimum horizontal angle, left (default: <code>0</code>)</li> <li><code>Max H Angle</code> - maximum horizontal angle, right (default: <code>0</code>)</li> <li><code>Laser Array Cycle Time</code> - time between two consecutive firings of the whole laser array in milliseconds (default: <code>0</code>); used for velocity distortion feature.</li> <li><code>Beam Divergence</code> - represents the deviation of photons from a single beam emitted by a LiDAR sensor (in degrees); used for simulating snow only (private feature).</li> <li>Noise Params:<ul> <li><code>Angular Noise Type</code> - angular noise type(default: <code>Ray Based</code>)</li> <li><code>Angular Noise St Dev</code> - angular noise standard deviation in degree(default: <code>0.05729578</code>)</li> <li><code>Angular Noise Mean</code> - angular noise mean in degrees(default: <code>0</code>)</li> <li><code>Distance Noise St Dev Base</code> - distance noise standard deviation base in meters(default: <code>0.02</code>)</li> <li><code>Distance Noise Rise Per Meter</code> - distance noise standard deviation rise per meter(default: <code>0</code>)</li> <li><code>Distance Noise Mean</code> - distance noise mean in meters(default: <code>0</code>)</li> </ul> </li> <li>Additional options (available for some Lidar Model Preset)<ul> <li><code>Min Range</code> - minimum range of the sensor (if not avaiable, the range is different for each laser in <code>Laser Array</code>)</li> <li><code>Max Range</code> - maximum range of the sensor (if not avaiable, the range is different for each laser in <code>Laser Array</code>)</li> <li><code>High Resolution Mode Enabled</code> - whether to activate high resolution mode (available for <code>Hesai Pandar 128E4X</code> LiDAR model)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#output-data","title":"Output Data","text":"<p><code>LidarSensor</code> provides public methods to extend this pipeline with additional <code>RGL</code> nodes. In this way, other components can request point cloud processing operations and receive data in the desired format.</p> <p>Example of how to get XYZ point cloud data:</p> <ol> <li>To obtain point cloud data from another component you have to create a new <code>RGLNodeSequence</code> with RGL node to yield XYZ field and connect it to <code>LidarSensor</code>:    <pre><code>rglOutSubgraph = new RGLNodeSequence().AddNodePointsYield(\"OUT_XYZ\", RGLField.XYZ_F32);\nlidarSensor = GetComponent&lt;LidarSensor&gt;();\nlidarSensor.ConnectToWorldFrame(rglOutSubgraph); // you can also connect to Lidar frame using ConnectToLidarFrame\n// You can add a callback to receive a notification when new data is ready\nlidarSensor.onNewData += HandleLidarDataMethod;\n</code></pre></li> <li>To get data from <code>RGLNodeSequence</code> call <code>GetResultData</code>:    <pre><code>Vector3[] xyz = new Vector3[0];\nrglOutSubgraph.GetResultData&lt;Vector3&gt;(ref xyz);\n</code></pre></li> </ol>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#rgl-lidar-publisher-script","title":"Rgl Lidar Publisher (script)","text":"<p><code>RglLidarPublisher</code> extends the main <code>RGL</code> pipeline created in <code>LidarSensor</code> with <code>RGL</code> nodes that produce point clouds in specific format and publish them to the ROS2 topic. Thanks to the ROS2 integration with <code>RGL</code>, point clouds can be published directly from the native library. <code>RGL</code> creates ROS2 node named <code>/RobotecGPULidar</code> with publishers generated by <code>RGL</code> nodes.</p> <p>Currently, <code>RglLidarPublisher</code> implements two ROS2 publishers:</p> <ul> <li>rosPCL24 - a 24-byte point cloud format used by Autoware</li> <li>rosPCL48 - a 48-byte extended version point cloud format used by Autoware</li> </ul> <p>Details on the construction of these formats are available in the <code>PointCloudFormats</code> under the following path:</p> <pre><code>Assets/AWSIM/Scripts/Sensors/LiDAR/PointCloudFormats.cs\n</code></pre> <p>rosPCL48 format</p> <p>For a better understanding of the rosPCL48 format, we encourage you to familiarize yourself with the point cloud pre-processing process in Autoware, which is described here.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#elements-configurable-from-the-editor-level_1","title":"Elements configurable from the editor level","text":"<ul> <li><code>Pcl 24 Topic</code> - the ROS2 topic on which the <code>PointCloud2</code> message is published(default: <code>\"lidar/pointcloud\"</code>)</li> <li><code>Pcl 48 Topic</code> - the ROS2 topic on which the <code>PointCloud2</code> message is published(default: <code>\"lidar/pointcloud_ex\"</code>)</li> <li><code>Frame ID</code> - frame in which data are published, used in <code>Header</code> (default: <code>\"world\"</code>)</li> <li><code>Publish PCL24</code> - if publish cloud PCL24 (default: <code>true</code>)</li> <li><code>Publish PCL48</code> - if publish cloud PCL48 (default: <code>true</code>)</li> <li><code>Qos Settings</code> - Quality of service profile used in the publication(default: <code>Best effort</code>, <code>Volatile</code>, <code>Keep last</code>, <code>5</code>)</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#published-topics","title":"Published Topics","text":"<ul> <li>Frequency: <code>10Hz</code></li> <li>QoS:  <code>Best effort</code>, <code>Volatile</code>, <code>Keep last/5</code></li> </ul> Category Topic Message type <code>frame_id</code> PointCloud 24-byte format <code>/lidar/pointcloud</code> <code>sensor_msgs/PointCloud2</code> <code>world</code> PointCloud 48-byte format <code>/lidar/pointcloud_ex</code> <code>sensor_msgs/PointCloud2</code> <code>world</code>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#point-cloud-visualization-script","title":"Point Cloud Visualization (script)","text":"<p>A component visualizing a point cloud obtained from <code>RGL</code> in the form of a <code>Vector3</code> list as colored points in the Unity scene. Based on the defined color table, it colors the points depending on the height at which they are located.</p> <p>The obtained points are displayed as the vertices of mesh, and their coloring is possible thanks to the use of <code>PointCloudMaterial</code> material which can be found in the following path:</p> <pre><code>Assets/RGLUnityPlugin/Resources/PointCloudMaterial.mat\n</code></pre> <p><code>Point Cloud Visualization</code> preview:</p> <p></p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#elements-configurable-from-the-editor-level_2","title":"Elements configurable from the editor level","text":"<ul> <li><code>Point Shape</code> - the shape of the displayed points (default: <code>Box</code>)</li> <li><code>Point Size</code> - the size of the displayed points (default: <code>0.05</code>)</li> <li><code>Colors</code> - color list used depending on height(default: <code>6</code> colors: <code>red, orange, yellow, green, blue, violet</code>)</li> <li><code>Auto Compute Coloring Heights</code> - automatic calculation of heights limits for a list of colors (default: <code>false</code>)</li> <li><code>Min Coloring Height</code> - minimum height value from which color matching is performed, below this value all points have the first color from the list (default: <code>0</code>)</li> <li><code>Max Coloring Height</code> - maximum height value from which color matching is performed, above this value all points have the last color from the list (default: <code>20</code>)</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#read-material-information","title":"Read material information","text":"<p>To ensure the publication of the information described in this section, GameObjects must be adjusted accordingly. This tutorial describes how to do it.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#intensity-texture","title":"Intensity Texture","text":"<p><code>RGL Unity Plugin</code> allows assigning an <code>Intensity Texture</code> to the GameObjects to produce a point cloud containing information about the lidar ray intensity of hit. It can be used to distinguish different levels of an object's reflectivity.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#output-data_1","title":"Output data","text":"<p>Point cloud containing intensity is published on the ROS2 topic via <code>RglLidarPublisher</code> component. The intensity value is stored in the <code>intensity</code> field of the <code>sensor_msgs/PointCloud2</code> message.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#instance-segmentation","title":"Instance segmentation","text":"<p><code>RGL Unity Plugin</code> allows assigning an ID to GameObjects to produce a point cloud containing information about hit objects. It can be used for instance/semantic segmentation tasks. This tutorial describes how to do it.</p> <p>LidarInstanceSegmentationDemo</p> <p>If you would like to see how <code>LidarInstanceSegmentationDemo</code> works using <code>RGL</code> or run some tests, we encourage you to familiarize yourself with this section.</p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#output-data_2","title":"Output data","text":"<p>Point cloud containing hit objects IDs is published on the ROS2 topic via <code>RglLidarPublisher</code> component. It is disabled by default. Properties related to this feature are marked below:</p> <p></p>"},{"location":"Components/Sensors/LiDARSensor/LiDARSensor/#dictionary-mapping","title":"Dictionary mapping","text":"<p>The resulting simulation data contains only the id of objects without their human-readable names. To facilitate the interpretation of such data, a function has been implemented to save a file with a dictionary mapping instance ID to GameObject names. It writes pairs of values in the <code>yaml</code> format:</p> <ul> <li>The name of the GameObject</li> <li>Category ID of <code>SemanticCategory</code> component</li> </ul> <p>To enable saving dictionary mapping set output file path to the <code>Semantic Category Dictionary File</code> property in the <code>Scene Manager</code> component:</p> <p></p> <p>The dictionary mapping file will be saved at the end of the simulation.</p>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/","title":"RGLUnityPlugin","text":""},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#rglunityplugin","title":"RGLUnityPlugin","text":"<p>Robotec GPU Lidar (<code>RGL</code>) is an open source high performance lidar simulator running on CUDA-enabled GPUs. It is a cross-platform solution compatible with both Windows and Linux operating systems. <code>RGL</code> utilizes <code>RTX</code> cores for acceleration, whenever they are accessible.</p> <p><code>RGL</code> is used in AWSIM for performance reasons. Thanks to it, it is possible to perform a large number of calculations using the GPU, which is extremely helpful due to the size of the scenes. AWSIM is integrated with <code>RGL</code> out-of-the-box - using <code>RGLUnityPlugin</code> asset.</p> <p>Warning</p> <p>If you want to use <code>RGL</code> in your scene, make sure the scene has an <code>RGLSceneManager</code> component added and all objects meet the usage requirements.</p>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#concept","title":"Concept","text":"<p>Describing the concept of using <code>RGL</code> in AWSIM, we distinguish:</p> <ul> <li>Mesh - a handle to the on-GPU data of the 3D model of objects that in AWSIM are provided in the form of Mesh Filter component. <code>RGLUnityPlugin</code> supports two types of meshes: static (rendered by Mesh Renderer) and animated (rendered by Skinned Mesh Renderer). Static meshes could be shared between Entities.</li> </ul> <ul> <li>Entity - represents a 3D object on the scene with its position and rotation. It consists of a lightweight reference to a Mesh and a transformation matrix of the object.</li> </ul> <ul> <li>Scene - a location where raytracing occurs. It is a set of entites uploaded by <code>SceneManager</code> script to the RGL Native Library.</li> </ul> <ul> <li>Node - performs specific operations such as setting rays for raytracing, transforming rays, performing raytracing, and manipulating output formats. In AWSIM, the main sequence of <code>RGL</code> nodes that simulates LiDAR is created in the <code>LidarSensor</code> script. Other scripts usually create nodes to get requested output or preprocess point cloud, and then connect those nodes to the <code>LidarSensor</code>.</li> </ul> <ul> <li>Graph - a collection of connected Nodes that can be run to calculate results. It allows users to customize functionality and output format by adding or removing Nodes.</li> </ul> <p>Producing a point cloud is based on the use of a Scene containing Entities with Meshes, and placing an Ego Entity with LiDAR sensor that creates a Graph describing ray pattern and performing raytracing. In subsequent frames of the simulation, <code>SceneManager</code> synchronizes the scene between Unity and <code>RGL</code>, and LiDAR sensor updates rays pose on the scene and triggers Graph to perform raytracing and format desired output.</p>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#package-structure","title":"Package structure","text":"<p><code>RGLUnityPlugin</code> asset contains:</p> <ul> <li>Plugins - dynamically loaded libraries for Windows and Linux (<code>*.dll</code> and <code>*.so</code> files).</li> <li>Resources - visualization shader and material.</li> <li>Scripts - scripts for using <code>RGL</code> in the Unity - details below.</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#scripts","title":"Scripts","text":"<ul> <li><code>SceneManager</code> - responsible for syncing the scene between Unity and <code>RGL</code>.</li> <li><code>LidarSensor</code> - provide lidar configuration and create <code>RGL</code> pipeline to simulate lidar.</li> <li><code>PointCloudVisualization</code> - visualize point cloud on the Unity scene.</li> <li><code>IntensityTexture</code> - adds slot for <code>Intensity Texture ID</code> to the GameObject</li> <li><code>SemanticCategory</code> - adds category ID to the GameObject</li> <li><code>RGLDebugger</code> - provides configuration for Native <code>RGL</code> debug tools (logging and tape).</li> <li>A set of classes providing tools to define LiDAR specification (mostly: ray poses):<ul> <li><code>LidarModels</code> - enumeration of some real-world LiDARs names.</li> <li><code>LidarConfiguration</code> - top-level configuration class, horizontal ranges, distance range, laser array.</li> <li><code>LidarConfigurationLibrary</code> - provides a number of pre-defined <code>LidarConfigurations</code>.</li> <li><code>LaserArray</code> - definition of a (vertical) array of lasers.</li> <li><code>LaserArrayLibrary</code> - provides a number of pre-defined <code>LaserArrays</code>.</li> <li><code>Laser</code> - describes offsets of a single laser within a <code>LaserArray</code>.</li> <li><code>LidarNoiseParams</code> - describes a LiDAR noise that can be simulated</li> </ul> </li> <li><code>LowLevelWrappers</code> scripts - provides some convenience code to call Native <code>RGL</code> functions.</li> <li><code>Utilities</code> scripts - miscellaneous utilities to make rest of the code clearer.</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#scenemanager","title":"SceneManager","text":"<p>Each scene needs <code>SceneManager</code> component to synchronize models between Unity and <code>RGL</code>. On every frame, it detects changes in the Unity's scene and propagates the changes to native <code>RGL</code> code. When necessary, it obtains 3D models from GameObjects on the scene, and when they are no longer needed, it removes them.</p> <p>Three different strategies to interact with in-simulation 3D models are implemented. <code>SceneManager</code> uses one of the following policies to construct the scene in <code>RGL</code>:</p> <ul> <li><code>Only Colliders</code> - data is computed based on the colliders only, which are geometrical primitives or simplified Meshes. This is the fastest option, but will produce less accurate results, especially for the animated entities.</li> <li><code>Regular Meshes And Colliders Instead Of Skinned</code> - data is computed based on the regular meshes for static Entities (with <code>MeshRenderers</code> component) and the colliders for animated Entities (with <code>SkinnedMeshRenderer</code> component). This improves accuracy for static Entities with a negligible additional performance cost.</li> <li><code>Regular Meshes And Skinned Meshes</code> - uses regular meshes for both static and animated Entities. This incurs additional performance, but produces the most realistic results.</li> </ul> Mesh Source Strategy Static Entity Animated Entity (NPC) <code>Only Colliders</code> Collider Collider <code>Regular Meshes And Colliders Instead Of Skinned</code> Regular Mesh Collider <code>Regular Meshes And Skinned Meshes</code> Regular Mesh Regular Mesh <p>Mesh source can be changed in the <code>SceneManager</code> script properties:</p> <p></p> <p>Performance</p> <p><code>SceneManager</code> performance depends on mesh source option selected.</p>"},{"location":"Components/Sensors/LiDARSensor/RGLUnityPlugin/#usage-requirements","title":"Usage requirements","text":"<p>Objects, to be detectable by <code>RGL</code>, must fulfill the following requirements:</p> <ol> <li>Contain one of the components: <code>Collider</code>, <code>Mesh Renderer</code>, or <code>Skinned Mesh Renderer</code> - it depends on <code>SceneManager</code> mesh source parameter.</li> <li> <p>Be readable from CPU-accessible memory - it can be achieved using the <code>Read/Write Enabled</code> checkbox in mesh settings.</p> <p>Readable objects</p> <p>Primitive Objects are readable by default.</p> <p>Example</p> <p>The activated Readable option in the mesh should look like this.</p> <p></p> </li> </ol>"},{"location":"Components/Sensors/LiDARSensor/ReadMaterialInformation/","title":"Read Material Information","text":"<p><code>RGL Unity Plugin</code> allows to:</p> <ul> <li>assigning an <code>Intensity Texture</code> to the GameObjects to produce a point cloud containing information about the lidar ray intensity of hit. It can be used to distinguish different levels of an object's reflectivity.</li> <li>assigning an ID to GameObjects to produce a point cloud containing information about hit objects. It can be used for instance/semantic segmentation tasks. Below describes how to ensure the publication of this information.</li> </ul>"},{"location":"Components/Sensors/LiDARSensor/ReadMaterialInformation/#add-intensity-texture-assignment","title":"Add Intensity Texture assignment","text":"<p>To enable reading material information, add <code>IntensityTexture</code> component to every <code>GameObject</code> that is expected to have non-default intensity values.</p> <p></p> <p>After that desired texture has to be inserted into the <code>Intensity Texture</code> slot.</p> <p>The texture has to be in <code>R8</code> format. That means <code>8bit</code> in the red channel (<code>255</code> possible values).</p> <p></p> <p>When the texture is assigned, the intensity values will be read from the texture and added to the point cloud if and only if the mesh component in the <code>GameObject</code> has a set of properly created texture coordinates.</p> <p>The expected number of texture coordinates is equal to the number of vertices in the mesh. The quantity of indices is not relevant. In other cases, the texture will be no read properly.</p>"},{"location":"Components/Sensors/LiDARSensor/ReadMaterialInformation/#add-id-assignment","title":"Add ID assignment","text":"<p>To enable segmentation, add <code>SemanticCategory</code> component to every GameObject that is expected to have a distinct ID. All meshes that belong to a given object will inherit its ID. ID inheritance mechanism allows IDs to be overwritten for individual meshes/objects. This solution also enables the creation of coarse categories (e.g., <code>Pedestrians</code>, <code>Vehicles</code>)</p> <p>Example</p> <p><code>SemanticCategory</code> component is assigned to the <code>Taxi</code> GameObject. All meshes in the <code>Taxi</code> GameObject will have the same instance ID as <code>Taxi</code>:* </p> <p>Example</p> <p>The driver has its own <code>SemanticCategory</code> component, so his instance ID will differ from the rest of the meshes: </p> <p>Example</p> <p><code>SemanticCategory</code> component is assigned to the <code>Vehicles</code> GameObject that contains all of the cars on the scene: </p>"},{"location":"Components/Sensors/LiDARSensor/ReadMaterialInformation/#dictionary-mapping","title":"Dictionary mapping","text":"<p>The resulting simulation data contains only the id of objects without their human-readable names. To facilitate the interpretation of such data, a function has been implemented to save a file with a dictionary mapping instance ID to GameObject names. It writes pairs of values in the <code>yaml</code> format:</p> <ul> <li>The name of the GameObject</li> <li>Category ID of <code>SemanticCategory</code> component</li> </ul> <p>To enable saving dictionary mapping set output file path to the <code>Semantic Category Dictionary File</code> property in the <code>Scene Manager</code> component:</p> <p></p> <p>The dictionary mapping file will be saved at the end of the simulation.</p>"},{"location":"Components/Sensors/VehicleStatusSensor/","title":"Vehicle Status Sensor","text":""},{"location":"Components/Sensors/VehicleStatusSensor/#vehiclestatussensor","title":"VehicleStatusSensor","text":""},{"location":"Components/Sensors/VehicleStatusSensor/#introduction","title":"Introduction","text":"<p><code>VehicleStatusSensor</code> is a component that is designed to aggregate information about the current state of the vehicle. It aggregates information about:</p> <ul> <li>Control mode - currently active control mode, e.g. <code>AUTONOMOUS</code> or <code>MANUAL</code>.</li> <li>Gear status - currently engaged gearbox gear, e.g. <code>DRIVE</code> or <code>REVERSE</code>.</li> <li>Steering status - current angle of the steering tire in radians left, e.g. <code>0.1745</code> (10\u00b0).</li> <li>Turn indicators status - current status of the direction indicators, e.g. <code>DISABLE</code> or <code>ENABLE_LEFT</code>.</li> <li>Hazard lights status - current status of the hazard lights, e.g. <code>DISABLE</code> or <code>ENABLE</code>.</li> <li>Velocity status - current lateral, longitudinal and heading velocities values, e.g <code>{0.2, 0.0, 0.0}</code>.</li> </ul>"},{"location":"Components/Sensors/VehicleStatusSensor/#prefab","title":"Prefab","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/VehicleStatusSensor.prefab\n</code></pre>"},{"location":"Components/Sensors/VehicleStatusSensor/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p>This sensor is added directly to the URDF link in the <code>EgoVehicle</code> prefab.</p> <p></p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p>"},{"location":"Components/Sensors/VehicleStatusSensor/#components","title":"Components","text":"<p>All features are implemented within the Vehicle Report Ros2 Publisher (script) which can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/*\n</code></pre>"},{"location":"Components/Sensors/VehicleStatusSensor/#vehicle-report-ros2-publisher-script","title":"Vehicle Report Ros2 Publisher (script)","text":"<p>The script is responsible for updating and publishing each of the aggregated data on a separate topic. Therefore, it has 6 publishers publishing the appropriate type of message with a constant frequency - one common for all data.</p>"},{"location":"Components/Sensors/VehicleStatusSensor/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>* Report Topic</code> - topic on which suitable type of information is published(default: listed in the table below)</li> <li><code>Publish Hz</code> - frequency of publications on each topic(default: <code>30Hz</code>)</li> <li><code>Frame ID</code> - frame in which data is published, used in <code>Header</code>(default: <code>base_link</code>)</li> <li><code>QoS</code>- Quality of service profile used in the publication(default assumed as <code>\"system_default\"</code>: <code>Reliable</code>, <code>Volatile</code>, <code>Keep last/1</code>)</li> <li><code>Vehicle</code> - the object from which all published data are read(default: <code>None</code>)</li> </ul> <p>Vehicle configuration</p> <p>An important element of the script configuration that must be set is the scene Object (<code>Vehicle</code>). It will be used for reading all the data needed. The appropriate <code>EgoVehicle</code> object should be selected.</p> <p>If you can't select the right object, make sure it's set up correctly - it has got added all the scripts needed for <code>EgoVehicle</code>.</p>"},{"location":"Components/Sensors/VehicleStatusSensor/#published-topics","title":"Published topics","text":"<ul> <li>Frequency: <code>30Hz</code></li> <li>QoS: <code>Reliable</code>, <code>Volatile</code>, <code>Keep last/1</code></li> </ul> Category Topic Message type <code>frame_id</code> Control mode <code>/vehicle/status/control_mode</code> <code>autoware_vehicle_msgs/ControlModeReport</code> - Gear status <code>/vehicle/status/gear_status</code> <code>autoware_vehicle_msgs/GearReport</code> - Steering status <code>/vehicle/status/steering_status</code> <code>autoware_vehicle_msgs/SteeringReport</code> - Turn indicators status <code>/vehicle/status/turn_indicators_status</code> <code>autoware_vehicle_msgs/TurnIndicatorsReport</code> - Hazard lights status <code>/vehicle/status/hazard_lights_status</code> <code>autoware_vehicle_msgs/HazardLightsReport</code> - Velocity status <code>/vehicle/status/velocity_status</code> <code>autoware_vehicle_msgs/VelocityReport</code> <code>base_line</code>"},{"location":"Components/Traffic/NPCs/Pedestrian/","title":"Pedestrian","text":""},{"location":"Components/Traffic/NPCs/Pedestrian/#introduction","title":"Introduction","text":"<p><code>NPCPedestrian</code> is an object that simulates a human standing or moving on the scene. It can move cyclically in any chosen place thanks to the available scripts. Traffic light tracking will be implemented in the future.</p> <p></p> <p>Sample scene</p> <p>If you would like to see how <code>NPCPedestrian</code> works or run some tests, we encourage you to familiarize yourself with the <code>NPCPedestrianSample</code> default scene described in this section.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#prefab-and-fbx","title":"Prefab and Fbx","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/NPCs/Pedestrians/humanElegant.prefab\n</code></pre>"},{"location":"Components/Traffic/NPCs/Pedestrian/#visual-elements","title":"Visual elements","text":"<p>Prefab is developed using models available in the form of <code>*.fbx</code> file. From this file, the visual elements of the model, <code>Animator</code> and <code>LOD</code> were loaded. The <code>Animator</code> and <code>LOD</code> are added as components of the main-parent GameObject in prefab, while the visual elements of the model are added as its children.</p> <p><code>*.fbx</code> file can be found under the following path:</p> <pre><code>Assets/AWSIM/Models/NPCs/Pedestrians/Human/humanElegant.fbx\n</code></pre> <p><code>NPCPedestrian</code> prefab has the following content:</p> <p></p> <p>The <code>ReferencePoint</code> is used by the NPC Pedestrian (script) described here.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p>Pedestrians implemented in the scene are usually added in one aggregating object - in this case it is <code>NPCPedestrians</code>. This object is added to the <code>Environment</code> prefab.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#components","title":"Components","text":"<p>There are several components responsible for the full functionality of <code>NPCPedestrian</code>:</p> <ul> <li>Animator - provides motion animations in the scene, which are composed of clips controlled by the controller.</li> <li>LOD Group - provides level of detail configuration for shaders - affects GPU usage.</li> <li>Rigidbody - ensures that the object is controlled by the physics engine in Unity - e.g. pulled downward by gravity.</li> <li>NPC Pedestrian (script) - ensures that the movement of the object is combined with animation.</li> <li>Simple Pedestrian Walker Controller (script) - provides pedestrian movement in a specific direction and distance in a cyclical manner.</li> </ul> <p>Scripts can be found under the following path:</p> <pre><code>Assets/AWSIM/Scripts/NPCs/Pedestrians/*\n</code></pre>"},{"location":"Components/Traffic/NPCs/Pedestrian/#rigidbody","title":"Rigidbody","text":"<p><code>Rigidbody</code> ensures that the object is controlled by the physics engine. In order to connect the animation to the object, the <code>Is Kinematic</code> option must be enabled. By setting  <code>Is Kinematic</code>, each <code>NPCPedestrian</code> object will have no physical interaction with other objects - it will not react to a vehicle that hits it. The <code>Use Gravity</code> should be turned off - the correct position of the pedestrian in relation to the ground is ensured by the NPC Pedestrian (script). In addition, <code>Interpolate</code> should be turned on to ensure the physics engine's effects are smoothed out.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#lod-level-of-detail","title":"LOD (Level of Detail)","text":"<p><code>LOD</code> provides dependence of the level of detail of the object depending on the ratio of the GameObject\u2019s screen space height to the total screen height. The pedestrian model has two object groups: suffixed <code>LOD0</code> and <code>LOD1</code>. <code>LOD0</code> objects are much more detailed than <code>LOD1</code> - they have many more vertices in the Meshes. Displaying complex meshes requires more performance, so if the GameObject is a small part of the screen, less complex <code>LOD1</code> objects are used.</p> <p>In the case of the <code>NPCPedestrian</code> prefab, if its object is less than 25% of the height of the screen then objects with the <code>LOD1</code> suffix are used. For values less than 1% the object is culled.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#animator","title":"Animator","text":"<p><code>Animator</code> component provides animation assignments to a GameObject in the scene. It uses a developed <code>Controller</code> which defines which animation clips to use and controls when and how to blend and transition between them.</p> <p>The <code>AnimationController</code> for humans should have the two float parameters for proper transitions. Transitions between animation clips are made depending on the values of these parameters:</p> <ul> <li><code>moveSpeed</code> - pedestrian movement speed in \\({m}/{s}\\),</li> <li><code>rotateSpeed</code> - pedestrian rotation speed in \\({rad}/{s}\\).</li> </ul> <p>Developed controller can be found in the following path: <code>Assets/AWSIM/Models/NPCs/Pedestrians/Human/Human.controller</code></p> <p></p> <p>Walking to running transition</p> <p>The example shows the state of walking and then transitions to running as a result of exceeding the condition \\(\\mathrm{moveSpeed} &gt; 1.6\\)</p> <p> </p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#npc-pedestrian-script","title":"NPC Pedestrian (script)","text":"<p>The script takes the <code>Rigidbody</code> and <code>Animator</code> components and combines them in such a way that the actual animation depends on the movement of <code>Rigidbody</code>. It provides an inputs that allows the pedestrian to move - change his position and orientation. In addition, the <code>ReferencePoint</code> point is used to ensure that the pedestrian follows the ground plane correctly.</p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<ul> <li><code>Ray Cast Max Distance</code> - ray-cast max distance for locating the ground.</li> <li><code>Ray Cast Origin Offset</code> - upward offset of the ray-cast origin from the GameObject local origin for locating the ground.</li> </ul>"},{"location":"Components/Traffic/NPCs/Pedestrian/#input-data","title":"Input Data","text":"Category Type Description SetPosition Vector3 Move the <code>NPCPedestrian</code> so that the reference point is at the specified coordinates. SetRotation Vector3 Rotate the <code>NPCPedestrian</code> so that the orientation of the reference point becomes the specified one."},{"location":"Components/Traffic/NPCs/Pedestrian/#simple-pedestrian-walker-controller-script","title":"Simple Pedestrian Walker Controller (script)","text":"<p>Simple Pedestrian Walker Controller is a script that allows the pedestrian to cyclically move back and forth along a straight line. One-way motion is performed with a fixed time as parameter <code>Duration</code> and a constant linear velocity as parameter <code>Speed</code>. The script obviously uses the <code>NPCPedestrian</code> controls provided by the NPC Pedestrian (script) inputs.</p> <p>Pedestrian walking on the sidewalk</p> <p> </p>"},{"location":"Components/Traffic/NPCs/Pedestrian/#collider","title":"Collider","text":"<p><code>Collider</code> is an optional pedestrian component. By default, <code>NPCPedestrian</code> doesn't have this component added, It can be added if you want to detect a collision, e.g. with an <code>EgoVehicle</code>. There are several types of colliders, choose the right one and configure it for your own requirements.</p> <p>Capsule Collider</p> <p>An example of a <code>CapsuleCollider</code> that covers almost the entire pedestrian.</p> <p></p>"},{"location":"Components/Traffic/NPCs/Vehicle/","title":"Vehicle","text":""},{"location":"Components/Traffic/NPCs/Vehicle/#npcvehicle","title":"NPCVehicle","text":""},{"location":"Components/Traffic/NPCs/Vehicle/#introduction","title":"Introduction","text":"<p><code>NPCVehicle</code> is a non-playable object that simulates a vehicle that is stationary or moving around the scene. It can move on roads, more specifically <code>TrafficLanes</code>, thanks to the use of <code>TrafficSimulator</code> - which you can read more about here. Vehicles moving on the scene take into account each other - avoiding collisions, follow traffic lights and have an implemented mechanism of yielding the right of way.</p> <p></p> <p>Sample scene</p> <p>If you would like to see how <code>NPCVehicle</code> works or run some tests, we encourage you to familiarize yourself with the <code>NPCVehicleSample</code> default scene described in this section.</p> <p>Ego Vehicle</p> <p>If you are interested in the most important vehicle on the scene - <code>Ego Vehicle</code>, we encourage you to read this section.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#prefabs-and-fbxs","title":"Prefabs and Fbxs","text":"<p>Prefabs can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/NPCs/Vehicles/*\n</code></pre> <p>The table shows the available prefabs of the vehicles:</p> Hatchback SmallCar Taxi Truck Van Appearance Prefab <code>Hatchback.prefab</code> <code>SmallCar.prefab</code> <code>Taxi-64.prefab</code> <code>Truck_2t.prefab</code> <code>Van.prefab</code> <p><code>NPCVehicle</code> prefab has the following content:</p> <p></p> <p>As you can see, it consists of 2 parents for GameObjects: <code>Visuals</code> - aggregating visual elements, <code>Colliders</code> - aggregating colliders and single object <code>CoM</code>.  All objects are described in the sections below.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#visual-elements","title":"Visual elements","text":"<p>Prefabs are developed using models available in the form of <code>*.fbx</code> files. For each vehicle, the visuals elements and <code>LOD</code> were loaded from the appropriate <code>*.fbx</code> file. The <code>LOD</code> is always added as components of the main-parent GameObject in prefab, while the visual elements of the model are aggregated and added in object <code>Visuals</code>.</p> <p><code>*.fbx</code> file for each vehicle is located in the appropriate <code>Models</code> directory for the vehicle under the following path:</p> <pre><code>Assets/AWSIM/Models/NPCs/Vehicles/&lt;vehicle_name&gt;/Models/&lt;vehicle_name&gt;.fbx\n</code></pre> <p>As you can see, the additional visual element is <code>Driver</code>.</p> <p></p> <p>It was also loaded from the <code>*.fbx</code> file which can be found under the following path:</p> <pre><code>Assets/AWSIM/Models/NPCs/Vehicles/Driver/Model/Driver.fbx\n</code></pre> <p>Vehicle fbx</p> <p>The content of a sample <code>*.fbx</code> file is presented below, all elements except <code>Collider</code> have been added to the prefab as visual elements of the vehicle. <code>Collider</code> is used as the Mesh source for the <code>Mesh Collider</code> in the <code>BodyCollider</code> object.</p> <p>.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#link","title":"Link","text":"<p>The default scene does not have vehicles implemented in fixed places, but they are spawned by <code>RandomTrafficSimulator</code> which is located in the <code>Environment</code> prefab. Therefore, before starting the simulation, no <code>NPCVehicle</code> object is on the scene.</p> <p>When you run the simulation, you can see objects appearing as children of <code>RandomTrafficSimulator</code>:</p> <p></p> <p>In each <code>NPCVehicle</code> prefab, the local coordinate system of the vehicle (main prefab link) should be defined in the axis of the rear wheels projected onto the ground - in the middle of the distance between them. This aspect holds significance when characterizing the dynamics of the object, as it provides convenience in terms of describing its motion and control.</p> <p> </p>"},{"location":"Components/Traffic/NPCs/Vehicle/#components","title":"Components","text":"<p>There are several components responsible for the full functionality of <code>NPCVehicle</code>:</p> <ul> <li>LOD Group - provides level of detail configuration for shaders - affects GPU usage.</li> <li>Rigidbody - ensures that the object is controlled by the physics engine in Unity - e.g. pulled downward by gravity.</li> <li>NPC Vehicle (script) - provides the ability to change the position and orientation of the vehicle, as well as to control the turn signals and brake light.</li> </ul> <p>Script can be found under the following path:</p> <pre><code>Assets/AWSIM/Scripts/NPCs/Vehicles\n</code></pre>"},{"location":"Components/Traffic/NPCs/Vehicle/#com","title":"CoM","text":"<p><code>CoM</code> (Center of Mass) is an additional link that is defined to set the center of mass in the <code>Rigidbody</code>. The NPC Vehicle (script) is responsible for its assignment. This measure should be defined in accordance with reality. Most often, the center of mass of the vehicle is located in its center, at the height of its wheel axis - as shown below.</p> <p> </p>"},{"location":"Components/Traffic/NPCs/Vehicle/#colliders","title":"Colliders","text":"<p>Colliders are used to ensure collision between objects. In <code>NPCVehicle</code>, the main <code>BodyCollider</code> collider and <code>Wheels Colliders</code> colliders for each wheel were added.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#body-collider","title":"Body Collider","text":"<p><code>BodyCollider</code> is a vehicle Object responsible for ensuring collision with other objects. Additionally it can be used to detect these collisions. The <code>MeshCollider</code> uses a Mesh of an Object to build its <code>Collider</code>. The Mesh for the <code>BodyCollider</code> was also loaded from the <code>*.fbx</code> file similarly to the visual elements.</p> <p></p>"},{"location":"Components/Traffic/NPCs/Vehicle/#wheels-colliders","title":"Wheels Colliders","text":"<p><code>WheelsColliders</code> are an essential element from the point of view of driving vehicles on the road. They are the only ones that have contact with the roads and it is important that they are properly configured. Each vehicle, apart from the visual elements related to the wheels, should also have 4 colliders - one for each wheel.</p> <p>To prevent inspector entry for <code>WheelCollider</code> the <code>WheelColliderConfig</code> has been developed. It ensures that friction is set to 0 and only wheel suspension and collisions are enabled.</p> <p></p> <p>Wheel Collider Config</p> <p>For a better understanding of the meaning of <code>WheelCollider</code> we encourage you to read this manual.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#lod","title":"LOD","text":"<p><code>LOD</code> provides dependence of the level of detail of the object depending on the ratio of the GameObject\u2019s screen space height to the total screen height. Vehicle models have only one <code>LOD0</code> group, therefore there is no reduction in model complexity when it does not occupy a large part of the screen. It is only culled when it occupies less than 2% of the height.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#rigidbody","title":"Rigidbody","text":"<p><code>Rigidbody</code> ensures that the object is controlled by the physics engine. The <code>Mass</code> of the vehicle should approximate its actual weight. In order for the vehicle to physically interact with other objects - react to collisions, <code>Is Kinematic</code> must be turned off. The <code>Use Gravity</code> should be turned on - to ensure the correct behavior of the body during movement. In addition, <code>Interpolate</code> should be turned on to ensure the physics engine's effects are smoothed out.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#npc-vehicle-script","title":"NPC Vehicle (script)","text":"<p>The script takes the <code>Rigidbody</code> and provides an inputs that allows the <code>NPCVehicle</code> to move. Script inputs give the ability to set the position and orientation of the vehicle, taking into account the effects of suspension and gravity. In addition, the script uses the <code>CoM</code> link reference to assign the center of mass of the vehicle to the <code>Rigidbody</code>.</p> <p>Script inputs are used by <code>RandomTrafficSimulator</code>, which controls the vehicles on the scene - it is described here.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#input-data","title":"Input Data","text":"Category Type Description SetPosition Vector3 Move the <code>NPCVehicle</code> so that its x, z coordinates are same as the specified coordinates. Pitch and roll are determined by physical operations that take effects of suspension and gravity into account. SetRotation Vector3 Rotate the <code>NPCVehicle</code> so that its yaw becomes equal to the specified one. Vertical movement is determined by physical operations that take effects of suspension and gravity into account. <p><code>Visual Object Root</code> is a reference to the parent aggregating visuals, it can be used to disable the appearance of visual elements of the <code>NPCVehicle</code> in the scene.</p> <p>Whereas <code>Bounds</code> Represents an axis aligned bounding box of the <code>NPCVehicle</code>. It is used primarily to detect collisions between vehicles in the event of spawning, yielding and others. Moreover, vehicle bounds are displayed by Gizmos.</p> <p></p> <p>The settings of the remaining elements, i.e. the <code>Axle</code> and the <code>Lights</code>, are described here and here.</p> <p>No Gizmo visualization</p> <p>If you don't see Gizmo's visual elements, remember to turn them on.</p> <p></p>"},{"location":"Components/Traffic/NPCs/Vehicle/#axle-settings","title":"Axle Settings","text":"<p>This part of the settings is responsible for the proper connection of visual elements with the collider for each wheel - described earlier. The objects configured in this section are used to control the vehicle - its wheel speed and steering angle, which are calculated based on the input values. Correct configuration is very important from the point of view of the <code>NPCVehicle</code> movement on the road.</p>"},{"location":"Components/Traffic/NPCs/Vehicle/#lights-settings","title":"Lights Settings","text":"<p>This part of the settings is related to the configuration of materials emission - used when a specific lighting is activated. There are 3 types of lights: <code>Brake</code>, <code>Left Turn Signal</code> and <code>Right Turn Signal</code>. Each of the lights has its visual equivalent in the form of a Mesh. In the case of <code>NPCVehicle</code> all of the lights are included in the <code>Body</code> object Mesh, which has many materials - including those related to lights.</p> <p>For each type of light, the appropriate <code>Material Index</code> (equivalent of element index in mesh) and <code>Lighting Color</code> are assigned - yellow for <code>Turn Signals</code>, red for <code>Break</code>.</p> <p><code>Lighting Intensity</code> values are also configured - the greater the value, the more light will be emitted. This value is related to <code>Lighting Exposure Weight</code> parameter that is an exposure weight - the lower the value, the more light is emitted.</p> <p>The brake light is switched on depending on the speed of the <code>NPCVehicle</code>, while <code>RandomTrafficSimulator</code> is responsible for switching the turn signals on and off.</p> <p></p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/","title":"Add Random Traffic Environment","text":""},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#add-environment-for-random-traffic","title":"Add Environment for Random Traffic","text":"<p>This document describes the steps to properly configuer <code>RandomTrafficSimulator</code> in your environment.</p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#map-preparation","title":"Map preparation","text":"<p>The 3D map model should be added to the scene. Please make sure that the <code>Environment</code> component with appropriate <code>mgrsOffsetPosition</code> is attached to the root GameObject. </p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#annotate-traffic-lights","title":"Annotate Traffic Lights","text":"<p>Please attach <code>TrafficLight</code> component to all traffic light GameObjects placed on scene.  </p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#load-lanelet","title":"Load Lanelet","text":"<p>The lanelet load process can be performed by opening <code>AWSIM -&gt; Random Traffic -&gt; Load Lanelet</code> at the top toolbar of Unity Editor. </p> <p>You should be prompted with a similar window to the one presented below. Please adjust the parameters for the loading process if needed.</p> <p></p> <p>Waypoint settings affect the density and accuracy of the generated waypoints. The parameters are described below:</p> <ul> <li>Resolution: resolution of resampling. Lower values provide better accuracy at the cost of processing time.</li> <li>Min Delta Length: minimum length(m) between adjacent points.</li> <li>Min Delta Angle: minimum angle(deg) between adjacent edges. Lowering this value produces a smoother curve.</li> </ul> <p>To generate the Lanelet2 map representation in your simulation, please click the <code>Load</code> button. Environment components should be generated and placed as child objects of the <code>Environment</code> GameObject. You can check their visual representation by clicking consecutive elements in the scene hierarchy.</p> <p></p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#annotate-traffic-intersections","title":"Annotate Traffic Intersections","text":"<p>To annotate intersection please, add an empty GameObject named <code>TrafficIntersections</code> at the same level as the <code>TrafficLanes</code> GameObject.</p> <p>For each intersection repeat the following steps:</p> <ol> <li>Add an GameObject named <code>TrafficIntersection</code> as a child object of the <code>TrafficIntersections</code> object.</li> <li>Attach a <code>TrafficIntersection</code> component to it.</li> <li>Add a  <code>BoxCollider</code> as a component of GameObject. It's size and position should cover the whole intersection. This is used for detecting vehicles in the intersection.</li> <li>Set <code>TrafficLightGroups</code>. Each group is controlled to have different signals, so facing traffic lights should be added to the same group. These groupings are used in traffic signal control.</li> <li>Specify the signal control pattern.</li> </ol>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#annotate-right-of-ways-on-uncontrolled-intersections","title":"Annotate right of ways on uncontrolled intersections","text":"<p>For the vehicles to operate properly it is needed to annotate the right of way of <code>TrafficLane</code> manually on intersections without traffic lights.</p> <p></p> <p>To set the right of way, please:</p> <ul> <li>Select a straight lane that is not right of way in the intersection. The selected lane should be highlighted as presented below.</li> <li>Click the <code>Set RightOfWays</code> button to give the lane priority over other lanes. </li> <li>Please check if all lanes that intersect with the selected lane are highlighted yellow. This means that the right of way was applied correctly. </li> </ul>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#annotate-stop-lines","title":"Annotate stop lines","text":"<p>For each right turn lane that yields to the opposite straight or left turn lane, a stop line needs to be defined near the center of the intersection.  If there is no visible stop line, a <code>StopLine</code> component should be added to the scene, near the center of the intersection and associated with <code>TrafficLane</code>.</p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#assign-intersection-trafficlanes","title":"Assign Intersection TrafficLanes","text":"<p>To make the yielding rules work properly, it is necessary to catagorize the <code>TrafficLanes</code>. The ones that belong to an intersection have the <code>IntersectionLane</code> variable set to true.</p> <p>To automate the assignment of the corresponding <code>IntersectionLane</code> to each <code>TrafficLane</code>, the script <code>AssignIntersectionTrafficLanes</code> can be used. </p> <ol> <li>At the time of assignment, add it as a component to some object in the scene (e.g. to the <code>Environment</code> object).</li> <li>Disable the component (uncheck the checkbox next to the script name).</li> <li>Assign to <code>TrafficLanesObjectsParent</code> GameObject, which contains all <code>TrafficLanes</code> objects.</li> <li>Check all 4 options.</li> <li>Enable the component (check the checkbox next to the script name).</li> </ol> <p>Check the log to see if all operations were completed: </p> <p>As a result, the names of <code>TrafficLane</code> objects should have prefixes with sequential numbers and <code>TrafficLane</code> at intersections should be marked. <code>TrafficLanes</code> with <code>IntersectionLane</code> set to True are displayed by Gizmos in green color, if <code>IntersectionLane</code> is False their color is white.  </p>"},{"location":"Components/Traffic/RandomTraffic/AddRandomTrafficEnvironment/#check-final-configuration","title":"Check final configuration","text":"<p>Once all the components are ready, the simulation can be run. Check carefully if the vehicles are moving around the map correctly. For each intersection, review the settings of the relevant components if vehicles are unable to proceed.</p>"},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/","title":"Random Traffic Simulator","text":""},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#random-traffic-simulator","title":"Random Traffic Simulator","text":"<p>The <code>RandomTrafficSimulator</code> simulates city traffic with respect to all traffic rules. The system allows for random selection of car models and the paths they follow. It also allows adding static vehicles in the simulation.</p> <p></p>"},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#getting-started","title":"Getting Started","text":""},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#overview","title":"Overview","text":"<p>The random traffic system consists of the following components:</p> <ul> <li><code>RandomTrafficSimulator</code>:  manages lifecycle of NPCs and simulates NPC behaviours.</li> <li><code>TrafficLane</code>, <code>TrafficIntersection</code> and <code>StopLine</code>: represent traffic entities</li> <li><code>NPCVehicle</code>: vehicle models (NPCs) controlled by <code>RandomTrafficSimulator</code></li> </ul> <p></p>"},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#components-settings","title":"Components Settings","text":"<p>The following section describes Unity Editor components settings.</p>"},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#random-traffic-simulator_1","title":"Random Traffic Simulator","text":"Parameter Description General Settings Seed Seed value for random generator Ego Vehicle Transform of ego vehicle Vehicle Layer Mask LayerMask that masks only vehicle(NPC and ego) colliders Ground Layer Mask LayerMask that masks only ground colliders of the map NPC Vehicle Settings Max Vehicle Count Maximum number of NPC vehicles to be spawned in simulation NPC Prefabs Prefabs representing controlled vehicles. They must have <code>NPCVehicle</code> component attached. Spawnable Lanes <code>TrafficLane</code> components where NPC vehicles can be spawned during traffic simulation Vehicle Config Parameters for NPC vehicle control<code>Sudden Deceleration</code> is a deceleration related to emergency braking Debug Show Gizmos Enable the checkbox to show editor gizmos that visualize behaviours of NPCs"},{"location":"Components/Traffic/RandomTraffic/RandomTrafficSimulator/#gizmos","title":"Gizmos","text":"<p>Gizmos are useful for checking current behavior of NPCs and its causes. Gizmos have a high computational load so please disable them if the simulation is laggy. </p>"},{"location":"Components/Traffic/RandomTraffic/YieldingRules/","title":"Yielding Rules","text":""},{"location":"Components/Traffic/RandomTraffic/YieldingRules/#yielding-rules","title":"Yielding rules","text":"<p>The <code>RandomTrafficSimulator</code> assumes that there are 10 phases of yielding priority:</p> <p>RandomTrafficYielding scene</p> <p>If you would like to see how <code>RandomTrafficSimulator</code> with yielding rules works or run some tests, we encourage you to familiarize yourself with the <code>RandomTrafficYielding</code> scene described in this section.</p> <ol> <li> <p><code>NONE</code> - state in which it is only checked if a vehicle is approaching the intersection. If yes, a transition to state <code>ENTERING_INTERSECTION</code> is made.</p> </li> <li> <p><code>ENTERING_INTERSECTION</code> - state in which it is checked if any of the situations <code>LANES_RULES_ENTERING_INTERSECTION</code>, <code>LEFT_HAND_RULE_ENTERING_INTERSECTION</code>,  <code>INTERSECTION_BLOCKED</code>  occur, if yes the state of the vehicle is changed to one matching the situation - to determine if the vehicle must yield priority. If none of these situations occur only the entry into the intersection will result in a transition to <code>AT_INTERSECTION</code>.</p> </li> <li> <p><code>AT_INTERSECTION</code> - state in which it is checked if any of the situations <code>LANES_RULES_AT_INTERSECTION</code>, <code>LEFT_HAND_RULE_AT_INTERSECTION</code>,  <code>FORCING_PRIORITY</code>  occur, if yes the state of the vehicle is changed to one matching the situation - to determine if the vehicle must yield priority. If none of these situations occur only leaving the intersection will result in a transition to <code>NONE</code>.</p> </li> <li> <p><code>INTERSECTION_BLOCKED</code> -  when vehicle A is approaching the intersection, it yields priority to vehicle B, which should yield priority, but is forcing it - this refers to a situation in which vehicle B has entered the intersection and has already passed its stop point vehicle B isn\u2019t going to stop but has to leave the intersection. Until now, vehicle A has continued to pass through the intersection without taking vehicle B into account, now it is checking if any vehicle is forcing priority (vehicle A has <code>INTERSECTION_BLOCKED</code> state). (vehicle A is red car with blue sphere, B is the white car to which it points) </p> </li> <li> <p><code>LEFT_HAND_RULE_ENTERING_INTERSECTION</code> -  vehicle A, before entering the intersection where the traffic lights are off, yields priority to vehicles (ex. B) that are approaching to the intersection and are on the left side of vehicle A. Until now, situations in which the lights are off were not handled. If a vehicle didn't have a red light and was going straight - it just entered the intersection. Now vehicle A checks if the vehicles on the left (ex. B) have a red light, if not it yields them priority. (vehicle A is truck car with gray sphere, B is the white car to which it points) </p> </li> <li> <p><code>LEFT_HAND_RULE_AT_INTERSECTION</code> -  when vehicle A is already at the intersection, yields priority to vehicles (ex. B) that are also at the intersection and are on its left side - in cases where no other yielding rules are resolved between them (i.e. there are no <code>RightOfWayLanes</code> between them). (vehicle A is red, B is white) </p> </li> <li> <p><code>LANES_RULES_ENTERING_INTERSECTION</code> - when vehicle B intends to turn left and is approaching at the intersection where it needs to yield to vehicle A which is going straight ahead, then it goes to state <code>LANES_RULES_ENTERING_INTERSECTION</code>. The introduced changes take into account that a vehicle approaching the intersection considers not only the vehicles at the intersection but also those which are approaching it (at a distance of less than <code>minimumDistanceToIntersection</code> to the intersection). (vehicle B is truck with yellow sphere, A the white car to which it points) </p> </li> <li> <p><code>LANES_RULES_AT_INTERSECTION</code> - when vehicle B intends to turn right and is already at the intersection where it needs to yield to vehicle A which is approaching the intersection, then it goes to state <code>LANES_RULES_AT_INTERSECTION</code>. The introduced changes take into account that a vehicle approaching the intersection considers not only the vehicles at the intersection but also those which are approaching it (at a distance of less than <code>minimumDistanceToIntersection</code> to the intersection). (vehicle B is car with red sphere, A the white car to which it points) </p> </li> <li> <p><code>FORCING_PRIORITY</code> -  state in which some vehicle B should yield priority to a vehicle A but doesn't - for some reason, most likely it could be some unusual situation in which all other rules have failed. Then vehicle A which is at intersection yields priority to a vehicle that is forcing priority. In such a situation, vehicle A transitions to state <code>FORCING_PRIORITY</code>. It is very rare to achieve this state, but it does happen.</p> </li> </ol>"},{"location":"Components/Traffic/RandomTraffic/YieldingRules/#gizmos-markings","title":"Gizmos Markings","text":"<ul> <li>No sphere means the vehicle is in one of the following states - <code>NONE</code>, <code>ENTERING_INTERSECTION</code> or <code>AT_INTERSECTION</code>.</li> <li>The gray sphere means that the vehicle is approaching the intersection and yields priority to the vehicle on the left side - <code>LEFT_HAND_RULE_ENTERING_INTERSECTION</code>.</li> <li>The black sphere means that the vehicle is at the intersection and yields priority to a vehicle on the left side - <code>LEFT_HAND_RULE_AT_INTERSECTION</code>.</li> <li>The yellow sphere means that the vehicle yields due to lanes rules before entering the intersection - <code>LANES_RULES_ENTERING_INTERSECTION</code>.</li> <li>The red sphere means that it yields due to lanes rules but is already at the intersection - <code>LANES_RULES_AT_INTERSECTION</code>.</li> <li>The blue sphere means that a vehicle is approaching the intersection and yields priority to a vehicle that is forcing priority - <code>INTERSECTION_BLOCKED</code> - when the turning vehicle begins to yield, then the blue sphere disappears. However, if the turning vehicle continues to turn (does not yield because it has passed the stopping point), then the vehicle going straight stops before the intersection and allows the turning vehicle to leave the intersection.</li> <li>The pink sphere means that a vehicle is at intersection and yields priority to a vehicle that is forcing priority - <code>FORCING_PRIORITY</code>.</li> </ul>"},{"location":"Components/Traffic/TrafficComponents/","title":"Traffic Components","text":"<p>This section</p> <p>This section is still under development!</p> <p>This is a section that describes in detail all components related to simulated traffic in the <code>Environment</code> prefab.</p>"},{"location":"Components/Traffic/TrafficComponents/#architecture","title":"Architecture","text":"<p>The random traffic system consists of the following components:</p> <ul> <li> <p><code>TrafficManager</code></p> <p>It is a top level interface meant to be used on the Unity scene. <code>TrafficManager</code> runs all elements needed for a successful traffic simulation. This component manages all <code>TrafficSimulators</code> so they don't work against each other. It gives you the possibility to configure the <code>TrafficSimulators</code>.</p> </li> </ul> <ul> <li> <p><code>TrafficSimulator</code></p> <p>Technically it is not a component, it is crucial to understand what it is and what it does in order to correctly configure the <code>TrafficManager</code>. <code>TrafficSimulator</code> manages <code>NPCVehicles</code> spawning. There can be many <code>TrafficSimulators</code> on the scene. They are added and configured in the <code>TrafficManager</code> component. Every <code>TrafficSimulator</code> manages some part of the traffic it is responsible for - meaning it has spawned the <code>NPCVehicles</code> and set their configuration.</p> <ul> <li><code>RandomTrafficSimulator</code> - spawns and controls <code>NPCVehicles</code> driving randomly</li> <li><code>RouteTrafficSimulator</code> - spawns and controls <code>NPCVehicles</code> driving on a defined route</li> </ul> <p>TrafficSimulator inaccessibility</p> <p>It is not possible to get direct access to the <code>TrafficSimulator</code>. It should be added and configured through the <code>TrafficManager</code> component.</p> </li> </ul> <ul> <li> <p><code>TrafficLane</code>, <code>TrafficIntersection</code> and <code>StopLine</code></p> <p>These components represent traffic entities. They are used to control and manage the traffic with respect to traffic rules and current road situation.</p> </li> </ul> <ul> <li> <p><code>NPCVehicle</code></p> <p>The vehicle models (NPCs) spawned by one of the <code>TrafficSimulators</code>. They are spawned according to the <code>TrafficSimulator</code> configuration and either drive around the map randomly (when spawned by a <code>RandomTrafficSimulator</code>) or follow the predefined path (when spawned by a <code>RouteTrafficSimulator</code>). <code>NPCVehicles</code> are managed by one central knowledge base.</p> </li> </ul> <p>The process of spawning a <code>NPCVehicle</code> and its later behavior control is presented on the following sequence diagram.</p> <p></p> <p>Sequence Diagram Composition</p> <p>Please note that the diagram composition has been simplified to the level of GameObjects and chosen elements of the GameObjects for the purpose of improving readability.</p>"},{"location":"Components/Traffic/TrafficComponents/#lanelet2","title":"Lanelet2","text":"<p>Lanelet2 is a library created for handling a map focused on automated driving. It also supports ROS and ROS2 natively. In AWSIM Lanelet2 is used for reading and handling a map of all roads. Specifically it does contain all <code>TrafficLanes</code> and <code>StopLines</code>. You may also see us referring to the actual map data file (<code>*.osm</code>) as a Lanelet2.</p> <p>Lanelet2 official page</p> <p>If you want to learn more we encourage to visit the official project page.</p>"},{"location":"Components/Traffic/TrafficComponents/#randomtrafficsimulator","title":"RandomTrafficSimulator","text":"<p>Nomenclature</p> <p>Please note that</p> <ul> <li>a GameObject</li> <li>an element spawning <code>NPCVehicles</code> randomly and</li> <li>the whole system of traffic control</li> </ul> <p>are named <code>RandomTrafficSimulator</code>. Keep this in mind when reading the following page - so you don't get confused.</p> <p><code>RandomTrafficSimulator</code> simulates traffic with respect to all traffic rules. The system allows for random selection of car models and the paths they follow. It also allows adding static vehicles in the simulation.</p>"},{"location":"Components/Traffic/TrafficComponents/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p>The <code>RandomTrafficSimulator</code> consists of several GameObjects.</p> <ul> <li><code>RandomTrafficSimulator</code> - this is an Object consisting of a Traffic Manager (script).     You can learn more about it here.</li> <li><code>TrafficIntersections</code> - this is a parent Object for all <code>TrafficIntersections</code>.     You can learn more about it here.</li> <li><code>TrafficLanes</code> - this is a parent Object for all <code>TrafficLanes</code>.     You can learn more about it here.</li> <li><code>StopLines</code> - this is a parent Object for all <code>StopLines</code>.     You can learn more about it here.</li> </ul>"},{"location":"Components/Traffic/TrafficComponents/#components","title":"Components","text":"<p><code>RandomTrafficSimulator</code> only has one component: Traffic Manager (script) which is described below.</p>"},{"location":"Components/Traffic/TrafficComponents/#trafficmanager-script","title":"TrafficManager (script)","text":"<p>Traffic Manager (script) is responsible for all of top level management of the <code>NPCVehicles</code>. It managed spawning of <code>NPCVehicles</code> on <code>TrafficLanes</code>.</p> <p><code>TrafficManager</code> uses the concept of <code>TrafficSimulators</code>. One <code>TrafficSimulator</code> is responsible for managing its set of <code>NPCVehicles</code>. Every <code>TrafficSimulator</code> spawns its own <code>NPCVehicles</code> independently. The vehicles spawned by one <code>TrafficSimulator</code> do respect its configuration. <code>TrafficSimulators</code> can be interpreted as <code>NPCVehicle</code> spawners with different configurations each. Many different <code>TrafficSimulators</code> can be added to the <code>TrafficManager</code>.</p> <p>If a random mode is selected (<code>RandomTrafficSimulator</code>) then <code>NPCVehicles</code> will spawn in random places (from the selected list) and drive in random directions. To be able to reproduce the behavior of the <code>RandomTrafficSimulator</code> a <code>Seed</code> can be specified - which is used for the pseudo-random numbers generation.</p> <p><code>TrafficManager</code> script also configures all of the spawned <code>NPCVehicles</code>, so that they all have common parameters</p> <ul> <li><code>Acceleration</code> - the acceleration used by the vehicles at all times when accelerating.</li> <li><code>Deceleration</code> - the value of deceleration used in ordinary situations.</li> <li><code>Sudden Deceleration</code> - deceleration used when standard <code>Deceleration</code> is not sufficient to avoid accident.</li> <li><code>Absolute Deceleration</code> - value of deceleration used when no other deceleration allows to avoid the accident.</li> </ul> <p>The <code>Vehicle Layer Mask</code> and <code>Ground Layer Mask</code> are used to make sure all vehicles can correctly interact with the ground to guarantee simulation accuracy.</p> <p><code>Max Vehicle Count</code> specifies how many <code>NPCVehicles</code> can be present on the scene at once. When the number of <code>NPCVehicles</code> on the scene is equal to this value the <code>RandomTrafficSimulator</code> stops spawning new vehicles until some existing vehicles drive away and disappear.</p> <p>The <code>EgoVehicle</code> field provides the information about Ego vehicle used for correct behavior of<code>NPCVehicles</code>when interacting with Ego.</p> <p><code>Show Gizmos</code> checkbox specifies whether the Gizmos visualization should be displayed when running the simulation.</p> <p><code>Show Yielding Phase</code> checkbox specifies whether yielding phases should be displayed by Gizmos - in the form of spheres above vehicles, details in the Markings section.</p> <p><code>Show Obstacle Checking</code> checkbox specifies whether obstacle checking should be displayed by Gizmos - in the form of boxes in front of vehicles</p> <p><code>Show Spawn Points</code> checkbox specifies whether spawn points should be displayed by Gizmos - in the form of flat cuboids on roads.</p> <p>Gizmos performance</p> <p>Gizmos have a high computational load. Enabling them may cause the simulation to lag.</p> <p>As mentioned earlier - <code>TrafficManager</code> may contain multiple <code>TrafficSimulators</code>. The two available variants of <code>TrafficSimulator</code> are described below</p> <ul> <li><code>RandomTrafficSimulator</code></li> <li><code>RouteTrafficSimulator</code></li> </ul> <p><code>TrafficSimulators</code> should be interpreted as spawning configurations for some group of <code>NPCVehicles</code> on the scene.</p>"},{"location":"Components/Traffic/TrafficComponents/#random-traffic","title":"Random Traffic","text":"<p>When using <code>RandomTrafficSimulator</code> the <code>NPCVehicle</code> prefabs (NPC Prefabs) can be chosen as well as Spawnable Lanes. The later are the only <code>TrafficLanes</code> on which the <code>NPCVehicles</code> can spawn. Upon spawning one of the Spawnabe Lanes is chosen and - given the vehicle limits are not reached - one random NPCVehicle from the Npc prefabs list is spawned on that lane. After spawning, the NPCVehicle takes a random route until it drives out of the map - then it is destroyed.</p> <p>The <code>Maximum Spawns</code> field specifies how many Vehicles should be spawned before this <code>TrafficSimulator</code> stops working. Set to <code>0</code> to disable this restriction.</p>"},{"location":"Components/Traffic/TrafficComponents/#route-traffic","title":"Route Traffic","text":"<p>When using <code>Route traffic Simulator</code> the <code>NPCVehicle</code> prefabs (NPC Prefabs) as well as Route can be chosen. The later is an ordered list of <code>TrafficLanes</code> that all spawned vehicles will drive on. Given the vehicle limit is not reached - the <code>RouteTrafficSimulator</code> will spawn one of the Npc Prefabs chosen randomly on the first Route element (<code>Element 0</code>). After the first vehicle drives off the next one will spawn according to the configuration. It is important for all Route elements to be connected and to be arranged in order of appearance on the map. The NPCVehicle disappears after completing the Route.</p> <p>The <code>Maximum Spawns</code> field specifies how many Vehicles should be spawned before this <code>TrafficSimulator</code> stops working. Set to <code>0</code> to disable this restriction.</p>"},{"location":"Components/Traffic/TrafficComponents/#parameter-explanation","title":"Parameter explanation","text":"Parameter Description General Settings Seed Seed value for random generator Ego Vehicle Transform of ego vehicle Vehicle Layer Mask LayerMask that masks only vehicle(NPC and ego) colliders Ground Layer Mask LayerMask that masks only ground colliders of the map Culling Distance Distance at which NPCs are culled relative to EgoVehicle Culling Hz Culling operation cycle NPCVehicle Settings Max Vehicle Count Maximum number of NPC vehicles to be spawned in simulation NPC Prefabs Prefabs representing controlled vehicles. They must have <code>NPCVehicle</code> component attached. Spawnable Lanes <code>TrafficLane</code> components where NPC vehicles can be spawned during traffic simulation Vehicle Config Parameters for NPC vehicle control<code>Sudden Deceleration</code> is a deceleration related to emergency braking Debug Show Gizmos Enable the checkbox to show editor gizmos that visualize behaviours of NPCs"},{"location":"Components/Traffic/TrafficComponents/#traffic-light-script","title":"Traffic Light (script)","text":"<p>Traffic Light (script) is a component added to every <code>TrafficLight</code> on the scene. It is responsible for configuring the <code>TrafficLight</code> behavior - the bulbs and their colors.</p> <p>The <code>Renderer</code> filed points to the renderer that should be configured - in this case it is always a <code>TrafficLight</code> renderer.</p> <p><code>Bulbs Emission Config</code> is a list describing available colors for this Traffic Light. Every element of this list configures the following</p> <ul> <li><code>Bulb Color</code> - the name of the configured color that will be used to reference this color</li> <li><code>Color</code> - the actual color with which a bulb should light up</li> <li><code>Intensity</code> - the intensity of the color</li> <li><code>Exposure Weight</code> - how bright should the color be when lighting up</li> </ul> <p>The <code>Bulb Material Config</code> is a list of available bulbs in a given Traffic Light. Every element describes a different bulb. Every bulb has the following aspects configured</p> <ul> <li><code>Bulb Type</code> - the name that will be usd to reference the configured bulb</li> <li><code>Material Index</code> - The index of a material of the configured bulb.     This is an index of a sub-mesh of the configured bulb in the Traffic Light mesh.     The material indices are described in detail here and here.</li> </ul>"},{"location":"Components/Traffic/TrafficComponents/#trafficintersections","title":"TrafficIntersections","text":"<p><code>TrafficIntersection</code> is a representation of a road intersection. It consists of several components. <code>TrafficIntersection</code> is used in the <code>Scene</code> for managing <code>TrafficLights</code>. All Traffic Lights present on one <code>Traffic Intersection</code> must be synchronized - this is why the logic of <code>TrafficLight</code> operation is included in the <code>TrafficIntersection</code>.</p>"},{"location":"Components/Traffic/TrafficComponents/#link-in-the-default-scene_1","title":"Link in the default Scene","text":"<p>Every <code>TrafficIntersection</code> has its own GameObject and is added as a child of the aggregate <code>TrafficIntersections</code> Object. <code>TrafficIntersections</code> are elements of an <code>Environment</code>, so they should be placed as children of an appropriate <code>Environment</code> Object.</p>"},{"location":"Components/Traffic/TrafficComponents/#components_1","title":"Components","text":"<p><code>TrafficIntersection</code> has the following components:</p> <ul> <li>Box Collider - marks the area of the Traffic Intersection, it should cover the whole intersection area</li> <li>Traffic Intersection (script) - controls all Traffic Lights on the given intersection according to the configuration</li> </ul>"},{"location":"Components/Traffic/TrafficComponents/#collider","title":"Collider","text":"<p>Every <code>TrafficIntersection</code> contains a Box Collider element. It needs to accurately cover the whole area of the <code>TrafficIntersection</code>. Box Collider - together with the Traffic Intersection (script) - is used for detecting vehicles entering the <code>TrafficIntersection</code>.</p>"},{"location":"Components/Traffic/TrafficComponents/#traffic-intersection-script","title":"Traffic Intersection (script)","text":"<p>Traffic Intersection (script) is used for controlling all <code>TrafficLights</code> on a given intersection. The <code>Collider Mask</code> field is a mask on which all Vehicle Colliders are present. It - together with Box Collider - is used for keeping track of how many Vehicles are currently present on the Traffic Intersection. The <code>Traffic Light Groups</code> and <code>Lighting Sequences</code> are described below.</p>"},{"location":"Components/Traffic/TrafficComponents/#traffic-light-groups","title":"Traffic Light Groups","text":"<p><code>Traffic Light Group</code> is a collection of all <code>Traffic Lights</code> that are in the same state at all times. This includes all redundant <code>Traffic Lights</code> shining in one direction as well as the ones in the opposite direction. In other words - as long as two <code>Traffic Lights</code> indicate exactly the same thing they should be added to the same <code>Traffic Light Group</code>. This grouping simplifies the creation of <code>Lighting Sequences</code>.</p>"},{"location":"Components/Traffic/TrafficComponents/#lighting-sequences","title":"Lighting Sequences","text":"<p><code>Lighting Sequences</code> is the field in which the whole intersection <code>Traffic Lights</code> logic is defined. It consists of many different Elements. Each Element is a collection of Orders that should take an effect for the period of time specified in the <code>Interval Sec</code> field. <code>Lighting Sequences</code> Elements are executed sequentially, in order of definition and looped  - after the last element sequence goes back to the first element.</p> <p>The <code>Group Lighting Orders</code> field defines which <code>Traffic Light Groups</code> should change their state and how. For every <code>Group Lighting Orders</code> Element the <code>Traffic Lights Group</code> is specified with the exact description of the goal state for all Traffic Lights in that group - which bulb should light up and with what color.</p> <p>One <code>Lighting Sequences</code> Element has many <code>Group Lighting Orders</code>, which means that for one period of time many different orders can be given. E.g. when <code>Traffic Lights</code> in one direction change color to green - <code>Traffic Lights</code> in the parallel direction change color to red.</p> <p>Traffic Light state persistance</p> <p>If in the given <code>Lighting Sequences</code> Element no order is given to some Traffic Light Group - this Group will keep its current state. When the next <code>Lighting Sequences</code> Element activates - the given <code>Traffic Light Group</code> will remain in an unchanged state.</p> Lighting Sequence Sample - details <p></p> Description Editor              Traffic Lights in Pedestrian Group 1change color to flashing green.             Other Groups keep theircurrent state.             This state lasts for 5 seconds.                       Traffic Lights in Pedestrian Group 1change color to solid red.             Other Groups keep theircurrent state.             This state lasts for 1 second.                       Traffic Lights in Vehicle Group 1change color to solid yellow.             Other Groups keep theircurrent state.             This state lasts for 5 seconds.                       Traffic Lights in Vehicle Group 1change color to solid red.             Other Groups keep theircurrent state.             This state lasts for 3 seconds.                       Traffic Lights in Vehicle Group 2change color to solid green.             Traffic Lights in Pedestrian Group 2change color to solid green.             Other Groups keep theircurrent state.             This state lasts for 15 seconds.                       Traffic Lights in Pedestrian Group 2change color to flashing green.             Other Groups keep theircurrent state.             This state lasts for 5 seconds.                       Traffic Lights in Pedestrian Group 2change color to solid red.             Other Groups keep theircurrent state.             This state lasts for 1 second.                       Traffic Lights in Vehicle Group 2change color to solid yellow.             Other Groups keep theircurrent state.             This state lasts for 5 seconds.                       Traffic Lights in Vehicle Group 2change color to solid red.             Other Groups keep theircurrent state.             This state lasts for 3 second.             Sequence loops back to thefirst element of the list."},{"location":"Components/Traffic/TrafficComponents/#trafficlanes","title":"TrafficLanes","text":"<p><code>TrafficLane</code> is a representation of a short road segment. It consists of several waypoints that are connected by straight lines. <code>TrafficLanes</code> are used as a base for a RandomTrafficSimulator. They allow <code>NPCVehicles</code> to drive on the specific lanes on the road and perform different maneuvers with respect to the traffic rules. TrafficLanes create a network of drivable roads when connected.</p>"},{"location":"Components/Traffic/TrafficComponents/#link-in-the-default-scene_2","title":"Link in the default Scene","text":"<p>Every <code>TrafficLane</code> has its own GameObject and is added as a child of the aggregate <code>TrafficLanes</code> Object. <code>TrafficLanes</code> are an element of an <code>Environment</code>, so they should be placed as children of an appropriate <code>Environment</code> Object.</p> <p><code>TrafficLanes</code> can be imported from the lanelet2 <code>*.osm</code> file.</p>"},{"location":"Components/Traffic/TrafficComponents/#components_2","title":"Components","text":"<p><code>TrafficLane</code> consists of an Object containing Traffic Lane (script).</p> <p><code>TrafficLane</code> has a transformation property - as every Object in Unity - however it is not used in any way. All details are configured in the Traffic Lane (script), the information in Object transformation is ignored.</p>"},{"location":"Components/Traffic/TrafficComponents/#traffic-lane-script","title":"Traffic Lane (script)","text":"<p>Traffic Lane (script) defines the <code>TrafficLane</code> structure. The <code>Waypoints</code> field is an ordered list of points that - when connected with straight lines - create a <code>TrafficLane</code>.</p> <p>Traffic Lane (script) coordinate system</p> <p><code>Waypoints</code> are defined in the <code>Environment</code> coordinate system, the transformation of GameObject is ignored.</p> <p><code>Turn Direction</code> field contains information on what is the direction of this <code>TrafficLane</code> - whether it is a right or left turn or straight road.</p> <p>Traffic lanes are connected using <code>Next Lanes</code> and <code>Prev Lanes</code> fields. This way individual <code>TrafficLanes</code> can create a connected road network. One Traffic Lane can have many <code>Next Lanes</code> and <code>Prev Lanes</code>. This represents the situation of multiple lanes connecting to one or one lane splitting into many - e.g. the possibility to turn and to drive straight.</p> <p>Right Of Way Lanes are described below.</p> <p>Every <code>TrafficLane</code> has to have a <code>Stop Line</code> field configured when the Stop Line is present on the end of the <code>TrafficLane</code>. Additionally the <code>Speed Limit</code> field contains the highest allowed speed on given <code>TrafficLane</code>.</p>"},{"location":"Components/Traffic/TrafficComponents/#right-of-way-lanes","title":"Right Of Way Lanes","text":"<p><code>Right Of Way Lanes</code> is a collection of <code>TrafficLanes</code>. Vehicle moving on the given <code>TrafficLane</code> has to give way to all vehicles moving on every <code>Right Of Way Lane</code>. It is determined based on basic traffic rules. Setting <code>Right Of Way Lanes</code> allows <code>RandomTrafficSimulator</code> to manage all <code>NPCVehicles</code> so they follow traffic rules and drive safely.</p> <p>In the Unity editor - when a <code>TrafficLane</code> is selected - aside from the selected <code>TrafficLane</code> highlighted in blue, all <code>Right Of Way Lanes</code> are highlighted in yellow.</p> <p>Right Of Way Lanes Sample - details</p> <p>The selected <code>TrafficLane</code> (blue) is a right turn on an intersection. This means, that before turning right the vehicle must give way to all vehicles driving from ahead - the ones driving straight as well as the ones turning left. This can be observed as <code>TrafficLanes</code> highlighted in yellow.</p> <p></p>"},{"location":"Components/Traffic/TrafficComponents/#stoplines","title":"StopLines","text":"<p><code>StopLine</code> is a representation of a place on the road where vehicles giving way to other vehicles should stop and wait. They allow <code>RandomTrafficSimulator</code> to manage <code>NPCVehicles</code> in safe and correct way - according to the traffic rules. All possible locations where a vehicle can stop in order to give way to other vehicles - that are enforced by an infrastructure, this does not include regular lane changing - need to be marked with <code>StopLines</code>.</p>"},{"location":"Components/Traffic/TrafficComponents/#link-in-the-default-scene_3","title":"Link in the default Scene","text":"<p>Every <code>StopLine</code> has its own GameObject and is added as a child of the aggregate <code>StopLines</code> Object. Stop Lines are an element of an <code>Environment</code>, so they should be placed as children of an appropriate <code>Environment</code> Object.</p> <p><code>StopLines</code> can be imported from the lanelet2 <code>*.osm</code> file.</p>"},{"location":"Components/Traffic/TrafficComponents/#components_3","title":"Components","text":"<p><code>StopLine</code> consists of an Object containing Stop Line (script).</p> <p>Stop Line has a transformation property - as every Object in Unity - however it is not used in any way. All details are configured in the Traffic Lane (script), the information in Object transformation is ignored.</p>"},{"location":"Components/Traffic/TrafficComponents/#stop-line-script","title":"Stop Line (script)","text":"<p>Stop Line (script) defines <code>StopLine</code> configuration. The <code>Points</code> field is an ordered list of points that - when connected - create a <code>StopLine</code>. The list of points should always have two elements that create a straight <code>StopLine</code>.</p> <p>Stop Line (script) coordinate system</p> <p><code>Points</code> are defined in the Environment coordinate system, the transformation of GameObject is ignored.</p> <p>The <code>Has Stop Sign</code> field contains information whether the configured <code>StopLine</code> has a corresponding <code>StopSign</code> on the scene.</p> <p>Every Stop Line needs to have a <code>Traffic Light</code> field configured with the corresponding <code>Traffic Light</code>. This information allows the <code>RandomTrafficSimulator</code> to manage the <code>NPCVehicles</code> in such a way that they respect the Traffic Lights and behave on the <code>Traffic Intersections</code> correctly.</p>"},{"location":"Components/Traffic/TrafficComponents/#gizmos","title":"Gizmos","text":"<p>Gizmos are a in-simulation visualization showing current and future moves of the <code>NPCVehicles</code>. They are useful for checking current behavior of NPCs and its causes. On the Scene they are visible as cuboid contours indicating which TrafficLanes will be taken by each vehicle in the near future.</p> <p>Gizmos computing</p> <p>Gizmos have a high computational load. Please disable them if the simulation is laggy.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/","title":"Add Vehicle","text":"<p>Ego Vehicle Component</p> <p>In this tutorial we will create a new <code>EgoVehicle</code>. To learn more about what an <code>EgoVehicle</code> is in AWSIM please visit Ego Vehicle description page.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#cerate-an-object","title":"Cerate an Object","text":"<p>Add a child Object to the Simulation called <code>EgoVehicle</code>.</p> <p></p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-rigidbody","title":"Add a Rigidbody","text":"<ol> <li> <p>While having a newly created <code>EgoVehicle</code> Object selected, in the Inspector view click on the 'Add Component' button, search for <code>Rigidbody</code> and select it.</p> <p></p> <p></p> </li> <li> <p>Configure Mass and Drag with the correct values for your Vehicle.</p> <p></p> </li> <li> <p>Configure Interpolation and Collision Detection.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-visual-elements","title":"Add visual elements","text":"<p>For a detailed explanation hwo to add visual elements of your Vehicle check out this dedicated tutorial.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-canter-of-mass","title":"Add a Canter of Mass","text":"<p>To add a center of mass to your vehicle you have to add a <code>CoM</code> child Object to the <code>EgoVehicle</code> Object (the same as in steps before).</p> <p>Then just set the position of the <code>CoM</code> Object in the Inspector view to represent real-world center of mass of the Vehicle.</p> <p></p> How do I know what is the Center of Mass of my Vehicle <p>The best way is to obtain a Center of Mass information from your Vehicle documentation.</p> <p>However, if this is not possible, you can try to estimate the Center of Mass of your vehicle. Best practice is to set the estimated Center of Mass as the following</p> <ul> <li>Evenly between the axles of the Vehicle</li> <li>Right in the middle of the Vehicles width</li> <li>Somewhere in the neighborhood of a quarter of the Vehicle height</li> </ul> <p>Note: This will vary very much depending on your Vehicle construction. For the best possible result please follow the Vehicle specifications.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-reflection-probe","title":"Add a Reflection Probe","text":"<ol> <li> <p>Add a new Object called <code>Reflection Probe</code> as a child to the <code>EgoVehicle</code> Object.</p> <p></p> </li> <li> <p>Click on the 'Add Component' button, in the windows that pops-up search for <code>Reflection Probe</code> and select it.</p> <p></p> <p>Note</p> <p>Please note that with <code>Reflection Probe</code> there should also be automatically added a <code>`HD Additional Reflection Data</code> Script.</p> <p></p> </li> <li> <p>Configure the <code>Reflection Probe</code> as you wish.</p> <p>Example Configuration</p> <p>Below you can see an example configuration of the <code>Reflection Probe</code>.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-colliders","title":"Add Colliders","text":"<p>For a detailed explanation how to add colliders to your Vehicle check out this dedicated tutorial.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-base-for-sensors-urdf","title":"Add a base for sensors (URDF)","text":"<p>You will most certainly want to add some sensors to your <code>EgoVehicle</code>. First you need to create a parent Object for all those sensors called <code>URDF</code>. To do this we will add a child Object <code>URDF</code> to the <code>EgoVehicle</code> Object.</p> <p></p> <p>This Object will be used as a base for all sensors we will add later.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-vehicle-script","title":"Add a Vehicle Script","text":"<p>To be able to control your <code>EgoVehicle</code> you need a <code>Vehicle</code> Script.</p> <ol> <li> <p>Add the <code>Vehicle</code> Script to the <code>EgoVehicle</code> Object.</p> <p></p> <p></p> </li> <li> <p>Configure the <code>Vehicle</code> Script Axle Settings and Center Of Mass Transform.</p> <p></p> </li> </ol> <p>Testing</p> <p>It is not possible to test this Script alone, but you can test the following</p> <ul> <li>Vehicle Keyboard Input Script</li> <li>Vehicle Visual Effects Script</li> <li>Vehicle Ros Input Script</li> </ul> <p>If components listed above work correctly this means the <code>Vehicle</code> Script works correctly too.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-vehicle-keyboard-input-script","title":"Add a Vehicle Keyboard Input Script","text":"<p>You can control your <code>EgoVehicle</code> in the simulation manually with just one Script called <code>Vehicle Keyboard Input</code>.</p> <p>If you want to add it just click the 'Add Component' button on the <code>EgoVehicle</code> Object and search for <code>Vehicle Keyboard Input</code> Script and select it.</p> <p></p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-vehicle-visual-effect-script","title":"Add a Vehicle Visual Effect Script","text":"<p>For a visual indication of a Vehicle status you will need a <code>Vehicle Visual Effect</code> Script. To add and configure it follow the steps below.</p> <ol> <li> <p>Add a <code>Vehicle Visual Effect</code> Script by clicking 'Add Component' button, searching for it and selecting it.</p> <p></p> </li> <li> <p>Configure the lights.</p> <p>Note</p> <p>In this step we will configure only <code>Brake Lights</code>, but should repeat this for every Light. The process is almost the same for all Lights - just change the mesh renderer and lighting settings according to your preference.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#how-to-test","title":"How to test","text":"<p>After configuring <code>Vehicle Visual Effect</code> Script it is advised to test whether everything works as expected.</p> <ol> <li> <p>Make sure you have a <code>Vehicle Keyboard Input</code> Script added and that it is enabled.</p> </li> <li> <p>If your scene does not have any models yet please turn the gravity off in <code>Rigidbody</code> configuration so that the Vehicle does not fall down into infinity.</p> <p></p> </li> <li> <p>Start the simulation.</p> <p></p> </li> <li> <p>Test the Turn Signals.</p> <p>You can control the Turn Signals with a <code>Vehicle Keyboard Input</code> Script. Activate the Turn Signals with one of the following keys</p> <ul> <li><code>1</code> - Left Turn Signal</li> <li><code>2</code> - Right Turn Signal</li> <li><code>3</code> - Hazard Lights</li> <li><code>4</code> - Turn Off all Signals</li> </ul> <p></p> </li> <li> <p>Test the Lights.</p> <p>You can control the lights by \"driving\" the Vehicle using <code>Vehicle Keyboard Input</code> Script. Although if you have an empty Environment like in this tutorial the Vehicle won't actually drive.</p> <p>To test Brake Lights change the gear to Drive by pressing <code>D</code> on the keyboard and activate braking by holding <code>arrow down</code>.</p> <p>To test the Reverse Light change the gear to Reverse by pressing <code>R</code> on the keyboard. The Reverse Light should turn on right away.</p> <p></p> </li> </ol> <p>Camera tip</p> <p>If you have not configured a camera or configured it in such a way that you can't see the Vehicle well you can still test most of the lights by changing views.</p> <ul> <li>Turn the Light (or Signal) on by pressing the appropriate key</li> <li>Move to the Scene View by pressing <code>ctrl + 1</code> - now you can move the camera freely</li> <li>To change the Lights you need to go back to Game View by pressing <code>ctrl + 2</code></li> </ul> <p>Pleas note that this method won't work for testing Brake Lights, as for them to work you need to keep the <code>arrow down</code> button pressed all the time.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-vehicle-ros-input-script","title":"Add a Vehicle Ros Input Script","text":"<p>For controlling your Vehicle with autonomous driving software (e.g. Autoware) you need a <code>Vehicle Ros Input</code> Script.</p> <p>Disable <code>Vehicle Keyboard Input</code> Script</p> <p>If you have added a <code>Vehicle Keyboard Input</code> Script in your Vehicle please disable it when using the <code>Vehicle Ros Input</code> Script.</p> <p>Not doing so will lead to the vehicle receiving two different inputs which will cause many problems.</p> <p></p> <p>Add it to the <code>EgoVehicle</code> Object by clicking on the 'Add Component' button, searching for it and selecting it.</p> <p></p> <p>The Script is configured to work with Autoware by default, but you can change the topics and Quality of Service settings as you wish.</p> <p>Note</p> <p>The <code>Vehicle</code> should be configured correctly, but if you have many Vehicles or something goes wrong, please select the right Vehicle in the <code>Vehicle</code> field by clicking on the small arrow icon and choosing the right item from the list.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#how-to-test_1","title":"How to test","text":"<p>The best way to test the Vehicle Ros Input Script is to run Autoware.</p> <ol> <li>Run the Scene same as on this page.</li> <li>Launch only the Autoware like on this page</li> <li>Plan a path in Autoware like here, if the Vehicle moves in AWSIM correctly then the Script is configured well.</li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-sensors","title":"Add Sensors","text":"<p>For a detailed explanation how to add sensors to your Vehicle check out this dedicated tutorial.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddAVehicle/#add-a-vehicle-to-scene","title":"Add a Vehicle to Scene","text":"<p>First you will have to save the Vehicle you created as a prefab, to easily add it later to different Scenes.</p> <ol> <li>Open the Vehicles directory in the Project view (<code>Assets/AWSIM/Prefabs/Vehicles</code>)</li> <li>Drag the Vehicle Object from the Hierarchy view to the Vehicles directory</li> </ol> <p></p> <p>After that, you can add the Vehicle you created to different Scenes by dragging it from Vehicles directory to the Hierarchy of different Scenes.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddColliders/","title":"Add Colliders","text":"<p>Next you need to add Colliders to your Vehicle. To do this follow the steps below.</p> <ol> <li> <p>Add a child Object called <code>Colliders</code> to the <code>EgoVehicle</code> Object.</p> <p></p> </li> <li> <p>Shift parent Object <code>Colliders</code> accordingly as in earlier steps where we shifted <code>Models</code>.</p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddColliders/#add-a-vehicle-collider","title":"Add a Vehicle Collider","text":"<ol> <li> <p>Add a child Object <code>Collider</code> to the <code>Colliders</code> Object.</p> <p></p> </li> <li> <p>Add a <code>Mesh Collider</code> component to the <code>Collider</code> Object by clicking on the 'Add Component' button in the Inspector view and searching for it.</p> <p></p> </li> <li> <p>Click on the arrow in mesh selection field and from the pop-up window select your collider mesh.     Next click on the check-box called <code>Convex</code>, by now your collider mesh should be visible in the editor.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddColliders/#add-wheel-colliders","title":"Add Wheel Colliders","text":"<ol> <li> <p>Add a child Object <code>Wheels</code> to the <code>Colliders</code> Object.</p> <p></p> </li> </ol> <p>Note</p> <p>In this tutorial we will add only one wheel collider, but you should repeat the step for all 4 wheels. That is, follow the instructions that follow this message for every wheel your Vehicle has.</p> <ul> <li>Front Left Wheel</li> <li>Front Right Wheel</li> <li>Rear Left Wheel</li> <li>Rear Right Wheel</li> </ul> <ol> <li> <p>Add a child Object <code>FrontLeftWheel</code> to the <code>Wheels</code> Object.</p> <p></p> </li> <li> <p>Add a <code>Wheel Collider</code> component to the <code>FrontLeftWheel</code> Object by clicking 'Add Component' and searching for it.</p> <p></p> </li> <li> <p>Add a <code>Wheel</code> Script to the <code>FrontLeftWheel</code> Object by clicking 'Add Component' and searching for it.</p> <p></p> </li> <li> <p>Drag <code>FrontLeftWheel</code> Object from the <code>WheelVisuals</code> to the <code>Wheel Visual Transform</code> field.</p> <p></p> </li> <li> <p>Add a <code>Wheel Collider Config</code> Script to the <code>FrontLeftWheel</code> Object by clicking 'Add Component' and searching for it.</p> <p></p> </li> <li> <p>Configure the <code>Wheel Collider Config</code> Script so that the Vehicle behaves as you wish.</p> <p></p> </li> <li> <p>Set the Transform of <code>FrontLeftWheel</code> Object to match the visuals of your Vehicle.</p> <p></p> </li> </ol> <p>Successful configuration</p> <p>If you have done everything right your <code>Colliders</code> Object should look similar to the one following.</p> <p></p>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/","title":"Add Sensors","text":""},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#awsim-sensors","title":"AWSIM Sensors","text":"<p>There is a number of different sensors available in AWSIM. Below we present a list of sensors with links to their individual pages.</p> <ul> <li>Vehicle Status Sensor</li> <li>GNSS</li> <li>LiDAR</li> <li>IMU</li> <li>Camera</li> </ul>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-links-for-sensors","title":"Add links for sensors","text":"<p>Best practice is to replicate a ROS sensors transformations tree in Unity using Objects.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#coordinate-system-conversion","title":"Coordinate system conversion","text":"<p>Please note that Unity uses less common left-handed coordinate system. Please keep this in mind while defining transformations. More details about right-handed and left-handed systems can be found here.</p> <p>To simplify the conversion process always remember that any point in ROS coordinate system <code>(x, y, z)</code> has an equivalent in the Unity coordinate system being <code>(-y, z, x)</code>.</p> <p>The same can be done with the rotation. ROS orientation described with roll, pitch and yaw <code>(r, p, y)</code> can be translated to Unity Rotation as follows <code>(p, -y, -r)</code>.</p> <p>Unit conversion</p> <p>Please remember to convert the rotation units. ROS uses radians and Unity uses degrees. The conversion from radians (<code>rad</code>) to degrees (<code>deg</code>) is as follows.</p> <pre><code>deg = rad * 180 / PI\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-transformations-tree","title":"Add transformations tree","text":"<p>URDF</p> <p>Before following this tutorial please make sure you have an <code>URDF</code> Object like it is shown shown in this section.</p> <p>First we will have to add a <code>base_link</code> which is the root of all transformations.</p> <p>Add a <code>base_link</code> Object as a child to the <code>URDF</code> Object.</p> <p></p> <p><code>base_link</code> transformation</p> <p>Please remember to set an appropriate transformation of the <code>base_link</code> Object so that it is identical as the <code>base_link</code> used in ROS in reference to the Vehicle.</p> <p>This is very important, as a mistake here will result in all subsequent sensors being misplaced.</p> <p>Inside the <code>base_link</code> we will represent all transformations contained in the ROS transformations tree.</p> <p>You will have to check your Vehicle specific configuration. You can do this in many ways, for example:</p> <ul> <li> <p>Check the ROS specific <code>.yaml</code> parameter files containing information about each transformation.</p> <p>Example</p> <p>Here we see an example <code>.yaml</code> file containing transformation from the <code>base_link</code> to the <code>sensor_kit_base_link</code>:</p> <pre><code>base_link:\n    sensor_kit_base_link:\n        x: 0.9\n        y: 0.0\n        z: 2.0\n        roll: -0.001\n        pitch: 0.015\n        yaw: -0.0364\n</code></pre> </li> </ul> <ul> <li> <p>Check the values with ROS command line tools (for more information on these please visit official ROS 2 documentation).</p> <p>You can run a command like the following to check a transformation between two frames.</p> <pre><code>ros2 run tf2_ros tf2_echo [source_frame] [target_frame]\n</code></pre> <p>Example</p> <p>Here we see an example command with the output containing transformation from the <code>base_link</code> to the <code>sensor_kit_base_link</code> (note that the line after <code>$</code> sign is the executed command):</p> <pre><code>$ ros2 run tf2_ros tf2_echo base_link sensor_kit_base_link\n[INFO] [1686654712.339110702] [tf2_echo]: Waiting for transform base_link -&gt;  sensor_kit_base_link: Invalid frame ID \"base_link\" passed to canTransform argument target_frame - frame does not exist\nAt time 0.0\n- Translation: [0.900, 0.000, 2.000]\n- Rotation: in Quaternion [-0.000, 0.008, -0.018, 1.000]\n- Rotation: in RPY (radian) [-0.001, 0.015, -0.036]\n- Rotation: in RPY (degree) [-0.057, 0.859, -2.086]\n- Matrix:\n0.999  0.036  0.015  0.900\n-0.036  0.999  0.000  0.000\n-0.015 -0.001  1.000  2.000\n0.000  0.000  0.000  1.000\n</code></pre> </li> </ul>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-one-sensor-link","title":"Add one sensor link","text":"<p>Note</p> <p>In this step we will only add one sensor link. You will have to repeat this step for every sensor you want to add to your Vehicle.</p> <p>Let's say we want to add a LiDAR that is facing right.</p> <p>We have the following configuration files.</p> <pre><code>base_link:\n    sensor_kit_base_link:\n        x: 0.9\n        y: 0.0\n        z: 2.0\n        roll: -0.001\n        pitch: 0.015\n        yaw: -0.0364\n</code></pre> <pre><code>sensor_kit_base_link:\n    velodyne_right_base_link:\n        x: 0.0\n        y: -0.56362\n        z: -0.30555\n        roll: -0.01\n        pitch: 0.71\n        yaw: -1.580\n</code></pre> <p>We can clearly see the structure of transformation tree. The transformations are as follows.</p> <pre><code>base_link -&gt; sensor_kit_base_link -&gt; velodyne_right_base_link\n</code></pre> <p>We need to start adding these transformation from the root of the tree. We will start with the <code>sensor_kit_base_link</code>, as the <code>base_link</code> already exists in our tree.</p> <ol> <li> <p>The first step is to add an Object named the same as the transformation frame (<code>sensor_kit_base_link</code>).</p> <p></p> </li> <li> <p>Next we have to convert the transformation from ROS standard to the Unity standard.     This is done with the formulas show in this section.</p> <p>The result of conversion of the coordinate systems and units is shown below.</p> <pre><code>Position:\n(0.9, 0.0, 2.0)             -&gt;  (0.0, 2.0, 0.9)\nRotation:\n(-0.001, 0.015, -0.0364)    -&gt;  (0.8594, 2.0856, 0.0573)\n</code></pre> <p>The resulting <code>sensor_kit_base_link</code> Object transformation is shown below.</p> <p></p> </li> </ol> <p>Now the same has to be done with the <code>velodyne_right_base_link</code>.</p> <ol> <li> <p>Add transformation Object (<code>velodyne_right_base_link</code>).</p> <p>Info</p> <p>Remember to correctly set the child Object, in this case we use <code>sensor_kit_base_link</code> as a child, because this is what the <code>.yaml</code> file says.</p> <p></p> </li> <li> <p>Convert the transformation into Unity coordinate system.</p> <p>The correct transformation is shown below.</p> <pre><code>Position:\n(0, -0.56362, -0.30555)     -&gt;  (0.56362, -0.30555, 0)\nRotation:\n(-0.01, 0.71, -1.580)       -&gt;  (40.68, 90.5273, 0.573)\n</code></pre> <p>The final <code>velodyne_right_base_link</code> Object transformation is shown below.</p> <p></p> </li> </ol> <p>Success</p> <p>If you have done everything right, after adding all of the sensor links your <code>URDF</code> Object tree should look something like the one following.</p> <p></p>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-sensors","title":"Add sensors","text":"<p>After adding links for all sensors you need to add the actual sensors into your Vehicle.</p> <p>Sensor position</p> <p>Please keep in mind, that we have created the sensor links in order to have an accurate transformations for all of the sensors. This implies that the Sensor Object itself can not have any transformation.</p> <p>If one of your Sensors, after adding it to the scene, is mispositioned, check whether the transformation is set to identity (position and rotation are zeros).</p> <p>When adding sensors almost all of them will have some common fields.</p> <ul> <li> <p>Frame Id</p> <p>Frame Id is the name of frame of reference against which the received data will be interpreted by the autonomous driving software stack.</p> <p>Remember that the Frame Id must exist internally in the ROS transformations tree.</p> </li> </ul> <ul> <li> <p>Topics</p> <p>Topics are names of broadcasting channels. You can set the names of topics as you like and the data from sensors will be broadcasted on these topics.</p> <p>Remember to configure your receiving end to listen on the same topics as broadcasting ones.</p> </li> </ul> <ul> <li> <p>Quality Of Service settings (QOS settings)</p> <p>Quality of service settings allow you to configure the behavior of the source node while broadcasting the sensor data. You can adjust these settings to suit your needs.</p> </li> </ul>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-a-vehicle-status-sensor","title":"Add a Vehicle Status Sensor","text":"<p>To add a Vehicle Status Sensor to your Vehicle simply locate the following directory in the Project view and drag a prefab of this Sensor into the <code>URDF</code> Object.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors\n</code></pre> <p></p> <p>Next in the Inspector View select your Vehicle.</p> <p></p> ROS message example <p>In this example you can see what a valid message from the Vehicle Status Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /vehicle/status/velocity_status\nheader:\n  stamp:\n    sec: 17\n    nanosec: 709999604\n  frame_id: base_link\nlongitudinal_velocity: 0.004912620410323143\nlateral_velocity: -0.005416259169578552\nheading_rate: 0.006338323466479778\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-a-lidar","title":"Add a LiDAR","text":"<p>Scene Manager</p> <p>Before continuing with this tutorial please check out a dedicated one focused on Scene Manager.</p> <p>To add a LiDAR to your Vehicle you will have to drag a model of the LiDAR to the link tree you have created in the earlier step.</p> <p>You can use the predefined RGL LiDAR models or any other LiDAR models. In this tutorial we will be using RGL <code>VelodyneVLP16</code> LiDAR model.</p> <p>Simply locate the following directory in the Project view and drag the prefab into the designated sensor link.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors/RobotecGPULidars\n</code></pre> <p></p> <p>LiDAR noise configuration</p> <p>LiDAR Sensor in simulation is returning a perfect result data. This is not an accurate representation of the real-world.</p> <p>LiDAR Sensor addresses this issue by applying a simulated noise to the output data. You can configure the noise parameters in the Inspector View under <code>Configuration -&gt; Noise Params</code> fields.</p> <p>You can optionally remove the noise simulation by unchecking the <code>Apply Distance/Angular Gaussian Noise</code>.</p> <p>You can also change the ranges of the LiDAR detection.</p> <p></p> <p>There is also a possibility to configure the visualization of the Point Cloud generated by the LiDAR. E.g. change the hit-point shape and size.</p> <p></p> ROS message example <p>In this example you can see what a valid message from the LiDAR Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /lidar/pointcloud\nheader:\n  stamp:\n    sec: 20\n    nanosec: 589999539\n  frame_id: world\nheight: 1\nwidth: 14603\nfields:\n- name: x\n  offset: 0\n  datatype: 7\n  count: 1\n- name: y\n  offset: 4\n  datatype: 7\n  count: 1\n- name: z\n  offset: 8\n  datatype: 7\n  count: 1\n- name: intensity\n  offset: 16\n  datatype: 7\n  count: 1\n- name: ring\n  offset: 20\n  datatype: 4\n  count: 1\nis_bigendian: false\npoint_step: 24\nrow_step: 350472\ndata:\n- 156\n- 218\n- 183\n- 62\n- 0\n- 189\n- 167\n- 187\n- 32\n- 58\n- 173\n- 189\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 200\n- 66\n- 1\n- 0\n- 0\n- 0\n- 198\n- 129\n- 28\n- 63\n- 0\n- 6\n- 230\n- 58\n- 128\n- 184\n- 93\n- 61\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 200\n- 66\n- 9\n- 0\n- 0\n- 0\n- 92\n- 2\n- 194\n- 62\n- 0\n- 141\n- 42\n- 187\n- 128\n- 89\n- 139\n- 189\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 200\n- 66\n- 2\n- 0\n- 0\n- 0\n- 187\n- 168\n- 42\n- 63\n- 0\n- 159\n- 175\n- 59\n- 160\n- 243\n- 185\n- 61\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 200\n- 66\n- 10\n- 0\n- 0\n- 0\n- 119\n- 186\n- 204\n- 62\n- 0\n- 254\n- 23\n- 59\n- 128\n- 143\n- 41\n- 189\n- 0\n- 0\n- 0\n- 0\n- 0\n- 0\n- 200\n- 66\n- 3\n- 0\n- 0\n- 0\n- 65\n- 241\n- 59\n- 63\n- 128\n- 0\n- 252\n- 187\n- '...'\nis_dense: true\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-an-imu","title":"Add an IMU","text":"<p>To add an IMU to your Vehicle you will have to drag a model of the IMU to the link tree you have created in the earlier step.</p> <p>You can use the provided or your own IMU Sensor. In this tutorial we will be using IMU Sensor provided with AWSIM.</p> <p>Simply locate the following directory in the Project view and drag the prefab into the designated sensor link.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors\n</code></pre> <p></p> ROS message example <p>In this example you can see what a valid message from the IMU Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /sensing/imu/tamagawa/imu_raw\nheader:\n  stamp:\n    sec: 20\n    nanosec: 589999539\n  frame_id: tamagawa/imu_link\norientation:\n  x: 0.0\n  y: 0.0\n  z: 0.0\n  w: 1.0\norientation_covariance:\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\nangular_velocity:\n  x: 0.014335081912577152\n  y: 0.008947336114943027\n  z: -0.008393825963139534\nangular_velocity_covariance:\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\nlinear_acceleration:\n  x: 0.006333829835057259\n  y: -0.005533283110707998\n  z: -0.0018753920448943973\nlinear_acceleration_covariance:\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n- 0.0\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-a-gnss","title":"Add a GNSS","text":"<p>To add a GNSS Sensor to your Vehicle you will have to drag a model of the GNSS to the link tree you have created in the earlier step.</p> <p>You can use the provided or your own GNSS Sensor. In this tutorial we will be using GNSS Sensor provided with AWSIM.</p> <p>Simply locate the following directory in the Project view and drag the prefab into the designated sensor link.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors\n</code></pre> <p></p> ROS message example <p>In this example you can see what a valid message from the GNSS Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /sensing/gnss/pose\nheader:\n  stamp:\n    sec: 8\n    nanosec: 989999799\n  frame_id: gnss_link\npose:\n  position:\n    x: 81656.765625\n    y: 50137.5859375\n    z: 44.60169219970703\n  orientation:\n    x: 0.0\n    y: 0.0\n    z: 0.0\n    w: 0.0\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-a-camera","title":"Add a Camera","text":"<p>To add a Camera Sensor to your Vehicle you will have to drag a model of the Camera to the link tree you have created in the earlier step.</p> <p>Simply locate the following directory in the Project view and drag the prefab into the designated sensor link.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors\n</code></pre> <p></p> <p>You can configure some aspects of the Camera to your liking.</p> <p>E.g. you can set the field of view (fov) of the camera by changing the <code>Field of View</code> field or manipulating the physical camera parameters like <code>Focal Length</code>.</p> <p></p> <p>The important thing is to configure the <code>Camera Sensor</code> Script correctly.</p> <p>Always check whether the correct <code>Camera Object</code> is selected and make sure that <code>Distortion Shader</code> and <code>Ros Image Shader</code> are selected.</p> <p>Example Camera Sensor Script configuration</p> <p></p> <p>You can add the live Camera preview onto the Scene. To do this select the <code>Show</code> checkbox. Additionally you can change how the preview is displayed. Change the <code>Scale</code> value to control the size of the preview (how many times smaller the preview will be compared to the actual screen size).</p> <p>Move the preview on the screen by changing the <code>X Axis</code> and <code>Y Axis</code> values on the <code>Image On Gui</code> section.</p> <p>Camera preview example</p> <p></p> <p>Testing camera with traffic light recognition</p> <p>You can test the Camera Sensor traffic light recognition by positioning the vehicle on the Unity Scene in such a way that on the Camera preview you can see the traffic lights.</p> <p>Remember to lock the Inspector view on Camera Object before dragging the whole Vehicle - this way you can see the preview while moving the vehicle.</p> <p></p> <p>Run the Scene the same as on this page.</p> <p>Launch only the Autoware like on this page.</p> <p>By default you should see the preview of traffic light recognition visualization in the bottom left corner of Autoware.</p> <p>Traffic lights recognition example in Autoware</p> <p></p> ROS message example <p>In this example you can see what a valid message from the Camera Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /sensing/camera/traffic_light/image_raw\nheader:\n  stamp:\n    sec: 14\n    nanosec: 619999673\n  frame_id: traffic_light_left_camera/camera_optical_link\nheight: 1080\nwidth: 1920\nencoding: bgr8\nis_bigendian: 0\nstep: 5760\ndata:\n- 145\n- 126\n- 106\n- 145\n- 126\n- 106\n- 145\n- 126\n- 106\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 105\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 104\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 126\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 103\n- 145\n- 124\n- 101\n- 145\n- 124\n- 101\n- 145\n- 124\n- 101\n- 145\n- 124\n- 101\n- 145\n- 123\n- 101\n- 145\n- 123\n- 101\n- 145\n- 123\n- 101\n- 145\n- 123\n- '...'\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#add-a-pose-sensor","title":"Add a Pose Sensor","text":"<p>To add a Pose Sensor to your Vehicle simply locate the following directory in the Project view and drag a prefab of this Sensor into the <code>base_link</code> Object.</p> <pre><code>Assets/AWSIM/Prefabs/Sensors\n</code></pre> <p></p> ROS message example <p>In this example you can see what a valid message from the Pose Sensor can look like.</p> <pre><code>$ ros2 topic echo --once /awsim/ground_truth/vehicle/pose\nheader:\n  stamp:\n    sec: 5\n    nanosec: 389999879\n  frame_id: base_link\npose:\n  position:\n    x: 81655.7578125\n    y: 50137.3515625\n    z: 42.8094367980957\n  orientation:\n    x: -0.03631274029612541\n    y: 0.0392342209815979\n    z: 0.02319677732884884\n    w: 0.9983005523681641\n---\n</code></pre>"},{"location":"Components/Vehicle/AddNewVehicle/AddSensors/#test-a-sensor","title":"Test a Sensor","text":"<p>You can test whether the Sensor works correctly in several ways.</p> <ol> <li> <p>Check whether the configuration is correct.</p> <p>In terminal source ROS with the following line (only if you haven't done so already).</p> <pre><code>source /opt/ros/humble/setup.bash\n</code></pre> <p>Check the details about the topic that your Sensor is broadcasting to with the following command.</p> <pre><code>ros2 topic info -v &lt;topic_name&gt;\n</code></pre> <p>Example</p> <p>In this example we can see that the message is broadcasted by AWSIM and nobody is listening. We can also examine the Quality of Service settings.</p> <pre><code>$ ros2 topic info -v /awsim/ground_truth/vehicle/pose\nType: geometry_msgs/msg/PoseStamped\n\nPublisher count: 1\n\nNode name: AWSIM\nNode namespace: /\nTopic type: geometry_msgs/msg/PoseStamped\nEndpoint type: PUBLISHER\nGID: 01.10.13.11.98.7a.b1.2a.ee.a3.5a.11.00.00.07.03.00.00.00.00.00.00.00.00\nQoS profile:\n  Reliability: RELIABLE\n  History (Depth): KEEP_LAST (1)\n  Durability: VOLATILE\n  Lifespan: Infinite\n  Deadline: Infinite\n  Liveliness: AUTOMATIC\n  Liveliness lease duration: Infinite\n\nSubscription count: 0\n</code></pre> </li> <li> <p>Check whether correct information is broadcasted.</p> <p>In terminal source ROS with the following line (only if you haven't done so already).</p> <pre><code>source /opt/ros/humble/setup.bash\n</code></pre> <p>View one transmitted message.</p> <pre><code>ros2 topic echo --once &lt;topic_name&gt;\n</code></pre> <p>Example</p> <p>In this example we can see the Vehicles location at the moment of executing the command.</p> <p>NOTE: The position and orientation are relative to the frame in the <code>header/frame_id</code> field (<code>base_link</code> in this example).</p> <pre><code>$ ros2 topic echo --once /awsim/ground_truth/vehicle/pose\nheader:\n  stamp:\n    sec: 46\n    nanosec: 959998950\n  frame_id: base_link\npose:\n  position:\n    x: 81655.7265625\n    y: 50137.4296875\n    z: 42.53997802734375\n  orientation:\n    x: 0.0\n    y: -9.313260163068549e-10\n    z: -6.36646204504876e-12\n    w: 1.0\n---\n</code></pre> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddVisualElements/","title":"Add Visual Elements","text":"<p>Your <code>EgoVehicle</code> needs many individual visual parts. Below we will add all needed visual elements.</p> <p>First in <code>EgoVehicle</code> Object add a child Object called <code>Models</code>.</p> <p></p> <p>Inside <code>Models</code> Object we will add all visual models of our <code>EgoVehicle</code>.</p>"},{"location":"Components/Vehicle/AddNewVehicle/AddVisualElements/#add-a-body","title":"Add a Body","text":"<p>First you will need to add a Body of your Vehicle. It will contain many parts, so first lets create a <code>Body</code> parent Object.</p> <p></p> <p>Next we will need to add Car Body</p> <ol> <li> <p>Add a child Object <code>BodyCar</code> to the <code>Body</code> Object.</p> <p></p> </li> <li> <p>To the <code>BodyCar</code> Object add a Mesh Filter.</p> <p>Click on the 'Add Component' button, search for <code>Mesh Filter</code> and select it. Next search for mesh of your vehicle and select it in the <code>Mesh</code> field.</p> <p></p> <p></p> </li> <li> <p>To the <code>BodyCar</code> Object add a Mesh Renderer.</p> <p>Click on the 'Add Component' button, search for <code>Mesh Filter</code> and select it</p> <p></p> <p></p> </li> <li> <p>Specify Materials.</p> <p>You need to specify what materials will be used for rendering your <code>EgoVehicle</code> model. Do this by adding elements to the <code>Materials</code> list and selecting the materials you wish to use as shown below.</p> <p></p> <p>Add as many materials as your model has sub-meshes.</p> <p>Tip</p> <p>When you add too many materials, meaning there will be no sub-meshes to apply these materials to, you will see this warning. In such a case please remove materials until this warning disappears.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddVisualElements/#add-interactive-body-parts","title":"Add interactive Body parts","text":"<p>In this step we will add the following parts</p> <ul> <li>Break light</li> <li>Reverse Light</li> <li>Right Turn Signal</li> <li>Left Turn Signal</li> <li>(optional) other visual elements required specifically by your Vehicle.</li> </ul> <p>Info</p> <p>It may seem like all of the elements above can be parts of the <code>Body</code> mesh, but it is important for these parts to be separate, because we need to be able to make them interactive (e.g. flashing turn signals).</p> <p>Other good reason for having different meshes for Vehicle parts is that you have a Vehicle model, but for the simulation you need to add e.g. a roof rack with sensors - which can be achieved by adding more meshes.</p> <p>Note</p> <p>We will illustrate this step only for Break Light, but you should repeat this step of the tutorial for each element of the list above.</p> <ol> <li> <p>Add a child Object to the <code>Body</code> Object.</p> <p></p> </li> <li> <p>Add a Mesh Filter and select the mesh (the same as in section before).</p> <p></p> <p></p> </li> <li> <p>Add a Mesh Renderer and select the materials (the same as in section before).</p> <p></p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/AddNewVehicle/AddVisualElements/#add-wheels","title":"Add Wheels","text":"<p>In this step we will add individual visuals for every wheel. This process is very similar to the one before.</p> <ol> <li> <p>Add a child Object to the <code>Models</code> Object called <code>WheelVisuals</code>.</p> <p></p> </li> </ol> <p>Note</p> <p>In this tutorial we will add only one wheel, but you should repeat the step for all 4 wheels. That is, follow the instructions that follow this message for every wheel your Vehicle has.</p> <ul> <li>Front Left Wheel</li> <li>Front Right Wheel</li> <li>Rear Left Wheel</li> <li>Rear Right Wheel</li> </ul> <ol> <li> <p>Add a child Object to the <code>WheelVisuals</code> Object called <code>FrontLeftWheel</code>.</p> <p></p> </li> <li> <p>Add a child Object to the <code>FrontLeftWheel</code> Object called <code>WheelFrontL</code>.     This Object will contain the actual wheel part.</p> <p></p> </li> <li> <p>Add a Mesh Filter and select the wheel mesh.</p> <p></p> <p></p> </li> <li> <p>Add a Mesh Renderer and select the wheel materials.</p> <p></p> <p></p> <p></p> </li> <li> <p>Repeat the steps before to add Breaks.</p> <p>The same way you have added the <code>WheelFrontL</code> Object now add the <code>WheelFrontLBreaks</code>. Naturally you will have to adjust the mesh and materials used as they will be different for breaks than for the wheel.</p> <p>Your final break configuration should look similar to the one following.</p> <p></p> </li> <li> <p>Set the <code>FrontLeftWheel</code> parent Object transformation to position the wheel in correct place.</p> <p></p> </li> </ol> <p>Successful configuration</p> <p>If you have done everything right your <code>WheelVisuals</code> Object should look similar to the one following.</p> <p></p>"},{"location":"Components/Vehicle/AddNewVehicle/AddVisualElements/#move-the-models","title":"Move the models","text":"<p>The last step to correctly configure Vehicle models is to shift them so that the <code>EgoVehicle</code> origin is in the center of fixed axis.</p> <p>This means you need to shift the whole <code>Models</code> Object accordingly (change the position fields in transformation).</p> <p>Tip</p> <p>Add a <code>dummy</code> Object as a child to the <code>EgoVehicle</code> Object (the same as in steps before) so it is located in the origin of the <code>EgoVehicle</code>.</p> <p>Now move <code>Models</code> around relative to the <code>dummy</code> - change position in the Inspector view. The <code>dummy</code> will help you see when the fixed axis (in case of the Lexus from example it is the rear axis) is aligned with origin of <code>EgoVehicle</code>.</p> <p>In the end delete the <code>dummy</code> Object as it is no longer needed.</p> <p></p>"},{"location":"Components/Vehicle/CustomizeSlip/","title":"Customize Slip","text":""},{"location":"Components/Vehicle/CustomizeSlip/#customize-slip","title":"Customize slip","text":"<p>By attaching a <code>GroundSlipMutiplier.cs</code> script to a collider (trigger), you can change the slip of the vehicle within the range of that collider.</p>"},{"location":"Components/Vehicle/CustomizeSlip/#sample-scene","title":"Sample scene","text":"<p><code>Assets\\AWSIM\\Scenes\\Samples\\VehicleSlipSample.unity</code></p>"},{"location":"Components/Vehicle/CustomizeSlip/#how-to-setup","title":"How to setup","text":"<ol> <li>Create collider</li> <li>Check IsTrigger</li> <li>Change properties of FowardSlip and SidewaySlip.</li> </ol>"},{"location":"Components/Vehicle/DynamicSensorPlacement/","title":"Dynamic Sensor Placement","text":""},{"location":"Components/Vehicle/DynamicSensorPlacement/#dynamic-sensor-placement","title":"Dynamic Sensor Placement","text":"<p>This tool allows you to dynamically place sensors on a car using positions and rotations specified in a URDF file.</p>"},{"location":"Components/Vehicle/DynamicSensorPlacement/#follow-the-steps-to-generate-the-urdf-file","title":"Follow the steps to generate the urdf file:","text":"<ol> <li> <p>Build Autoware by following the instruction in https://autowarefoundation.github.io/autoware-documentation/main/installation/autoware/source-installation/.</p> </li> <li> <p>Modify L3-4 in     <code>$HOME/autoware/src/universe/autoware.universe/launch/tier4_vehicle_launch/urdf/vehicle.xacro</code>     as the following,</p> <pre><code>    &lt;xacro:arg name=\"vehicle_model\" default=\"sample_vehicle\"/&gt;\n    &lt;xacro:arg name=\"sensor_model\" default=\"awsim_sensor_kit\"/&gt;\n</code></pre> </li> <li> <p>Run the following commands:</p> <pre><code>source $HOME/autoware/install/setup.bash\n\nros2 run xacro xacro -o awsim_vehicle.urdf $HOME/autoware/src/universe/autoware.universe/launch/tier4_vehicle_launch/urdf/vehicle.xacro\n</code></pre> </li> <li> <p>You should now have a URDF file named <code>awsim_vehicle.urdf</code>.</p> </li> </ol>"},{"location":"Components/Vehicle/DynamicSensorPlacement/#using-the-dynamic-sensor-placement-tool","title":"Using the Dynamic Sensor Placement tool:","text":"<ol> <li> <p>Open up AWSIM and place the generated <code>awsim_vehicle.urdf</code> in the Externals folder so the path will be <code>./Assets/AWSIM/Externals/awsim_vehicle.urdf</code>.</p> <p></p> </li> <li> <p>Now, under open up the <code>Dynamic sensor placement</code> tool under AWSIM menu.</p> <p></p> </li> <li> <p>Enter the path to the URDF file</p> <p></p> </li> <li> <p>Click \"Place sensors\". All the sensors should be automatically placed on the car.</p> </li> </ol>"},{"location":"Components/Vehicle/DynamicSensorPlacement/#note","title":"Note","text":"<ol> <li> <p>Only the game object named sensor_kit_base_link and its direct children will be assigned transforms if they are mentioned in the URDF file as children of sensor_kit_base_link. You will need to manually place the base_link game object.</p> <p></p> </li> <li> <p>Ensure the names of the game objects are exactly the same as those mentioned in the URDF file. Look for the name of the child of sensor_kit_base_link in the URDF file under the <code>&lt;joint&gt;</code> tag.</p> <p>For example:</p> <pre><code>&lt;joint name=\"velodyne_top_base_mount_joint\" type=\"fixed\"&gt;\n    &lt;origin rpy=\"0.0 0.0 1.575\" xyz=\"0.0 0.0 0.0\"/&gt;\n    &lt;parent link=\"sensor_kit_base_link\"/&gt;\n    &lt;child link=\"velodyne_top_base_link\"/&gt;\n&lt;/joint&gt;\n</code></pre> </li> <li> <p>Finally, ensure that all the direct children of the sensor game objects have their rotations and positions set to zero.</p> <p></p> </li> </ol>"},{"location":"Components/Vehicle/EgoVehicle/","title":"Ego Vehicle","text":""},{"location":"Components/Vehicle/EgoVehicle/#introduction","title":"Introduction","text":"<p><code>EgoVehicle</code> is a playable object that simulates a vehicle that can autonomously move around the scene. It has components (scripts) that make it possible to control it by keyboard or by Autoware (using ROS2 communication). Moreover, it provides sensory data needed for self-localization in space and detection of objects in the surrounding environment.</p> <p>The default prefab <code>EgoVehicle</code> was developed using a Lexus RX450h 2015 vehicle model with a configured sample sensor kit.</p> <p></p> <p>Own EgoVehicle prefab</p> <p>If you would like to develop your own <code>EgoVehicle</code> prefab, we encourage you to read this tutorial.</p>"},{"location":"Components/Vehicle/EgoVehicle/#supported-features","title":"Supported features","text":"<p>This vehicle model was created for Autoware simulation, and assuming that Autoware has already created a gas pedal map, this vehicle model uses acceleration as an input value. It has the following features:</p> <ul> <li>Longitudinal control by acceleration (\\(\\frac{m}{s^2}\\)).</li> <li>Lateral control by two-wheel model.</li> <li>Yaw, roll and pitch controlled by Physics engine.</li> <li>Mass-spring-damper suspension model (<code>WheelColliders</code>).</li> <li>Logical, not mechanical, automatic gears change.</li> <li>3D Mesh (<code>*.fbx</code>) as road surface for vehicle driving, gradient resistance.</li> </ul> <p>AutowareSimulation</p> <p>If you would like to see how <code>EgoVehicle</code> works or run some tests, we encourage you to familiarize yourself with the <code>AutowareSimulation</code> scene described in this section.</p>"},{"location":"Components/Vehicle/EgoVehicle/#lexus-rx450h-2015-parameters","title":"Lexus RX450h 2015 parameters","text":"Parameter Value Unit Mass \\(1500\\) \\(kg\\) Wheel base \\(2.5\\) \\(m\\) Tread width \\(Ft = 1.8; Rr = 1.8\\) \\(m\\) Center of Mass position \\(x = 0; y = 0.5; z = 0\\) \\(m\\) Moment of inertia \\(\\mathrm{yaw} = 2000; \\mathrm{roll} = 2000; \\mathrm{pitch} = 700\\) \\(kg \\cdot m^2\\) Spring rate \\(Ft = 55000; Rr = 48000\\) \\(N\\) Damper rate \\(Ft = 3000; Rr = 2500\\) \\(\\frac{N}{s}\\) Suspension stroke \\(Ft = 0.2; Rr = 0.2\\) \\(m\\) Wheel radius \\(0.365\\) \\(m\\) <p>Vehicle inertia</p> <p>In general, measuring the moment of inertia is not easy, and past papers published by NHTSA are helpful. Measured Vehicle Inertial Parameters - NHTSA 1998</p>"},{"location":"Components/Vehicle/EgoVehicle/#prefab-and-fbx","title":"Prefab and Fbx","text":"<p>Prefab can be found under the following path:</p> <pre><code>Assets/AWSIM/Prefabs/NPCs/Vehicles/Lexus RX450h 2015 Sample Sensor.prefab\n</code></pre> <p>EgoVehicle name</p> <p>In order to standardize the documentation, the name <code>EgoVehicle</code> will be used in this section as the equivalent of the prefab named <code>Lexus RX450h 2015 2015 Sample Sensor</code>.</p> <p><code>EgoVehicle</code> prefab has the following content:</p> <p></p> <p>As you can see, it consists of 3 parents for GameObjects:</p> <ul> <li><code>Models</code> - aggregating visual elements,</li> <li><code>Colliders</code> - aggregating colliders,</li> <li><code>URDF</code> - aggregating sensors,</li> <li>and 2 single GameObjects: <code>CoM</code> and <code>Reflection Probe</code>.</li> </ul> <p>All objects are described in the sections below.</p>"},{"location":"Components/Vehicle/EgoVehicle/#visual-elements","title":"Visual elements","text":"<p>Prefab is developed using models available in the form of <code>*.fbx</code> files. The visuals elements have been loaded from the appropriate <code>*.fbx</code> file and are aggregated and added in object <code>Models</code>.</p> <p><code>*.fbx</code> file for Lexus RX450h 2015 is located under the following path:</p> <pre><code>Assets/AWSIM/Models/Vehicles/Lexus RX450h 2015.fbx\n</code></pre> <p><code>Models</code> object has the following content:</p> <p></p> <p>As you can see, the additional visual element is <code>XX1 Sensor Kit</code>.</p> <p></p> <p>It was also loaded from the <code>*.fbx</code> file which can be found under the following path:</p> <pre><code>Assets/AWSIM/Models/Sensors/XX1 Sensor Kit.fbx\n</code></pre> <p>Lexus RX450h 2015.fbx</p> <p>The content of a sample <code>*.fbx</code> file is presented below, all elements except <code>Collider</code> have been added to the prefab as visual elements of the vehicle. <code>Collider</code> is used as the Mesh source for the <code>Mesh Collider</code> in the <code>BodyCollider</code> object.</p> <p></p>"},{"location":"Components/Vehicle/EgoVehicle/#link-in-the-default-scene","title":"Link in the default Scene","text":"<p>The default scene contains a single <code>Lexus RX450h 2015 Sample Sensor</code> prefab that is added as a child of the <code>EgoVehicle</code> GameObject.</p> <p></p> <p>In <code>EgoVehicle</code> prefab, the local coordinate system of the vehicle (main prefab link) should be defined in the axis of the rear wheels projected onto the ground - in the middle of the distance between them. This aspect holds significance when characterizing the dynamics of the object, as it provides convenience in terms of describing its motion and control.</p> <p> </p>"},{"location":"Components/Vehicle/EgoVehicle/#components","title":"Components","text":"<p>There are several components responsible for the full functionality of <code>Vehicle</code>:</p> <ul> <li>Rigidbody - ensures that the object is controlled by the physics engine in Unity - e.g. pulled downward by gravity.</li> <li>Vehicle (script) - provides the ability to set the acceleration of the vehicle and the steering angle of its wheels.</li> <li>Vehicle Keyboard Input (script) - provides the ability to set inputs in the Vehicle (script) via the keyboard.</li> <li>Vehicle Ros Input (script) - provides the ability to set inputs in the Vehicle (script) via subscribed ROS2 topics (outputs from Autoware).</li> <li>Vehicle Visual Effect - provides the ability to simulate vehicle lights, such as turn signals, brake lights, and hazard light.</li> </ul> <p>Scripts can be found under the following path:</p> <pre><code>Assets/AWSIM/Scripts/Vehicles/*\n</code></pre>"},{"location":"Components/Vehicle/EgoVehicle/#architecture","title":"Architecture","text":"<p>The <code>EgoVehicle</code> architecture - with dependencies - is presented on the following diagram.</p> <p></p> <p>The communication between <code>EgoVehicle</code> components is presented on two different diagrams - a flow diagram and a sequence diagram.</p> <p>The flow diagram presents a flow of information between the <code>EgoVehicle</code> components.</p> <p></p> <p>The sequence diagram provides a deeper insight in how the communication is structured and what are the steps taken by each component. Some tasks performed by the elements are presented for clarification.</p> <p></p> <p>Sequence diagram</p> <p>Please keep in mind, that Autoware message callbacks and the update loop present on the sequence diagram are executed independently and concurrently. One thing they have in common are resources - the Vehicle (script).</p>"},{"location":"Components/Vehicle/EgoVehicle/#com","title":"CoM","text":"<p><code>CoM</code> (Center of Mass) is an additional link that is defined to set the center of mass in the <code>Rigidbody</code>. The Vehicle (script) is responsible for its assignment. This measure should be defined in accordance with reality. Most often, the center of mass of the vehicle is located in its center, at the height of its wheel axis - as shown below.</p> <p> </p>"},{"location":"Components/Vehicle/EgoVehicle/#colliders","title":"Colliders","text":"<p>Colliders are used to ensure collision between objects. In <code>EgoVehicle</code>, the main <code>Collider</code> collider and colliders in <code>Wheels</code> <code>GameObject</code> for each wheel were added.</p> <p><code>Colliders</code> object has the following content:</p> <p></p>"},{"location":"Components/Vehicle/EgoVehicle/#bodycollider","title":"BodyCollider","text":"<p><code>Collider</code> is a vehicle object responsible for ensuring collision with other objects. Additionally, it can be used to detect these collisions. The <code>MeshCollider</code> takes a Mesh of object and builds its <code>Collider</code> based on it. The Mesh for the <code>Collider</code> was also loaded from the <code>*.fbx</code> file similarly to the visual elements.</p> <p></p>"},{"location":"Components/Vehicle/EgoVehicle/#wheels-colliders","title":"Wheels Colliders","text":"<p><code>WheelsColliders</code> are an essential elements from the point of view of driving vehicles on the road. They are the only ones that have contact with the roads and it is important that they are properly configured. Each vehicle, apart from the visual elements related to the wheels, should also have 4 colliders - one for each wheel.</p> <p>Wheel (script) provides a reference to the collider and visual object for the particular wheel. Thanks to this, the Vehicle (script) has the ability to perform certain actions on each of the wheels, such as:</p> <ul> <li>update the steering angle in <code>WheelCollider</code>,</li> </ul> <ul> <li>update the visual part of the wheel depending on the speed and angle of the turn,</li> </ul> <ul> <li>update the wheel contact information stored in the <code>WheelHit</code> object,</li> </ul> <ul> <li>update the force exerted by the tire forward and sideways depending on the acceleration (including cancellation of skidding),</li> </ul> <ul> <li>ensure setting the tire sleep (it is impossible to put <code>Rigidbody</code> to sleep, but putting all wheels to sleep allows to get closer to this effect).</li> </ul> <p>Wheel Collider Config (script) has been developed to prevent inspector entry for <code>WheelCollider</code> which ensures that friction is set to 0 and only wheel suspension and collisions are enabled.</p> <p></p> <p>Wheel Collider Config</p> <p>For a better understanding of the meaning of <code>WheelCollider</code> we encourage you to read this manual.</p>"},{"location":"Components/Vehicle/EgoVehicle/#rigidbody","title":"Rigidbody","text":"<p><code>Rigidbody</code> ensures that the object is controlled by the physics engine. The <code>Mass</code> of the vehicle should approximate its actual weight. In order for the vehicle to physically interact with other objects - react to collisions, <code>Is Kinematic</code> must be turned off. The <code>Use Gravity</code> should be turned on - to ensure the correct behavior of the body during movement. In addition, <code>Interpolate</code> should be turned on to ensure the physics engine's effects are smoothed out.</p>"},{"location":"Components/Vehicle/EgoVehicle/#reflection-probe","title":"Reflection Probe","text":"<p><code>Reflection Probe</code> is added to <code>EgoVehicle</code> prefab to simulate realistic reflections in a scene. It is a component that captures and stores information about the surrounding environment and uses that information to generate accurate reflections on objects in real-time. The values in the component are set as default.</p> <p>HD Additional Reflection Data (script) is additional component used to store settings for HDRP's reflection probes and is added automatically.</p>"},{"location":"Components/Vehicle/EgoVehicle/#urdf-and-sensors","title":"URDF and Sensors","text":"<p><code>URDF</code> (Unified Robot Description Format) is equivalent to the simplified <code>URDF</code> format used in ROS2. This format allows to define the positions of all sensors of the vehicle in relation to its local coordinate system. <code>URDF</code> is built using multiple GameObjects as children appropriately transformed with relation to its parent.</p> <p>A detailed description of the <code>URDF</code> structure and sensors added to prefab <code>Lexus RX450h 2015</code> is available in this section.</p>"},{"location":"Components/Vehicle/EgoVehicle/#vehicle-script","title":"Vehicle (script)","text":"<p>Vehicle (script) provides an inputs that allows the <code>EgoVehicle</code> to move. Script inputs provides the ability to set the acceleration of the vehicle and the steering angle of its wheels, taking into account the effects of suspension and gravity. It also provides an input to set the gear in the gearbox and to control the turn signals. Script inputs can be set by one of the following scripts: Vehicle Ros Input (script) or Vehicle Keyboard Input (script).</p> <p>The script performs several steps periodically:</p> <ul> <li>checks whether the current inputs meet the set limits and adjusts them within them,</li> <li>calculates the current linear velocity, angular velocity vector and local acceleration vector,</li> <li>set the current steering angle in the script for each wheel and perform their updates,</li> <li>if the current gear is <code>PARKING</code> and the vehicle is stopped (its speed and acceleration are below the set thresholds), it puts the vehicle (<code>Rigidbody</code>) and its wheels (Wheel (script)) to sleep,</li> <li>if the vehicle has not been put to sleep, it sets the current acceleration to each with the appropriate sign depending on the <code>DRIVE</code> and <code>REVERSE</code> gear.</li> </ul>"},{"location":"Components/Vehicle/EgoVehicle/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":"<p>The script uses the <code>CoM</code> link reference to assign the center of mass of the vehicle to the <code>Rigidbody</code>. In addiction, <code>Use inertia</code> allows to define the <code>inertia</code> tensor for component <code>Rigidbody</code> - by default it is disabled.</p> <p><code>Physics Settings</code> - allows to set values used to control vehicle physics:</p> <ul> <li><code>Sleep Velocity Threshold</code> - velocity threshold used to put vehicle to sleep,</li> <li><code>Sleep Time Threshold</code> - time threshold for which the vehicle must not exceed the <code>Sleep Velocity Threshold</code>, have contact with the ground and a set acceleration input equal to zero,</li> <li><code>Skidding Cancel Rate</code> - coefficient that determines the rate at which skidding is canceled, affects the anti-skid force of the wheels - the higher the value, the faster the cancellation of the skid.</li> </ul> <p><code>Axles Settings</code> contains references to (Wheel (script)) scripts to control each wheel. Thanks to them, the Vehicle (script) is able to set their steering angle and accelerations.</p> <p><code>Input Settings</code> - allows to set limits for values on script input:</p> <ul> <li><code>Max Steer Angle Input</code> - maximum value of acceleration set by the script (also negative),</li> <li><code>Max Acceleration Input</code> -  maximum steering angle of the wheels set by the script (also negative).</li> </ul> <p><code>Inputs</code> - are only used as a preview of the currently set values in the script input:</p> <p></p>"},{"location":"Components/Vehicle/EgoVehicle/#input-data","title":"Input Data","text":"Category Type Description AccelerationInput float Acceleration input (m/s^2). On the plane, output the force that will result in this acceleration. On a slope, it is affected by the slope resistance, so it does not match the input. SteerAngleInput float Vehicle steering input (degree). Negative steers left, positive right AutomaticShiftInput enumeration Vehicle gear shift input (AT).Values: <code>PARKING</code>, <code>REVERSE</code>, <code>NEUTRAL</code>, <code>DRIVE</code>. SignalInput enumeration Vehicle turn signal input.Values: <code>NONE</code>, <code>LEFT</code>, <code>RIGHT</code>, <code>HAZARD</code>."},{"location":"Components/Vehicle/EgoVehicle/#output-data","title":"Output data","text":"Category Type Description LocalAcceleration Vector3 Acceleration(m/s^2) in the local coordinate system of the vehicle Speed float Vehicle speed (m/s). SteerAngle float Vehicle steering angle (degree). Signal enumeration Vehicle turn signal. Velocity Vector3 Vehicle velocity (m/s) LocalVelocity Vector3 Vehicle local velocity (m/s) AngularVelocity Vector3 Vehicle angular velocity (rad/s) <p>The acceleration or deceleration of the vehicle is determined by <code>AutomaticShiftInput</code> and <code>AccelerationInput</code>. The vehicle will not move in the opposite direction of the (<code>DRIVE</code> or <code>REVERSE</code>) input.</p> <p>Example</p> <p>Sample vehicle behaves:</p> <ul> <li> <p>Sample 1 - vehicle will accelerate with input values (gradient resistance is received).</p> <pre><code>AutomaticShiftInput = DRIVE\nSpeed = Any value\nAccelerationInput &gt; 0\n</code></pre> </li> </ul> <ul> <li> <p>Sample 2 - vehicle will decelerate (like a brake).</p> <pre><code>AutomaticShiftInput = DRIVE\nSpeed &gt; 0\nAccelerationInput &lt; 0\n</code></pre> </li> </ul> <ul> <li> <p>Sample 3 - vehicle will continuously stop.</p> <pre><code>AutomaticShiftInput = DRIVE\nSpeed &lt;= 0\nAccelerationInput &lt; 0\n</code></pre> </li> </ul>"},{"location":"Components/Vehicle/EgoVehicle/#vehicle-ros-script","title":"Vehicle Ros (script)","text":"<p>Vehicle Ros (script) is responsible for subscribing to messages that are vehicle control commands. The values read from the message are set on the inputs of the Vehicle (script) script.</p> <p>The concept for vehicle dynamics is suitable for Autoware's <code>autoware_control_msgs/Control</code> and <code>autoware_vehicle_msgs/GearCommand</code> messages interface usage. The script sets gear, steering angle of wheels and acceleration of the vehicle (read from the aforementioned messages) to the Vehicle (script) input. In the case of VehicleEmergencyStamped message it sets the absolute acceleration equal to 0. In addition, also through Vehicle (script), the appropriate lights are turned on and off depending on TurnIndicatorsCommand and HazardLightsCommand  messages.</p>"},{"location":"Components/Vehicle/EgoVehicle/#elements-configurable-from-the-editor-level_1","title":"Elements configurable from the editor level","text":"<ul> <li><code>* Command Topic</code> - topic on which suitable type of information is subscribed (default: listed in the table below)</li> <li><code>QoS</code>- Quality of service profile used in the publication (default assumed as <code>\"system_default\"</code>: <code>Reliable</code>, <code>TransientLocal</code>, <code>Keep last/1</code>)</li> <li><code>Vehicle</code> - reference to a script in the vehicle object where the subscribed values are to be set (default: <code>None</code>)</li> </ul>"},{"location":"Components/Vehicle/EgoVehicle/#subscribed-topics","title":"Subscribed Topics","text":"<ul> <li>QoS: <code>Reliable</code>, <code>TransientLocal</code>, <code>KeepLast/1</code></li> </ul> Category Topic Message type Frequency (Autoware dependent) TurnIndicatorsCommand <code>/control/command/turn_indicators_cmd</code> <code>autoware_vehicle_msgs/TurnIndicatorsCommand</code> <code>10</code> HazardLightsCommand <code>/control/command/hazard_lights_cmd</code> <code>autoware_vehicle_msgs/HazardLightsCommand</code> <code>10</code> AckermannControlCommand <code>/control/command/control_cmd</code> <code>autoware_control_msgs/Control</code> <code>60</code> GearCommand <code>/control/command/gear_cmd</code> <code>autoware_vehicle_msgs/GearCommand</code> <code>10</code> VehicleEmergencyStamped <code>/control/command/emergency_cmd</code> <code>tier4_vehicle_msgs/msg/VehicleEmergencyStamped</code> <code>60</code> <p>ROS2 Topics</p> <p>If you would like to know all the topics used in communication Autoware with AWSIM, we encourage you to familiarize yourself with this section</p>"},{"location":"Components/Vehicle/EgoVehicle/#vehicle-keyboard-script","title":"Vehicle Keyboard (script)","text":"<p>Vehicle Keyboard (script) allows <code>EgoVehicle</code> to be controlled by the keyboard. Thanks to this, it is possible to switch on the appropriate gear of the gearbox, turn the lights on/off, set the acceleration and steering of the wheels. It's all set in the Vehicle (script) of the object assigned in the <code>Vehicle</code> field. The table below shows the available control options.</p> Button Option <code>d</code> Switch to move forward (drive gear) <code>r</code> Switch to move backwards (reverse gear) <code>n</code> Switch to neutral <code>p</code> Switch to parking gear <code>UP ARROW</code> Forward acceleration <code>DOWN ARROW</code> Reverse acceleration (decelerate) <code>LEFT/RIGHT ARROW</code> Turning <code>1</code> Turn left blinker on (right off) <code>2</code> Turn right blinker on (left off) <code>3</code> Turn on hazard lights <code>4</code> Turn off blinker or hazard lights <p>WASD</p> <p>Controlling the movement of the vehicle with <code>WASD</code> as the equivalent of arrow keys is acceptable, but remember that the <code>d</code> button engages the drive gear.</p>"},{"location":"Components/Vehicle/EgoVehicle/#elements-configurable-from-the-editor-level_2","title":"Elements configurable from the editor level","text":"<ul> <li><code>Max Acceleration</code>- maximum value of acceleration set by the script (also negative)</li> <li><code>Max Steer Angle</code> - maximum steering angle of the wheels set by the script (also negative)</li> </ul> <p>Value limits</p> <p><code>Max Acceleration</code> and <code>Max Steer Angle</code> values greater than those set in the Vehicle (script) are limited by the script itself - they will not be exceeded.</p>"},{"location":"Components/Vehicle/EgoVehicle/#vehicle-visual-effect-script","title":"Vehicle Visual Effect (script)","text":"<p>This part of the settings is related to the configuration of the emission of materials when a specific lighting is activated. There are 4 types of lights: <code>Brake</code>, <code>Left Turn Signal</code>, <code>Right Turn Signal</code> and <code>Reverse</code>. Each of the lights has its visual equivalent in the form of a Mesh. In the case of <code>EgoVehicle</code>, each light type has its own GameObject which contains the Mesh assigned.</p> <p></p> <p>For each type of light, the appropriate <code>Material Index</code> (equivalent of element index in mesh) and <code>Lighting Color</code> are assigned - yellow for <code>Turn Signals</code>, red for <code>Break</code> and white for <code>Reverse</code>.</p> <p><code>Lighting Intensity</code> values are also configured - the greater the value, the more light will be emitted. This value is related to <code>Lighting Exposure Weight</code> parameter that is a exposure weight - the lower the value, the more light is emitted.</p> <p>All types of lighting are switched on and off depending on the values obtained from the Vehicle (script) of the vehicle, which is assigned in the <code>Vehicle</code> field.</p>"},{"location":"Components/Vehicle/EgoVehicle/#elements-configurable-from-the-editor-level_3","title":"Elements configurable from the editor level","text":"<ul> <li><code>Turn Signal Timer Interval Sec</code> - time interval for flashing lights - value \\(0.5\\) means that the light will be on for \\(0.5s\\), then it will be turned off for \\(0.5s\\) and turned on again. (default: <code>0.5</code>)</li> </ul>"},{"location":"Components/Vehicle/FollowCamera/","title":"FollowCamera","text":""},{"location":"Components/Vehicle/FollowCamera/#introduction","title":"Introduction","text":"<p>The <code>FollowCamera</code> component is designed to track a specified target object within the scene. It is attached to the main camera and maintains a defined distance and height from the target. Additionally, it offers the flexibility of custom rotation around the target as an optional feature.</p>"},{"location":"Components/Vehicle/FollowCamera/#elements-configurable-from-the-editor-level","title":"Elements configurable from the editor level","text":""},{"location":"Components/Vehicle/FollowCamera/#required-member","title":"Required member","text":"<ul> <li><code>Target</code> - the transform component of the tracked object</li> </ul>"},{"location":"Components/Vehicle/FollowCamera/#base-setttings","title":"Base Setttings","text":"<ul> <li><code>Distance</code> - base distance between the camera and the target object</li> <li><code>Offset</code> - lateral offset of the camera position</li> <li><code>Height</code> - base height of the camera above the target object</li> <li><code>HeightMultiplier</code> - camera height multiplier</li> </ul>"},{"location":"Components/Vehicle/FollowCamera/#optional-movement-setttings","title":"Optional Movement Setttings","text":"<ul> <li><code>RotateAroundModeToggle</code> - toggle key between rotate around mode and default follow mode</li> <li><code>RotateAroundSensitivity</code> - mouse movement sensitivity for camera rotation around the target</li> <li><code>HeightAdjustmentSensitivity</code> - mouse movement sensitivity for camera height adjustment</li> <li><code>ZoomSensitivity</code> - mouse scroll wheel sensitivity for camera zoom</li> <li><code>InvertHorzAxis</code> - invert horizontal mouse movement</li> <li><code>InvertVertAxis</code> - invert vertical mouse movement</li> <li><code>InvertScrollWheel</code> - invert mouse scroll wheel</li> <li><code>MaxHeight</code> - maximum value of camera height</li> <li><code>MinDistance</code> - minimum value of camera distance to target object</li> <li><code>MaxDistance</code> - maximum value of camera distance to target object</li> </ul>"},{"location":"Components/Vehicle/FollowCamera/#rotate-around-mode","title":"Rotate Around Mode","text":"<p>Camera rotation around the target can be activated by pressing the <code>RotateAroundModeToggle</code> key (default <code>'C'</code> key). In this mode, the user can manually adjust the camera view at run-time using the mouse. To deactivate the Rotate Around mode, press the <code>RotateAroundModeToggle</code> key once more.</p> <p>In the Rotate Around Mode camera view can be controlled as follows:</p> <ul> <li><code>hold left shift + mouse movement</code>: to adjust the camera position,</li> <li><code>hold left shift + mouse scroll wheel</code>: to zoom in or out of the camera view,</li> <li><code>left shift + left arrow</code>: to set left camera view,</li> <li><code>left shift + right arrow</code>: to set right camera view,</li> <li><code>left shift + up arrow</code>: to set front camera view,</li> <li><code>left shift + down arrow</code>: to set back camera view.</li> </ul>"},{"location":"Components/Vehicle/FollowCamera/#optional","title":"Optional","text":"<p>An optional prefab featuring a UI panel, located at <code>Assets/Prefabs/UI/MainCameraView.prefab</code>, can be used to showcase a user guide. To integrate this prefab into the scene, drag and drop it beneath the Canvas object. This prefab displays instructions on how to adjust the camera view whenever the Rotate Around Mode is activated.</p>"},{"location":"Components/Vehicle/URDFAndSensors/","title":"URDF And Sensors","text":""},{"location":"Components/Vehicle/URDFAndSensors/#urdf-and-sensors","title":"URDF and Sensors","text":"<p>This section describes the placement of sensors in <code>EgoVehicle</code> on the example of a <code>Lexus RX450h 2015 Sample Sensor</code> prefab.</p> <p><code>URDF</code> (Unified Robot Description Format) is equivalent to the simplified URDF format used in ROS2. This format allows to define the positions of all sensors of the vehicle in relation to its main parent prefab coordinate system.</p> <p></p> <p><code>URDF</code> is added directly to the main parent of the prefab and there are no transforms between these objects. It is built using multiple GameObjects as children appropriately transformed with relation to its parent.</p> <p></p> <p>The transforms in the <code>URDF</code> object are defined using the data from the sensor kit documentation used in the vehicle. Such data can be obtained from sensor kit packages for Autoware, for example: <code>awsim_sensor_kit_launch</code> - it is used in the AWSIM compatible version of Autoware. This package contains a description of transforms between coordinate systems (frames) in the form of <code>*.yaml</code> files: sensors_calibration and sensor_kit_calibration.</p> <p>In the first file, the transform of the sensor kit frame (<code>sensor_kit_base_link</code>) relative to the local vehicle frame (<code>base_link</code>) is defined. In Unity, this transform is defined in the object <code>Sensor Kit</code>. While the second file contains a definition of the transformations of all sensors with respect to the sensor kit - they are described in the <code>Sensor Kit</code> subsections.</p> <p>Transformations</p> <p>Please note that the transformation Objects are intended to be a direct reflection of frames existing in ROS2. All frame Objects are defined as children of <code>base_link</code> and consist of nothing but a transformation - analogical to the one present in ROS2 (keep in mind the coordinate system conversion). The sensor Objects are added to the transformation Object with no transformation of their own.</p> <p>Coordinate system conventions</p> <p>Unity uses a left-handed convention for its coordinate system, while the ROS2 uses a right-handed convention. For this reason, you should remember to perform conversions to get the correct transforms.</p>"},{"location":"Components/Vehicle/URDFAndSensors/#base-link","title":"Base Link","text":"<p><code>Base Link</code> (frame named <code>base_link</code>) is the formalized local coordinate system in <code>URDF</code>. All sensors that publish data specified in some frame present in Autoware are defined in relation to <code>base_link</code>. It is a standard practice in ROS, that <code>base_link</code> is a parent transformation of the whole robot and all robot parts are defined in some relation to the <code>base_link</code>.</p> <p>If any device publishes data in the <code>base_link</code> frame - it is added as a direct child, with no additional transformation intermediate Object (<code>PoseSensor</code> is an example). However, if this device has its own frame, it is added as a child to its frame Object - which provides an additional transformation. The final transformation can consist of many intermediate transformation Objects. The frame Objects are added to the <code>base_link</code> (<code>GnssSensor</code> and its parent <code>gnss_link</code> are an example).</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#sensor-kit","title":"Sensor Kit","text":"<p><code>Sensor Kit</code> (frame named <code>sensor_kit_base_link</code>) is a set of objects that consists of all simulated sensors that are physically present in an autonomous vehicle and have their own coordinate system (frame). This set of sensors has its own frame <code>sensor_kit_base_link</code> that is relative to the <code>base_link</code>.</p> <p>In the <code>Lexus RX450h 2015 Sample Sensor</code> prefab, it is added to the <code>base_link</code> GameObject with an appropriately defined transformation. It acts as an intermediate frame GameObject. <code>Sensor Kit</code> is located on the top of the vehicle, so it is significantly shifted about the <code>Oy</code> and <code>Oz</code> axes. Sensors can be defined directly in this Object (<code>VelodyneVLP16</code> is an example), or have their own transformation Object added on top of the <code>sensor_kit_base_link</code> (like <code>GnssSensor</code> mentioned in the <code>base_link</code> section).</p> <p>The sensors described in this subsection are defined in relation to the <code>sensor_kit_base_link</code> frame.</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#lidars","title":"LiDARs","text":"<p><code>LidarSensor</code> is the component that simulates the LiDAR (Light Detection and Ranging) sensor. The LiDARs mounted on the top of autonomous vehicles are primarily used to scan the environment for localization in space and for detection and identification of obstacles. LiDARs placed on the left and right sides of the vehicle are mainly used to monitor the traffic lane and detect vehicles moving in adjacent lanes. A detailed description of this sensor is available in this section.</p> <p><code>Lexus RX450h 2015 Sample Sensor</code> prefab has one <code>VelodyneVLP16</code> prefab sensor configured on the top of the vehicle, mainly used for location in space, but also for object recognition. Since the top LiDAR publishes data directly in the <code>sensor_kit_base_link</code> frame, the prefab is added directly to it - there is no transform. The other two remaining LiDARs are defined, but disabled - they do not provide data from space (but you can enable them!).</p>"},{"location":"Components/Vehicle/URDFAndSensors/#top","title":"Top","text":""},{"location":"Components/Vehicle/URDFAndSensors/#left-disabled","title":"Left - disabled","text":""},{"location":"Components/Vehicle/URDFAndSensors/#right-disabled","title":"Right - disabled","text":""},{"location":"Components/Vehicle/URDFAndSensors/#imu","title":"IMU","text":"<p><code>IMUSensor</code> is a component that simulates an IMU (Inertial Measurement Unit) sensor. It measures acceleration and angular velocity of the <code>EgoVehicle</code>. A detailed description of this sensor is available in this section.</p> <p><code>Lexus RX450h 2015 Sample Sensor</code> has one such sensor located on the top of the vehicle. It is added to an Object <code>tamagawa/imu_link</code> that matches its <code>frame_id</code> and contains its transform with respect to <code>sensor_kit_base_link</code>. This transformation has no transition, but only rotation around the <code>Oy</code> and <code>Oz</code> axes. The transform is defined in such a way that its axis <code>Oy</code> points downwards - in accordance with the gravity vector.</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#gnss","title":"GNSS","text":"<p><code>GnssSensor</code> is a component which simulates the position of vehicle computed by the Global Navigation Satellite. A detailed description of this sensor is available in this section.</p> <p><code>Lexus RX450h 2015 Sample Sensor</code> prefab has one such sensor located on top of the vehicle. It is added to an Object <code>gnss_link</code> that matches its <code>frame_id</code> and contains its transform with respect to <code>sensor_kit_base_link</code>. The frame is slightly moved back along the <code>Oy</code> and <code>Oz</code> axes.</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#camera","title":"Camera","text":"<p><code>CameraSensor</code> is a component that simulates an RGB camera. Autonomous vehicles can be equipped with many cameras used for various purposes. A detailed description of this sensor is available in this section.</p> <p><code>Lexus RX450h 2015 Sample Sensor</code> prefab has one camera, positioned on top of the vehicle in such a way that the cameras field of view provides an image including traffic lights - the status of which must be recognized by Autoware. It is added to an Object <code>traffic_light_left_camera/camera_link</code> that matches its <code>frame_id</code> and contains its transform with respect to <code>sensor_kit_base_link</code>.</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#pose","title":"Pose","text":"<p><code>PoseSensor</code> is a component which provides access to the current position and rotation of the <code>EgoVehicle</code> -  is added as a ground truth.</p> <p>The position and orientation of <code>EgoVehicle</code> is defined as the position of the frame <code>base_link</code> in the global frame, so this Object is added directly as its child without a transform.</p> <p></p>"},{"location":"Components/Vehicle/URDFAndSensors/#vehiclesensor","title":"VehicleSensor","text":"<p><code>VehicleStatusSensor</code> is a component that is designed to aggregate information about the current state of the <code>EgoVehicle</code>, such as the active control mode, vehicle speed, steering of its wheels, or turn signal status. A detailed description of this sensor is available in this section.</p> <p>This Object is not strictly related to any frame, however, it is assumed as a sensor, therefore it is added to the <code>URDF</code>.</p> <p></p>"},{"location":"Components/Vehicle/VPPIntegration/","title":"VPP Integration","text":""},{"location":"Components/Vehicle/VPPIntegration/#vpp-integration-with-the-ego-vehicle","title":"VPP Integration with the Ego Vehicle","text":"<p>VPP is a package that provides realistic vehicle simulation. Here you can find a guide on how to integrate VPP with the Ego Vehicle.</p> <p>Example Vehicle</p> <p>It is recommended to take a look at the default Lexus vehicle prefab as example to see how the components are set up.</p>"},{"location":"Components/Vehicle/VPPIntegration/#initial-setup","title":"Initial setup","text":"<p>To be able to use VPP with the ego vehicle, please attach these components to the vehicle prefab: From the VPP package:</p> <p>Added to prefab root:</p> <ul> <li><code>Vehicle Controller</code></li> <li><code>Standard Input</code></li> <li><code>Camera Target</code> (Optional, you can use your own implementation)</li> <li><code>Telemetry Window</code> (Optional, used to display telemetry data)</li> <li><code>Visual Effects</code> (Optional, used to visualize the steering wheel)</li> <li><code>VP Vehicle Toolkit</code></li> </ul> <p>Wheel Colliders</p> <p>VPP uses its own implementation of the <code>Wheel Colliders</code>! For the wheel colliders, please add the <code>Wheel Colliders</code> provided by the VPP package. They share the same name with the default Unity components.</p> <p>After adding those scripts continue with adding the following components to the vehicle prefab:</p> <p>Added to prefab root:</p> <ul> <li><code>AutowareVPPAdapter.cs</code>: Used for applying the control inputs to the vehicle.</li> <li><code>Ros2ToVPPInput.cs</code>: Used for receiving the control inputs from the Autoware.</li> <li><code>VPPVehicleSignalHandler.cs</code>: Used for handling the signals from the vehicle.</li> <li><code>VehiclePedalMapLoader.cs</code>: Used for loading the pedal maps for the vehicle.</li> </ul> <p>To be able to report the vehicle state to the Autoware, you'll need this script in the URDF. This script is added in the <code>VehicleStatusSensor</code> prefab by default.</p> <p>Added to URDF:</p> <ul> <li><code>VPPToRos2Publisher.cs</code>: Used for publishing the vehicle state to the Autoware.</li> </ul>"},{"location":"Components/Vehicle/VPPIntegration/#setting-up-the-components","title":"Setting up the components","text":"<p>After doing the initial setup, we will need to configure the components for your vehicle.</p> <p>Set up the <code>Vehicle Controller</code> and the <code>Wheel Colliders</code> with the correct values for your vehicle. Using values from the real vehicles will result in a more realistic simulation.</p> <p>You can refer to VPP documentation for more detailed information about the VPP components.</p> <p>As for the other components we've added to the vehicle, we have to give their necessary references.</p> <p>Camera Controller</p> <p>If you've added the <code>Camera Target</code> component, you'll need a separate gameObject with a <code>Camera Controller</code>. Don't forget to assign vehicle Transform reference in this component. This is used by default in the <code>AutowareSimulation</code> scene.</p>"},{"location":"Components/Vehicle/VPPIntegration/#autowarevppadaptercs","title":"AutowareVPPAdapter.cs","text":"Variable Description <code>Front Wheels</code> Used for handling the steering input. Give front wheel colliders of the vehicle. <code>Simulate Steering</code> Simulate the steering instead of assigning wheel angles directly. <code>Steer Wheel Input</code> Change applied to steering wheel when simulating steering. Applied every in fixed update as an input to the VPP steering. (0-100) <code>Do Use Pedal Maps</code> Whether to use the pedal maps for the vehicle or not. If set to false the vehicle will use the PID Controller for the longitudinal control. <code>Emergency Brake Percent</code> The amount of brake pedal percent applied when the emergency is triggered. (0-1) <code>Update Position Offset Y</code> The height offset value for the vehicle position when initializing position of the vehicle from the Rviz. (In meters) <code>Update Position Ray Origin Y</code> The height of the raycast origin for the vehicle position when initializing position of the vehicle from the Rviz. (In meters)"},{"location":"Components/Vehicle/VPPIntegration/#ros2tovppinputcs","title":"Ros2ToVPPInput.cs","text":"Variable Description <code>Adapter</code> Reference to the <code>AutowareVPPAdapter</code> script. (Drag the object that has the corresponding component.) <code>Topics</code> Topics used in the Ros2 communication. You can change the topics according to your case. <ul> <li><code>QoS Settings</code>: Quality of Service settings for the Ros2 communication. Currently, it is as follows:</li> </ul> Variable Value <code>Reliability Policy</code> Reliable <code>Durability Policy</code> Transient Local <code>History Policy</code> Keep Last <code>Depth</code> 1 <ul> <li><code>Position QoS Input</code>: QoS settings for the position topic:</li> </ul> Variable Value <code>Reliability Policy</code> Reliable <code>Durability Policy</code> Volatile <code>History Policy</code> Keep Last <code>Depth</code> 1"},{"location":"Components/Vehicle/VPPIntegration/#vehiclesignalhandlercs","title":"VehicleSignalHandler.cs","text":"Variable Description <code>Adapter</code> Reference to the <code>AutowareVPPAdapter</code> script. (Drag the object that has the corresponding component.) <code>Vp Vehicle</code> Reference to the <code>Vehicle Controller</code> script. (Drag the object that has the corresponding component.) <p>Rest of the signal related settings are same as <code>VehicleVisualEffect.cs</code>. These two scripts are basically same.</p>"},{"location":"Components/Vehicle/VPPIntegration/#vehiclepedalmaploadercs","title":"VehiclePedalMapLoader.cs","text":"Variable Description <code>Accel Map Csv</code> The file reference of the acceleration pedal map. (Drag the corresponding csv file from the Project Window) <code>Brake Map Csv</code> The file reference of the brake pedal map. (Drag the corresponding csv file from the Project Window) <p>Pedal Maps</p> <p>These pedal maps are loaded in runtime. The default location for the pedal maps is <code>Assets/Resources/Pedal Maps/...</code>. You can find the Lexus's pedal maps in the same location.</p>"},{"location":"Components/Vehicle/VPPIntegration/#vpptoros2publishercs","title":"VPPToRos2Publisher.cs","text":"<p>Same as <code>VehicleStatusSensor.cs</code> but the <code>Adapter</code> reference is the <code>AutowareVPPAdapter</code> script.</p>"},{"location":"Components/Vehicle/VPPIntegration/#setting-up-the-child-objects","title":"Setting up the child objects","text":"<p>For the VPP to work correctly, you need to set up the child objects of the vehicle prefab.</p>"},{"location":"Components/Vehicle/VPPIntegration/#setting-up-reference-for-ackermann-steering","title":"Setting up reference for Ackermann Steering:","text":"<ol> <li>Create an empty game object named <code>\"Ackermann\"</code> and set as the direct child of the prefab. Then assign reference in    the <code>Vehicle Controller</code>.</li> <li>Move position of the created game object to the middle of the rear axle and set its height to the bottom of the rear    wheels. Make sure the rotations are <code>(0,0,0)</code></li> </ol> <p>Assign the reference in the <code>Vehicle Controller</code>. For details, you can refer to the VPP.</p> <p>VPP Reference: Ackermann</p>"},{"location":"Components/Vehicle/VPPIntegration/#setting-up-the-reference-for-dynamics","title":"Setting up the reference for Dynamics:","text":"<ol> <li> <p>Create an empty game object named <code>\"Dynamics\"</code> and set as the direct child of the prefab. Add the following    components to this object:</p> <ul> <li><code>Rolling Friction</code></li> <li><code>Anti-roll Bar</code> (x2)</li> </ul> </li> <li> <p>Create an empty game object named <code>\"Aero\"</code> and set as the child of the \"Dynamics\". Add the following    components to this object:</p> <ul> <li><code>Aerodynamic Surface</code></li> </ul> </li> </ol> <p>Assign the references in the <code>Vehicle Controller</code>. For configuring these components, you can refer to the VPP.</p> <p>VPP Reference: Dynamics</p>"},{"location":"DeveloperGuide/Contact/","title":"Contact","text":""},{"location":"DeveloperGuide/Contact/#contact","title":"Contact","text":"<p>English/\u65e5\u672c\u8a9e OK</p> <p>e-mail : takatoki.makino@tier4.jp</p> <p>twitter : https://twitter.com/mackierx111</p>"},{"location":"DeveloperGuide/Documentation/","title":"Documentation","text":""},{"location":"DeveloperGuide/Documentation/#documentation","title":"Documentation","text":"<p>This document uses Material for MkDocs.</p>"},{"location":"DeveloperGuide/Documentation/#local-hosting","title":"Local hosting","text":"<p>1 Install Material for <code>MkDocs</code>. </p><pre><code>$ pip install mkdocs-material\n</code></pre> 2 Hosting on localhost. <pre><code>$ cd AWSIM\n$ mkdocs serve\nINFO     -  Building documentation...\nINFO     -  Cleaning site directory\nINFO     -  Documentation built in 0.16 seconds\nINFO     -  [03:13:22] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO     -  [03:13:22] Serving on http://127.0.0.1:8000/\n</code></pre> <p>3 Access <code>http://127.0.0.1:8000/</code> with a web browser.</p> <p></p> <p>For further reference see Material for MkDocs - Getting started.</p>"},{"location":"DeveloperGuide/Documentation/#mkdocs-files","title":"MkDocs files","text":"<p>Use the following <code>/docs</code> directory and <code>mkdocs.yml</code> for new documentation files. </p><pre><code>AWSIM\n\u251c\u2500 docs/                // markdown and image file for each document.\n\u2514\u2500 mkdocs.yml           // mkdocs config.\n</code></pre> Create one directory per document. For example, the directory structure of this \"Documentation\" page might look like this. <pre><code>AWSIM\n\u2514\u2500 docs/                            // Root of all documents\n    \u2514\u2500 DeveloperGuide               // Category\n        \u2514\u2500 Documentation            // Root of each document\n            \u251c\u2500 index.md             // Markdown file\n            \u2514\u2500 image_0.png          // Images used in markdown file\n</code></pre>"},{"location":"DeveloperGuide/Documentation/#deploy-hosting","title":"Deploy &amp; Hosting","text":"<p>When docs are pushed to the main branch, they are deployed to GitHub Pages using GitHub Actions. See also Material for MkDocs - Publishing your site</p>"},{"location":"DeveloperGuide/EditorSetup/Graphy/","title":"Graphy Asset Setup","text":""},{"location":"DeveloperGuide/EditorSetup/Graphy/#graphy-asset-setup","title":"Graphy Asset Setup","text":""},{"location":"DeveloperGuide/EditorSetup/Graphy/#add-graphy-from-asset-store","title":"Add Graphy from Asset Store","text":"<p>1) Go to Unity Asset Store and add Graphy to personal assets.</p> <p>Graphy Asset Store link:</p> <ul> <li>https://assetstore.unity.com/packages/tools/gui/graphy-ultimate-fps-counter-stats-monitor-debugger-105778</li> </ul>"},{"location":"DeveloperGuide/EditorSetup/Graphy/#add-graphy-to-the-unity-editor","title":"Add Graphy to the Unity Editor","text":"<p>1) Open up the Unity Editor.   - Open up a temporary new scene by File -&gt; New Scene -&gt; Empty(Built-in) -&gt; Create   - This is due to a bug with Unity crashing on certain Linux configurations.   - Once the package is imported, you can open up the desired scene. 2) Go to the <code>Window</code> menu and select <code>Package Manager</code>. 3) Make sure the <code>My Assets</code> tab is selected from the top left of the Package Manager window. 4) Find &amp; select the Graphy from the list and click <code>Download</code> or <code>Import</code> from the bottom left of the Package Manager    window. 5) There will be a popup window showing contents of the package. Click <code>Import</code> to add Graphy to the project.</p> <p>After the import is complete, you should be able to see Graphy prefab in the <code>Hierarchy</code> window of the <code>AutowareSimulation</code> scene. If it's missing you can add it to scene by following steps below.</p>"},{"location":"DeveloperGuide/EditorSetup/Graphy/#integrating-graphy-into-custom-scenes","title":"Integrating Graphy into custom scenes","text":"<p>Graphy is pre-integrated within the <code>AutowareSimulation</code> scene. To incorporate Graphy into your own custom scenes, please adhere to the following steps:</p> <p>1) Go to the Assets folder in the Project window. 2) Open <code>Graphy &gt; Prefab</code> folder. 3) Drag the <code>Graphy</code> prefab into the scene. 4) You can customize your Graphy by selecting the Graphy prefab in the scene and changing the settings in the inspector    window.</p>"},{"location":"DeveloperGuide/EditorSetup/Graphy/#useful-links","title":"Useful links:","text":"<p>Unity Package manager:</p> <ul> <li>https://docs.unity3d.com/Manual/upm-ui.html</li> </ul> <p>Graphy Github page:</p> <ul> <li>https://github.com/Tayx94/graphy</li> </ul> <p>Graphy Documentation:</p> <ul> <li>https://github.com/Tayx94/graphy/blob/master/Readme!%20-%20Graphy%20-%20Documentation.pdf</li> </ul>"},{"location":"DeveloperGuide/EditorSetup/JetBrainsRider/","title":"Rider Configuration","text":""},{"location":"DeveloperGuide/EditorSetup/JetBrainsRider/#jetbrains-rider-setup-with-unity","title":"JetBrains Rider setup with Unity","text":""},{"location":"DeveloperGuide/EditorSetup/JetBrainsRider/#install-jetbrains-rider","title":"Install JetBrains Rider:","text":"<p>Follow the steps in:</p> <ul> <li>https://www.jetbrains.com/help/rider/Installation_guide.html#snap</li> </ul> <pre><code>sudo snap install rider --classic\n</code></pre>"},{"location":"DeveloperGuide/EditorSetup/JetBrainsRider/#install-net-sdk","title":"Install .NET SDK:","text":"<p>Follow the steps in:</p> <ul> <li>https://learn.microsoft.com/en-us/dotnet/core/install/linux-ubuntu#register-the-microsoft-package-repository</li> </ul> <pre><code># Get Ubuntu version\ndeclare repo_version=$(if command -v lsb_release &amp;&gt; /dev/null; then lsb_release -r -s; else grep -oP '(?&lt;=^VERSION_ID=).+' /etc/os-release | tr -d '\"'; fi)\n\n# Download Microsoft signing key and repository\nwget https://packages.microsoft.com/config/ubuntu/$repo_version/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\n\n# Install Microsoft signing key and repository\nsudo dpkg -i packages-microsoft-prod.deb\n\n# Clean up\nrm packages-microsoft-prod.deb\n\n# Update packages\nsudo apt update\nsudo apt install dotnet-sdk-8.0\n</code></pre>"},{"location":"DeveloperGuide/EditorSetup/JetBrainsRider/#connect-rider-to-unity-editor","title":"Connect Rider to Unity Editor:","text":"<p>Follow the steps in:</p> <ul> <li>https://www.jetbrains.com/help/rider/Unity.html#8f092cb7_9</li> </ul> <pre><code>1) Open an existing Unity project in the Unity Editor.\n\n2) Select Edit &gt; Preferences (Unity &gt; Settings on macOS) and open the External Tools page.\n\n3) In the External Script Editor, select a \"Rider\" installation.\n\n4) In the Preferences window, click \"Regenerate project files\" under the External Tools section.\n\n5) While still in the Unity Editor, right-click anywhere in the Project view and select Open C# Project.\n\n6) Rider will start automatically and open the solution related to this Unity project. Once the solution is loaded, Rider and the Unity Editor become connected. The Unity icon on the toolbar shows the current connection status:\n</code></pre>"},{"location":"DeveloperGuide/EditorSetup/VPPCommunityEdition/","title":"Vehicle Physics Pro CE Setup","text":""},{"location":"DeveloperGuide/EditorSetup/VPPCommunityEdition/#vehicle-physics-pro-community-edition","title":"Vehicle Physics Pro Community Edition","text":""},{"location":"DeveloperGuide/EditorSetup/VPPCommunityEdition/#add-vpp-ce-from-asset-store","title":"Add VPP-CE from Asset Store","text":"<ul> <li>Go to Unity Asset Store and add VPP-CE to personal assets.</li> </ul> <p>Vehicle Physics Pro Asset Store link</p>"},{"location":"DeveloperGuide/EditorSetup/VPPCommunityEdition/#add-vpp-ce-to-the-unity-editor","title":"Add VPP-CE to the Unity Editor","text":"<ol> <li>Open up the Unity Editor.</li> <li>Go to the <code>Window</code> menu and select <code>Package Manager</code>.</li> <li>Make sure the <code>My Assets</code> tab is selected from the top left of the Package Manager window.</li> <li>Find &amp; select the VPP-CE from the list and click <code>Download</code> or <code>Import</code> from the bottom left of the Package Manager    window.</li> <li>There will be a popup window showing contents of the package.    - Select <code>Sdk</code> folder to import. Rest of the content is optional.    - Click <code>Import</code> to add VPP-CE to the project.</li> </ol>"},{"location":"DeveloperGuide/EditorSetup/VPPCommunityEdition/#useful-links","title":"Useful links:","text":"<p>Unity Package manager:</p> <ul> <li>Manual</li> </ul> <p>VPP Website:</p> <ul> <li>Vehicle Physics</li> </ul>"},{"location":"DeveloperGuide/EditorSetup/VSCode/","title":"VSCode Configuration","text":""},{"location":"DeveloperGuide/EditorSetup/VSCode/#visual-studio-code-setup-with-unity","title":"Visual Studio Code setup with Unity","text":""},{"location":"DeveloperGuide/EditorSetup/VSCode/#install-visual-studio-code","title":"Install Visual Studio Code","text":"<p>Follow the steps in: - https://code.visualstudio.com/docs/setup/linux</p> <pre><code># Install the keys and repository\nsudo apt-get install wget gpg\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg\nsudo sh -c 'echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\nrm -f packages.microsoft.gpg\n\n# Then update the package cache and install the package using:\nsudo apt install apt-transport-https\nsudo apt update\nsudo apt install code\n</code></pre>"},{"location":"DeveloperGuide/EditorSetup/VSCode/#install-the-dotnet-sdk","title":"Install the Dotnet SDK","text":"<p>Follow the steps in: - https://learn.microsoft.com/en-us/dotnet/core/install/linux-ubuntu#register-the-microsoft-package-repository</p> <pre><code># Get Ubuntu version\ndeclare repo_version=$(if command -v lsb_release &amp;&gt; /dev/null; then lsb_release -r -s; else grep -oP '(?&lt;=^VERSION_ID=).+' /etc/os-release | tr -d '\"'; fi)\n\n# Download Microsoft signing key and repository\nwget https://packages.microsoft.com/config/ubuntu/$repo_version/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\n\n# Install Microsoft signing key and repository\nsudo dpkg -i packages-microsoft-prod.deb\n\n# Clean up\nrm packages-microsoft-prod.deb\n\n# Update packages\nsudo apt update\n\nsudo apt install dotnet-sdk-8.0\n</code></pre>"},{"location":"DeveloperGuide/EditorSetup/VSCode/#install-the-extensions","title":"Install the extensions","text":"<p>Follow the steps in: - https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit - https://marketplace.visualstudio.com/items?itemName=VisualStudioToolsForUnity.vstuc - https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp</p> <p>Launch VS Code Quick Open (Ctrl+P), paste the following command, and press enter. - <code>ext install ms-dotnettools.csdevkit</code> Repeat for: - <code>ext install VisualStudioToolsForUnity.vstuc</code> - <code>ext install ms-dotnettools.csharp</code></p>"},{"location":"DeveloperGuide/EditorSetup/VSCode/#configure-the-unity","title":"Configure the Unity","text":"<ul> <li>Open up the Unity Editor</li> <li><code>Edit</code> -&gt; <code>Preferences</code> -&gt; <code>External Tools</code> -&gt; <code>External Script Editor</code></li> <li>Select <code>Visual Studio Code</code></li> <li>If it's not there, click <code>Browse</code> and navigate and select <code>/usr/bin/code</code></li> </ul> <p>It should all be configured now. You can either open up a script by double clicking in the Project window in Unity or by opening up the project in VS Code: - <code>Assets</code> -&gt; <code>Open C# Project</code></p> <p>Syntax highlighting and CTRL-click navigation should work out of the box.</p> <p>For more advanced features such as debugging, check the Unity Development with VS Code Documentation.</p>"},{"location":"DeveloperGuide/EditorSetup/VSCode/#additional-notes","title":"Additional notes","text":"<p>In the AWSIM project, the package Visual Studio Editor is already installed to satisfy the requirement from the Unity for Visual Studio Code extension.</p>"},{"location":"DeveloperGuide/HowToContribute/","title":"How to Contribute","text":""},{"location":"DeveloperGuide/HowToContribute/#how-to-contribute","title":"How to Contribute","text":"<p>Everyone is welcome!</p>"},{"location":"DeveloperGuide/HowToContribute/#how-can-i-get-help","title":"How can I get help?","text":"<p>Do not open issues for general support questions as we want to keep GitHub issues for confirmed bug reports. Instead, open a discussion in the Q&amp;A category. The trouble shooting page at AWSIM and at Autoware will be also helpful.</p>"},{"location":"DeveloperGuide/HowToContribute/#issue","title":"Issue","text":"<p>Before you post an issue, please search Issues and Discussions Q&amp;A catecory to check if it is not a known issue.</p> <p>This page is helpful how to create an issue from a repository.</p>"},{"location":"DeveloperGuide/HowToContribute/#bug-report","title":"Bug report","text":"<p>If you find a new bug, please create an issue here</p>"},{"location":"DeveloperGuide/HowToContribute/#feature-request","title":"Feature request","text":"<p>If you propose a new feature or have an idea, please create an issue here</p>"},{"location":"DeveloperGuide/HowToContribute/#task","title":"Task","text":"<p>If you have plan to contribute AWSIM Labs, please create an issue here.</p>"},{"location":"DeveloperGuide/HowToContribute/#question","title":"Question","text":"<ul> <li>If you need a general support, please open a discussion in the Q&amp;A category.</li> <li>You can contribute by Answering questions and supporting other contributors</li> </ul>"},{"location":"DeveloperGuide/HowToContribute/#pull-requests","title":"Pull requests","text":"<p>If you have an idea to improve the simulation, you can submit a pull request. The following process should be followed:</p> <ol> <li>Create a derived branch (<code>feature/***</code>) from the <code>main</code> branch.</li> <li>Create a pull request to the <code>main</code> branch.</li> </ol> <p>Please keep the following in mind, while developing new features:</p> <ul> <li>Git branching guidelines</li> <li>C# coding convention.</li> </ul>"},{"location":"DeveloperGuide/License/","title":"License","text":""},{"location":"DeveloperGuide/License/#awsim-licenses","title":"AWSIM Licenses","text":"<p>AWSIM License applies to tier4/AWSIM repositories and all content contained in the Releases.</p> <ul> <li>AWSIM specific code is distributed under Apache2.0 License. The following extensions are included (<code>*.cs</code> <code>*.compute</code> <code>*.xml</code>)</li> <li>AWSIM specific assets are distributed under CC BY-NC License. The following extensions are included (<code>*.fbx</code> <code>*.pcd</code> <code>*.osm</code> <code>*.png</code> <code>*.anim</code> <code>*.unitypackage</code> <code>*.x86_64</code>)</li> </ul>"},{"location":"DeveloperGuide/License/#apache20-license","title":"Apache2.0 License","text":"<pre><code>**********************************************************************************\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2022 TIER IV, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"DeveloperGuide/License/#cc-by-nc-license","title":"CC BY-NC License","text":"<pre><code>**********************************************************************************\n\nAttribution-NonCommercial 4.0 International\n\n=======================================================================\n\nCreative Commons Corporation (\"Creative Commons\") is not a law firm and\ndoes not provide legal services or legal advice. Distribution of\nCreative Commons public licenses does not create a lawyer-client or\nother relationship. Creative Commons makes its licenses and related\ninformation available on an \"as-is\" basis. Creative Commons gives no\nwarranties regarding its licenses, any material licensed under their\nterms and conditions, or any related information. Creative Commons\ndisclaims all liability for damages resulting from their use to the\nfullest extent possible.\n\nUsing Creative Commons Public Licenses\n\nCreative Commons public licenses provide a standard set of terms and\nconditions that creators and other rights holders may use to share\noriginal works of authorship and other material subject to copyright\nand certain other rights specified in the public license below. The\nfollowing considerations are for informational purposes only, are not\nexhaustive, and do not form part of our licenses.\n\n     Considerations for licensors: Our public licenses are\n     intended for use by those authorized to give the public\n     permission to use material in ways otherwise restricted by\n     copyright and certain other rights. Our licenses are\n     irrevocable. Licensors should read and understand the terms\n     and conditions of the license they choose before applying it.\n     Licensors should also secure all rights necessary before\n     applying our licenses so that the public can reuse the\n     material as expected. Licensors should clearly mark any\n     material not subject to the license. This includes other CC-\n     licensed material, or material used under an exception or\n     limitation to copyright. More considerations for licensors:\n    wiki.creativecommons.org/Considerations_for_licensors\n\n     Considerations for the public: By using one of our public\n     licenses, a licensor grants the public permission to use the\n     licensed material under specified terms and conditions. If\n     the licensor's permission is not necessary for any reason--for\n     example, because of any applicable exception or limitation to\n     copyright--then that use is not regulated by the license. Our\n     licenses grant only permissions under copyright and certain\n     other rights that a licensor has authority to grant. Use of\n     the licensed material may still be restricted for other\n     reasons, including because others have copyright or other\n     rights in the material. A licensor may make special requests,\n     such as asking that all changes be marked or described.\n     Although not required by our licenses, you are encouraged to\n     respect those requests where reasonable. More considerations\n     for the public:\n    wiki.creativecommons.org/Considerations_for_licensees\n\n=======================================================================\n\nCreative Commons Attribution-NonCommercial 4.0 International Public\nLicense\n\nBy exercising the Licensed Rights (defined below), You accept and agree\nto be bound by the terms and conditions of this Creative Commons\nAttribution-NonCommercial 4.0 International Public License (\"Public\nLicense\"). To the extent this Public License may be interpreted as a\ncontract, You are granted the Licensed Rights in consideration of Your\nacceptance of these terms and conditions, and the Licensor grants You\nsuch rights in consideration of benefits the Licensor receives from\nmaking the Licensed Material available under these terms and\nconditions.\n\n\nSection 1 -- Definitions.\n\n  a. Adapted Material means material subject to Copyright and Similar\n     Rights that is derived from or based upon the Licensed Material\n     and in which the Licensed Material is translated, altered,\n     arranged, transformed, or otherwise modified in a manner requiring\n     permission under the Copyright and Similar Rights held by the\n     Licensor. For purposes of this Public License, where the Licensed\n     Material is a musical work, performance, or sound recording,\n     Adapted Material is always produced where the Licensed Material is\n     synched in timed relation with a moving image.\n\n  b. Adapter's License means the license You apply to Your Copyright\n     and Similar Rights in Your contributions to Adapted Material in\n     accordance with the terms and conditions of this Public License.\n\n  c. Copyright and Similar Rights means copyright and/or similar rights\n     closely related to copyright including, without limitation,\n     performance, broadcast, sound recording, and Sui Generis Database\n     Rights, without regard to how the rights are labeled or\n     categorized. For purposes of this Public License, the rights\n     specified in Section 2(b)(1)-(2) are not Copyright and Similar\n     Rights.\n  d. Effective Technological Measures means those measures that, in the\n     absence of proper authority, may not be circumvented under laws\n     fulfilling obligations under Article 11 of the WIPO Copyright\n     Treaty adopted on December 20, 1996, and/or similar international\n     agreements.\n\n  e. Exceptions and Limitations means fair use, fair dealing, and/or\n     any other exception or limitation to Copyright and Similar Rights\n     that applies to Your use of the Licensed Material.\n\n  f. Licensed Material means the artistic or literary work, database,\n     or other material to which the Licensor applied this Public\n     License.\n\n  g. Licensed Rights means the rights granted to You subject to the\n     terms and conditions of this Public License, which are limited to\n     all Copyright and Similar Rights that apply to Your use of the\n     Licensed Material and that the Licensor has authority to license.\n\n  h. Licensor means the individual(s) or entity(ies) granting rights\n     under this Public License.\n\n  i. NonCommercial means not primarily intended for or directed towards\n     commercial advantage or monetary compensation. For purposes of\n     this Public License, the exchange of the Licensed Material for\n     other material subject to Copyright and Similar Rights by digital\n     file-sharing or similar means is NonCommercial provided there is\n     no payment of monetary compensation in connection with the\n     exchange.\n\n  j. Share means to provide material to the public by any means or\n     process that requires permission under the Licensed Rights, such\n     as reproduction, public display, public performance, distribution,\n     dissemination, communication, or importation, and to make material\n     available to the public including in ways that members of the\n     public may access the material from a place and at a time\n     individually chosen by them.\n\n  k. Sui Generis Database Rights means rights other than copyright\n     resulting from Directive 96/9/EC of the European Parliament and of\n     the Council of 11 March 1996 on the legal protection of databases,\n     as amended and/or succeeded, as well as other essentially\n     equivalent rights anywhere in the world.\n\n  l. You means the individual or entity exercising the Licensed Rights\n     under this Public License. Your has a corresponding meaning.\n\n\nSection 2 -- Scope.\n\n  a. License grant.\n\n       1. Subject to the terms and conditions of this Public License,\n          the Licensor hereby grants You a worldwide, royalty-free,\n          non-sublicensable, non-exclusive, irrevocable license to\n          exercise the Licensed Rights in the Licensed Material to:\n\n            a. reproduce and Share the Licensed Material, in whole or\n               in part, for NonCommercial purposes only; and\n\n            b. produce, reproduce, and Share Adapted Material for\n               NonCommercial purposes only.\n\n       2. Exceptions and Limitations. For the avoidance of doubt, where\n          Exceptions and Limitations apply to Your use, this Public\n          License does not apply, and You do not need to comply with\n          its terms and conditions.\n\n       3. Term. The term of this Public License is specified in Section\n          6(a).\n\n       4. Media and formats; technical modifications allowed. The\n          Licensor authorizes You to exercise the Licensed Rights in\n          all media and formats whether now known or hereafter created,\n          and to make technical modifications necessary to do so. The\n          Licensor waives and/or agrees not to assert any right or\n          authority to forbid You from making technical modifications\n          necessary to exercise the Licensed Rights, including\n          technical modifications necessary to circumvent Effective\n          Technological Measures. For purposes of this Public License,\n          simply making modifications authorized by this Section 2(a)\n          (4) never produces Adapted Material.\n\n       5. Downstream recipients.\n\n            a. Offer from the Licensor -- Licensed Material. Every\n               recipient of the Licensed Material automatically\n               receives an offer from the Licensor to exercise the\n               Licensed Rights under the terms and conditions of this\n               Public License.\n\n            b. No downstream restrictions. You may not offer or impose\n               any additional or different terms or conditions on, or\n               apply any Effective Technological Measures to, the\n               Licensed Material if doing so restricts exercise of the\n               Licensed Rights by any recipient of the Licensed\n               Material.\n\n       6. No endorsement. Nothing in this Public License constitutes or\n          may be construed as permission to assert or imply that You\n          are, or that Your use of the Licensed Material is, connected\n          with, or sponsored, endorsed, or granted official status by,\n          the Licensor or others designated to receive attribution as\n          provided in Section 3(a)(1)(A)(i).\n\n  b. Other rights.\n\n       1. Moral rights, such as the right of integrity, are not\n          licensed under this Public License, nor are publicity,\n          privacy, and/or other similar personality rights; however, to\n          the extent possible, the Licensor waives and/or agrees not to\n          assert any such rights held by the Licensor to the limited\n          extent necessary to allow You to exercise the Licensed\n          Rights, but not otherwise.\n\n       2. Patent and trademark rights are not licensed under this\n          Public License.\n\n       3. To the extent possible, the Licensor waives any right to\n          collect royalties from You for the exercise of the Licensed\n          Rights, whether directly or through a collecting society\n          under any voluntary or waivable statutory or compulsory\n          licensing scheme. In all other cases the Licensor expressly\n          reserves any right to collect such royalties, including when\n          the Licensed Material is used other than for NonCommercial\n          purposes.\n\n\nSection 3 -- License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the\nfollowing conditions.\n\n  a. Attribution.\n\n       1. If You Share the Licensed Material (including in modified\n          form), You must:\n\n            a. retain the following if it is supplied by the Licensor\n               with the Licensed Material:\n\n                 i. identification of the creator(s) of the Licensed\n                    Material and any others designated to receive\n                    attribution, in any reasonable manner requested by\n                    the Licensor (including by pseudonym if\n                    designated);\n\n                ii. a copyright notice;\n\n               iii. a notice that refers to this Public License;\n\n                iv. a notice that refers to the disclaimer of\n                    warranties;\n\n                 v. a URI or hyperlink to the Licensed Material to the\n                    extent reasonably practicable;\n\n            b. indicate if You modified the Licensed Material and\n               retain an indication of any previous modifications; and\n\n            c. indicate the Licensed Material is licensed under this\n               Public License, and include the text of, or the URI or\n               hyperlink to, this Public License.\n\n       2. You may satisfy the conditions in Section 3(a)(1) in any\n          reasonable manner based on the medium, means, and context in\n          which You Share the Licensed Material. For example, it may be\n          reasonable to satisfy the conditions by providing a URI or\n          hyperlink to a resource that includes the required\n          information.\n\n       3. If requested by the Licensor, You must remove any of the\n          information required by Section 3(a)(1)(A) to the extent\n          reasonably practicable.\n\n       4. If You Share Adapted Material You produce, the Adapter's\n          License You apply must not prevent recipients of the Adapted\n          Material from complying with this Public License.\n\n\nSection 4 -- Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that\napply to Your use of the Licensed Material:\n\n  a. for the avoidance of doubt, Section 2(a)(1) grants You the right\n     to extract, reuse, reproduce, and Share all or a substantial\n     portion of the contents of the database for NonCommercial purposes\n     only;\n\n  b. if You include all or a substantial portion of the database\n     contents in a database in which You have Sui Generis Database\n     Rights, then the database in which You have Sui Generis Database\n     Rights (but not its individual contents) is Adapted Material; and\n\n  c. You must comply with the conditions in Section 3(a) if You Share\n     all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not\nreplace Your obligations under this Public License where the Licensed\nRights include other Copyright and Similar Rights.\n\n\nSection 5 -- Disclaimer of Warranties and Limitation of Liability.\n\n  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\n     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS\n     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF\n     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,\n     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,\n     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR\n     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,\n     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT\n     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT\n     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\n\n  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\n     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,\n     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,\n     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,\n     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR\n     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN\n     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR\n     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR\n     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\n\n  c. The disclaimer of warranties and limitation of liability provided\n     above shall be interpreted in a manner that, to the extent\n     possible, most closely approximates an absolute disclaimer and\n     waiver of all liability.\n\n\nSection 6 -- Term and Termination.\n\n  a. This Public License applies for the term of the Copyright and\n     Similar Rights licensed here. However, if You fail to comply with\n     this Public License, then Your rights under this Public License\n     terminate automatically.\n\n  b. Where Your right to use the Licensed Material has terminated under\n     Section 6(a), it reinstates:\n\n       1. automatically as of the date the violation is cured, provided\n          it is cured within 30 days of Your discovery of the\n          violation; or\n\n       2. upon express reinstatement by the Licensor.\n\n     For the avoidance of doubt, this Section 6(b) does not affect any\n     right the Licensor may have to seek remedies for Your violations\n     of this Public License.\n\n  c. For the avoidance of doubt, the Licensor may also offer the\n     Licensed Material under separate terms or conditions or stop\n     distributing the Licensed Material at any time; however, doing so\n     will not terminate this Public License.\n\n  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public\n     License.\n\n\nSection 7 -- Other Terms and Conditions.\n\n  a. The Licensor shall not be bound by any additional or different\n     terms or conditions communicated by You unless expressly agreed.\n\n  b. Any arrangements, understandings, or agreements regarding the\n     Licensed Material not stated herein are separate from and\n     independent of the terms and conditions of this Public License.\n\n\nSection 8 -- Interpretation.\n\n  a. For the avoidance of doubt, this Public License does not, and\n     shall not be interpreted to, reduce, limit, restrict, or impose\n     conditions on any use of the Licensed Material that could lawfully\n     be made without permission under this Public License.\n\n  b. To the extent possible, if any provision of this Public License is\n     deemed unenforceable, it shall be automatically reformed to the\n     minimum extent necessary to make it enforceable. If the provision\n     cannot be reformed, it shall be severed from this Public License\n     without affecting the enforceability of the remaining terms and\n     conditions.\n\n  c. No term or condition of this Public License will be waived and no\n     failure to comply consented to unless expressly agreed to by the\n     Licensor.\n\n  d. Nothing in this Public License constitutes or may be interpreted\n     as a limitation upon, or waiver of, any privileges and immunities\n     that apply to the Licensor or You, including from the legal\n     processes of any jurisdiction or authority.\n\n=======================================================================\n\nCreative Commons is not a party to its public\nlicenses. Notwithstanding, Creative Commons may elect to apply one of\nits public licenses to material it publishes and in those instances\nwill be considered the \u201cLicensor.\u201d The text of the Creative Commons\npublic licenses is dedicated to the public domain under the CC0 Public\nDomain Dedication. Except for the limited purpose of indicating that\nmaterial is shared under a Creative Commons public license or as\notherwise permitted by the Creative Commons policies published at\ncreativecommons.org/policies, Creative Commons does not authorize the\nuse of the trademark \"Creative Commons\" or any other trademark or logo\nof Creative Commons without its prior written consent including,\nwithout limitation, in connection with any unauthorized modifications\nto any of its public licenses or any other arrangements,\nunderstandings, or agreements concerning use of licensed material. For\nthe avoidance of doubt, this paragraph does not form part of the\npublic licenses.\n\nCreative Commons may be contacted at creativecommons.org\n</code></pre>"},{"location":"DeveloperGuide/TroubleShooting/","title":"Trouble shooting","text":""},{"location":"DeveloperGuide/TroubleShooting/#trouble-shooting","title":"Trouble shooting","text":"<p>This document describes the most common errors encountered when working with AWSIm or autoware.</p> Trouble Solution Massive output of Plugins errors <code>git clone</code> the AWSIM repository again error : <code>RuntimeError: error not set, at C:\\ci\\ws\\src\\ros2\\rcl\\rcl\\src\\rcl\\node.c:262</code> Set up environment variables and config around ROS2 correctly. For example: - Environment variables - <code>cyclonedds_config.xml</code> <code>$ ros2 topic list</code> is not displayed - your machine <code>ROS_DOMAIN_ID</code> is different- <code>ROS2</code> is not sourced Using AWSIM on Windows and Autoware on Ubuntu.  <code>$ ros2 topic list</code> is not displayed. Allow the communication in Windows Firewall self-driving stops in the middle of the road. Check if your map data is correct (PointCloud, VectorMap, 3D fbx models) Connecting AWSIM and Autoware results in bad network Make ros local host-only. Include the following in the .bashrc (The password will be requested at terminal startup after OS startup.)  <code>export ROS_LOCALHOST_ONLY=1</code><code>export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp</code><code>if [ ! -e /tmp/cycloneDDS_configured ]; then</code><code>sudo sysctl -w net.core.rmem_max=2147483647</code><code>sudo ip link set lo multicast on</code><code>touch /tmp/cycloneDDS_configured</code><code>fi</code> Lidar (colored pointcloud on RViz ) does not show. Reduce processing load by following command. This can only be applied to Autoware's <code>awsim-stable</code> branch.  <code>cd &lt;path_to_your_autoware_folder&gt;</code><code>wget \"https://drive.google.com/uc?export=download&amp;id=11mkwfg-OaXIp3Z5c3R58Pob3butKwE1Z\" -O patch.sh</code><code>bash patch.sh &amp;&amp; rm patch.sh</code> Error when starting AWSIM binary. <code>segmentation fault (core dumped)</code> - Check if yourNvidia drivers or Vulkan API are installed correctly - When building binary please pay attantion whether the <code>Graphic Jobs</code> option in <code>Player Settings</code> is disabled. It should be disabled since it may produce segmentation fault errors. Please check forum for more details. Initial pose does not match automatically. Set initial pose manually.  Unity crashes and check the log for the cause of the error. Editor log file locationWindows : <code>C:\\Users\\username\\AppData\\Local\\Unity\\Editor\\Editor.log</code>Linux : <code>~/.config/unity3d/.Editor.log</code> Player log file location Windows : <code>C:\\Users\\username\\AppData\\LocalLow\\CompanyName\\ProductName\\output_log.txt</code>Linux :<code>~/.config/unity3d/CompanyName/ProductName/Player.log</code>See also : Unity Documentation - Log Files Safe mode dialog appears when starting UnityEditor.  or  error : <code>No usable version of libssl was found</code> 1. download libssl  <code>$ wget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb</code>  2. install  <code>sudo dpkg -i libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb</code> (Windows) Unity Editor's error:<code>Plugins: Failed to load 'Assets/RGLUnityPlugin/Plugins/Windows/x86_64/RobotecGPULidar.dll' because one or more of its dependencies could not be loaded.</code> Install Microsoft Visual C++ Redistributable packages for Visual Studio 2015, 2017, 2019, and 2022 (X64 Architecture) (Windows) Built-binary or Unity Editor freeze when simulation started Update/Install latest NIC(NetworkInterfaceCard) drivers for your PC.Especially, if you can find latest drivers provided by chip vendors for the interfaces (not by Microsoft), we recommend vendors' drivers."},{"location":"GettingStarted/QuickStartDemo/","title":"Quick Start Demo","text":""},{"location":"GettingStarted/QuickStartDemo/#quick-start-demo","title":"Quick Start Demo","text":"<p>Below you can find instructions on how to setup the self-driving demo of AWSIM simulation controlled by Autoware. The instruction assumes using the Ubuntu OS.</p>"},{"location":"GettingStarted/QuickStartDemo/#demo-configuration","title":"Demo configuration","text":"<p>The simulation provided in the AWSIM demo is configured as follows:</p> AWSIM Demo Settings Vehicle Lexus RX 450h Environment Japan Tokyo Nishishinjuku Sensors GNSSIMU3 x VLP16Traffic Light Camera Traffic Randomized traffic ROS2 humble"},{"location":"GettingStarted/QuickStartDemo/#pc-specs","title":"PC specs","text":"<p>Please make sure that your machine meets the following requirements in order to run the simulation correctly:</p> Required PC Specs OS Ubuntu 22.04 CPU 6c12t or higher GPU RTX 2080 or higher Nvidia Driver (Ubuntu 22) &gt;=545"},{"location":"GettingStarted/QuickStartDemo/#dds-configuration","title":"DDS configuration","text":"<p>In order to run AWSIM Labs with the best performance and without hogging the network, please follow the steps below.</p> <p>Add the following lines to <code>~/.bashrc</code> file:</p> <pre><code>if [ ! -e /tmp/cycloneDDS_configured ]; then\n    sudo sysctl -w net.core.rmem_max=2147483647\n    sudo sysctl -w net.ipv4.ipfrag_time=3\n    sudo sysctl -w net.ipv4.ipfrag_high_thresh=134217728     # (128 MB)\n    sudo ip link set lo multicast on\n    touch /tmp/cycloneDDS_configured\nfi\n</code></pre> <p>Every time you restart this machine, and open a new terminal, the above commands will be executed.</p> <p>Until you restart the machine, they will not be executed again.</p>"},{"location":"GettingStarted/QuickStartDemo/#cyclonedds-configuration","title":"CycloneDDS configuration","text":"<p>Save the following as <code>cyclonedds.xml</code> in your home directory <code>~</code>: </p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;CycloneDDS xmlns=\"https://cdds.io/config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"https://cdds.io/config https://raw.githubusercontent.com/eclipse-cyclonedds/cyclonedds/master/etc/cyclonedds.xsd\"&gt;\n    &lt;Domain Id=\"any\"&gt;\n        &lt;General&gt;\n            &lt;Interfaces&gt;\n                &lt;NetworkInterface name=\"lo\" priority=\"default\" multicast=\"default\" /&gt;\n            &lt;/Interfaces&gt;\n            &lt;AllowMulticast&gt;default&lt;/AllowMulticast&gt;\n            &lt;MaxMessageSize&gt;65500B&lt;/MaxMessageSize&gt;\n        &lt;/General&gt;\n        &lt;Internal&gt;\n            &lt;SocketReceiveBufferSize min=\"10MB\"/&gt;\n            &lt;Watermarks&gt;\n                &lt;WhcHigh&gt;500kB&lt;/WhcHigh&gt;\n            &lt;/Watermarks&gt;\n        &lt;/Internal&gt;\n    &lt;/Domain&gt;\n&lt;/CycloneDDS&gt;\n</code></pre> <p>Make sure the following lines are added to the <code>~/.bashrc</code> file:</p> <pre><code>export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp\nexport CYCLONEDDS_URI=/home/your_username/cyclonedds.xml\n</code></pre> <p>Replace <code>your_username</code> with your actual username.</p> <p>Note</p> <p>You should use the absolute path to the <code>cyclonedds.xml</code> file.</p> <p>Warning</p> <p>A system restart is required for these changes to work.</p> <p>Warning</p> <p>DO NOT set <code>export ROS_LOCALHOST_ONLY=1</code>. CycloneDDS configuration will be enough.  </p>"},{"location":"GettingStarted/QuickStartDemo/#start-the-demo","title":"Start the demo","text":""},{"location":"GettingStarted/QuickStartDemo/#running-the-awsim-demo","title":"Running the AWSIM demo","text":"<p>To run the simulator, please follow the steps below.</p> <ol> <li> <p>Install Nvidia GPU driver (Skip if already installed).</p> <ol> <li>Add Nvidia driver to apt repository <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\n</code></pre></li> <li>Install the recommended version of the driver. <pre><code>sudo ubuntu-drivers autoinstall\n\n# or install a specific version (following was tested)\nsudo apt install nvidia-driver-550\n</code></pre></li> <li>Reboot your machine to make the installed driver detected by the system. <pre><code>sudo reboot\n</code></pre></li> <li>Open terminal and check if <code>nvidia-smi</code> command is available and outputs summary similar to the one presented below. <pre><code>$ nvidia-smi\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3080        Off |   00000000:2D:00.0  On |                  N/A |\n| 30%   40C    P8             35W /  320W |    5299MiB /  10240MiB |      7%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n...\n</code></pre></li> </ol> </li> <li> <p>Install Vulkan Graphics Library (Skip if already installed).</p> <ol> <li>Update the environment. <pre><code>sudo apt update\n</code></pre></li> <li>Install the library. <pre><code>sudo apt install libvulkan1\n</code></pre></li> </ol> </li> <li> <p>Download and Run AWSIM Demo binary.</p> <ol> <li> <p>Download the latest release from:</p> <p>AWSIM Labs GitHub Releases Page</p> </li> <li> <p>Unzip the downloaded file.</p> </li> <li> <p>Make the file executable.</p> <p>Right click the <code>awsim_labs.x86_64</code> file and check the <code>Execute</code> checkbox</p> <p></p> <p>or execute the command below.</p> <pre><code>chmod +x &lt;path to AWSIM folder&gt;/awsim_labs.x86_64\n</code></pre> </li> <li> <p>Launch <code>awsim_labs.x86_64</code>.     </p><pre><code>./&lt;path to AWSIM folder&gt;/awsim_labs.x86_64\n</code></pre> <p>It may take some time for the application to start the so please wait until image similar to the one presented below is visible in your application window.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"GettingStarted/QuickStartDemo/#launching-autoware","title":"Launching Autoware","text":"<p>In order to configure and run the Autoware software with the AWSIM demo, please:</p> <ol> <li> <p>Download <code>map files (pcd, osm)</code> and unzip them.     Download Map files (pcd, osm)</p> </li> <li> <p>Clone Autoware and move to the directory. </p><pre><code>git clone https://github.com/autowarefoundation/autoware.git\ncd autoware\n</code></pre> </li> <li>Switch branch to <code>main</code>. <pre><code>git checkout main\n</code></pre></li> <li>Configure the environment. (Skip if Autoware environment has been configured before) <pre><code>./setup-dev-env.sh\n</code></pre></li> <li>Create the <code>src</code> directory and clone external dependent repositories into it. <pre><code>mkdir src\nvcs import src &lt; autoware.repos\n</code></pre></li> <li>Install dependent ROS packages. <pre><code>source /opt/ros/humble/setup.bash\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n</code></pre></li> <li>Build the workspace. <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_EXPORT_COMPILE_COMMANDS=1\n</code></pre></li> <li>Launch Autoware. <pre><code>source install/setup.bash\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_labs_sensor_kit map_path:=&lt;absolute path of map folder&gt;\n\n# Use the absolute path for the map folder, don't use the ~ operator.\n\n# Example:\nros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_labs_sensor_kit map_path:=/home/your_username/autoware_map/nishishinjuku_autoware_map\n</code></pre> </li> </ol>"},{"location":"GettingStarted/QuickStartDemo/#lets-run-the-self-driving-simulation","title":"Let's run the self driving simulation","text":"<ol> <li> <p>Launch AWSIM and Autoware according to the steps described earlier in this document. </p> </li> <li> <p>The Autoware will automatically set its pose estimation as presented below. </p> </li> <li> <p>Set the navigation goal for the vehicle.  </p> </li> <li> <p>Optionally, you can define an intermediate point through which the vehicle will travel on its way to the destination.  The generated path can be seen on the image below. </p> </li> <li> <p>Enable self-driving.</p> </li> </ol> <p>To make the vehicle start navigating please engage its operation using the command below.</p> <pre><code>cd autoware\nsource install/setup.bash\nros2 topic pub /autoware/engage autoware_vehicle_msgs/msg/Engage '{engage: True}' -1\n</code></pre> <p></p> <p>The self-driving simulation demo has been successfully launched!</p>"},{"location":"GettingStarted/QuickStartDemo/#troubleshooting","title":"Troubleshooting","text":"<p>In case of any problems with running the sample AWSIM binary with Autoware, start with checking our Troubleshooting page with the most common problems.</p>"},{"location":"GettingStarted/QuickStartDemo/#appendix","title":"Appendix","text":"<ul> <li>AWSIM ROS2 topic list</li> </ul>"},{"location":"GettingStarted/SetupUnityProject/","title":"Setup Unity Project","text":""},{"location":"GettingStarted/SetupUnityProject/#setup-unity-project","title":"Setup Unity Project","text":"<p>Info</p> <p>It is advised to checkout the Quick Start Demo tutorial before reading this section.</p> <p>This page is a tutorial for setting up a AWSIM Unity project.</p>"},{"location":"GettingStarted/SetupUnityProject/#environment-preparation","title":"Environment preparation","text":""},{"location":"GettingStarted/SetupUnityProject/#system-setup","title":"System setup","text":"Ubuntu 22Windows <ol> <li>Make sure your machine meets the required hardware specifications.<ul> <li>NOTE: PC requirements may vary depending on simulation contents which may change as the simulator develops</li> </ul> </li> <li>Prepare a desktop PC with Ubuntu 22.04 installed.</li> <li>Install Nvidia drivers and Vulkan Graphics API.</li> <li>Install git.</li> <li>Follow the DDS configuration guide.</li> </ol> <ol> <li>Make sure your machine meets the required hardware specifications.<ul> <li>NOTE: PC requirements may vary depending on simulation contents which may change as the simulator develops</li> </ul> </li> <li>Prepare a desktop PC with Windows 10 or 11 (64 bit) installed.</li> <li>Install git.</li> <li>Install Microsoft Visual C++ Redistributable packages for Visual Studio 2015, 2017, 2019, and 2022 (X64 Architecture)</li> </ol>"},{"location":"GettingStarted/SetupUnityProject/#ros-2","title":"ROS 2","text":"<p>AWSIM comes with a standalone flavor of <code>Ros2ForUnity</code>. This means that, to avoid internal conflicts between different ROS 2 versions, you shouldn't run the Editor or AWSIM binary with ROS 2 sourced.</p> <p>Warning</p> <p>Do not run the AWSIM, Unity Hub, or the Editor with ROS 2 sourced.</p> Ubuntu 22Windows <ul> <li>Make sure that the terminal which you are using to run Unity Hub, Editor, or AWSIM doesn't have ROS 2 sourced.</li> <li>It is common to have ROS 2 sourced automatically with <code>~/.bashrc</code> or <code>~/.profile</code>. Make sure it is not obscuring your working environment:<ul> <li>Running Unity Hub from the Ubuntu GUI menu takes the environment configuration from <code>~/.profile</code>.</li> <li>Running Unity Hub from the terminal uses the current terminal configuration from <code>~/.profile</code> and <code>~/.bashrc</code>.</li> <li>Running Unity Editor from the UnityHub inherits the environment setup from the Unity Hub.</li> </ul> </li> </ul> <ul> <li>Make sure your Windows environment variables are ROS 2 free.</li> </ul>"},{"location":"GettingStarted/SetupUnityProject/#unity-installation","title":"Unity installation","text":"<p>Info</p> <p>AWSIM's Unity version is currently 2022.3.36f1</p> <p>Follow the steps below to install Unity on your machine:</p>"},{"location":"GettingStarted/SetupUnityProject/#install-unityhub","title":"Install UnityHub:","text":"<ol> <li>Install UnityHub to manage Unity projects. Please go to Unity download page and download latest <code>UnityHub.AppImage</code>.</li> <li>Install Unity 2022.3.36f1 via UnityHub:<ul> <li>Open new terminal, navigate to directory where <code>UnityHub.AppImage</code> is download and execute the following command: <pre><code>./UnityHub.AppImage\n</code></pre></li> </ul> </li> </ol>"},{"location":"GettingStarted/SetupUnityProject/#install-unity-editor","title":"Install Unity Editor:","text":"<ol> <li>Click <code>Installs</code> from the left menu and then click the <code>Install Editor</code> button from the top right.</li> <li>From the new window click the <code>Official releases</code> tab and check for the current version of Unity.    - If you can't find the version, click to <code>Archive</code> and search for the version in the Unity archives page. It will direct you to Unity Archive.</li> <li> <p>After finding the version, click the download/install button to start the installation process.    - At this point, your Unity installation process should have started.</p> <p>=== \"Ubuntu 22\"    - *NOTE: If the installation process has not started after clicking the download/install button, please copy the hyperlink (by right clicking the button and selecting <code>Copy link address</code>) and add it as an argument for Unity Hub app. An example command:    </p><pre><code>./UnityHub.AppImage unityhub://2022.3.21f1/d91830b65d9b\n</code></pre> </li> <li> <p>After successful installation the version will be available in the <code>Installs</code> tab in Unity Hub.</p> </li> </ol>"},{"location":"GettingStarted/SetupUnityProject/#open-awsim-project","title":"Open AWSIM project","text":"<p>To open the Unity AWSIM project in Unity Editor:</p> Using Unity HubUsing Terminal <ol> <li> <p>Make sure you have the AWSIM repository cloned and ROS 2 is not sourced.     </p><pre><code>git clone git@github.com:autowarefoundation/AWSIM.git\n</code></pre> </li> <li> <p>Launch UnityHub.     </p><pre><code>./UnityHub.AppImage\n</code></pre> <p>Info</p> <p>If you are launching the Unity Hub from the Ubuntu applications menu (without the terminal), make sure that system optimizations are set. To be sure, run the terminal at least once before running the Unity Hub. This will apply the OS settings.</p> </li> <li> <p>Open the project in UnityHub</p> <ul> <li>Click the <code>Open</code> button from the <code>Projects</code> tab,</li> <li>Navigate the directory where the AWSIM repository was cloned to and select the directory,</li> <li>The project should be added to <code>Projects</code> tab in Unity Hub. To launch the project in Unity Editor simply click the <code>AWSIM</code> project.</li> <li>The project is now ready to use.</li> <li>The project can be opened by double clicking on the project name or selecting it from the list then clicking the <code>Open</code> button on the top right.</li> </ul> </li> </ol> <ol> <li> <p>Enter the AWSIM directory (make sure ROS 2 is not sourced).     </p><pre><code>cd AWSIM\n</code></pre> </li> <li> <p>If your Unity Editor is in default location, run the project using the editor command.     </p><pre><code>~/Unity/Hub/Editor/[your_editor_version]/Editor/Unity -projectPath .\n</code></pre> <p>Info</p> <p>Please specify your Unity Editor version in the path. If your Unity Editor is installed in different location, please adjust the path accordingly.</p> </li> </ol> <p>Warning</p> <p>If you get the safe mode dialog when starting UnityEditor, you may need to install openssl.</p> <ol> <li>download libssl <code>$ wget http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.13_amd64.deb</code></li> <li>install <code>sudo dpkg -i libssl1.0.0_1.0.2n-1ubuntu5.13_amd64.deb</code></li> </ol>"},{"location":"GettingStarted/SetupUnityProject/#import-external-packages","title":"Import external packages","text":"<p>To properly run and use AWSIM project in Unity it is required to download map package which is not included in the repository.</p> <ol> <li> <p>Download and import <code>Nishishinjuku_URP_v0.1.0.unitypackage</code></p> <p>Download Map Package</p> </li> <li> <p>In Unity Editor, from the menu bar at the top, select <code>Assets -&gt; Import Package -&gt; Custom Package...</code> and navigate the <code>Nishishinjuku_URP.unitypackage</code> file you've downloaded and open.</p> </li> <li>Click <code>Import</code> button in the popup window to import the package.</li> <li><code>Nishishinjuku</code> package should be successfully imported under <code>Assets/AWSIM/Externals/</code>directory. You can access the directory from the <code>Project</code> window in Unity Editor.</li> </ol> <p>Info</p> <p>The Externals directory is added to the <code>.gitignore</code> because the map has a large file size and should not be directly uploaded to the repository.</p>"},{"location":"GettingStarted/SetupUnityProject/#import-graphy-asset","title":"Import Graphy Asset","text":"<p>Import Graphy by following these instructions: Graphy Asset Setup</p>"},{"location":"GettingStarted/SetupUnityProject/#run-the-demo-in-editor","title":"Run the demo in Editor","text":"<p>The following steps describe how to run the demo in Unity Editor:</p> <ol> <li>Open the <code>AutowareSimulation.unity</code> scene placed under <code>Assets/AWSIM/Scenes/Main</code> directory</li> <li>Run the simulation by clicking <code>Play</code> button placed at the top section of Editor.</li> <li>Now you should see the simulation running in the Editor.</li> </ol> <p>If you encounter any issues while running the simulation, please refer to the Troubleshooting section or open an issue in the AWSIM Labs GitHub.</p>"},{"location":"GettingStarted/UsingOpenSCENARIO/","title":"Using OpenSCENARIO","text":""},{"location":"GettingStarted/UsingOpenSCENARIO/#using-openscenario","title":"Using OpenSCENARIO","text":"<p>Warning</p> <p>Running AWSIM with scenario_simulator_v2 is still a prototype, so stable running is not guaranteed.</p> <p>Below you can find instructions on how to setup the OpenSCENARIO execution using scenario_simulator_v2 with AWSIM as a simulator The instruction assumes using the Ubuntu OS.</p>"},{"location":"GettingStarted/UsingOpenSCENARIO/#prerequisites","title":"Prerequisites","text":"<p>Follow Setup Unity Project tutorial</p>"},{"location":"GettingStarted/UsingOpenSCENARIO/#build-autoware-with-scenario_simulator_v2","title":"Build Autoware with <code>scenario_simulator_v2</code>","text":"<p>In order to configure the Autoware software with the AWSIM demo, please:</p> <ol> <li>Clone RobotecAI's Autoware and move to the directory.    <pre><code>git clone git@github.com:RobotecAI/autoware-1.git\ncd autoware\n</code></pre></li> <li>Check out to the <code>awsim-ss2-stable</code> branch    <pre><code>git checkout awsim-ss2-stable\n</code></pre></li> <li>Configure the environment. (Skip if Autoware environment has been configured before)    <pre><code>./setup-dev-env.sh\n</code></pre></li> <li>Create the <code>src</code> directory and clone external dependent repositories into it.    <pre><code>mkdir src\nvcs import src &lt; autoware.repos\nvcs import src &lt; simulator.repos\n</code></pre></li> <li> <p>Download <code>shinjuku_map.zip</code> archive</p> </li> <li> <p>Unzip it to <code>src/simulator</code> directory    </p><pre><code>unzip &lt;Download directory&gt;/shinjuku_map.zip -d src/simulator\n</code></pre> </li> <li>Install dependent ROS packages.    <pre><code>source /opt/ros/humble/setup.bash\nrosdep update\nrosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO\n</code></pre></li> <li>Build the workspace.    <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=\"-w\"\n</code></pre></li> </ol>"},{"location":"GettingStarted/UsingOpenSCENARIO/#running-the-demo","title":"Running the demo","text":"<ol> <li> <p>Download <code>AWSIM_v1.2.0_ss2.zip</code> &amp; Run archive</p> </li> <li> <p>Launch <code>scenario_test_runner</code>.    </p><pre><code>source install/setup.bash\nros2 launch scenario_test_runner scenario_test_runner.launch.py                        \\\narchitecture_type:=awf/universe  record:=false                                         \\\nscenario:='$(find-pkg-share scenario_test_runner)/scenario/sample_awsim.yaml'          \\\nsensor_model:=awsim_sensor_kit  vehicle_model:=sample_vehicle                          \\\nlaunch_simple_sensor_simulator:=false autoware_launch_file:=\"e2e_simulator.launch.xml\" \\\ninitialize_duration:=260 port:=8080\n</code></pre> </li> </ol>"},{"location":"GettingStarted/UsingOpenSCENARIO/#troubleshooting","title":"Troubleshooting","text":"<p>In case of problems, make sure that the regular demo work well with the Autoware built above. Follow the troubleshooting page there if necessary.</p>"},{"location":"GettingStarted/UsingOpenSCENARIO/#appendix","title":"Appendix","text":"<ul> <li>AWSIM ROS2 topic list</li> </ul>"},{"location":"Introduction/AWSIM/","title":"AWSIM Labs","text":""},{"location":"Introduction/AWSIM/#awsim-labs","title":"AWSIM Labs","text":"<p>AWSIM Labs is a fork of TIER IV/AWSIM, an open-source simulator made with Unity for autonomous driving research and development. It is developed for self-driving software like Autoware. This simulator aims to bridge the gap between the virtual and real worlds, enabling users to train and evaluate their autonomous systems in a safe and controlled environment before deploying them on real vehicles. It provides a realistic virtual environment for training, testing, and evaluating various aspects of autonomous driving systems.</p> <p>AWSIM simulates a variety of real-world scenarios, with accurate physics and sensor models. It offers a wide range of sensors, such as: Cameras, GNSS, IMU and  LiDARs, allowing developers to simulate their  autonomous vehicle's interactions with the environment accurately. The simulator also models dynamic objects, such as pedestrians, other vehicles, and traffic lights, making it possible to study interactions and decision-making in complex traffic scenarios. This enables the testing and evaluation of perception, planning, and control algorithms under different sensor configurations and scenarios.</p> <p>AWSIM supports a flexible and modular architecture, making it easy to customize and extend its capabilities. Users can modify the current or add a new environment with their own assets and traffic rules to create custom scenarios to suit their specific research needs. This allows for the development and testing of advanced algorithms in diverse driving conditions.</p> <p>Because AWSIM was developed mainly to work with Autoware, it supports:</p> <ul> <li>Ubuntu 22.04 and Windows 10/11</li> <li>ROS2 Humble distribution</li> </ul> <p>Prerequisites</p> <p>You can read more about the prerequisites and running AWSIM here.</p> <p>Connection with Autoware</p> <p>Introduction about how the connection between AWSIM and Autoware works can be read here.</p>"},{"location":"Introduction/AWSIM/#why-was-awsim-developed","title":"Why was AWSIM developed?","text":"<p>The main objectives of AWSIM are to facilitate research and development in autonomous driving, enable benchmarking of algorithms and systems, and foster collaboration and knowledge exchange within the autonomous driving community. By providing a realistic and accessible platform, AWSIM aims to accelerate the progress and innovation in the field of autonomous driving.</p>"},{"location":"Introduction/AWSIM/#architecture","title":"Architecture","text":"<p>To describe the architecture of AWSIM, first of all, it is necessary to mention the <code>Scene</code>. It contains all the objects occurring in the simulation of a specific scenario and their configurations. The default AWSIM scene that is developed to work with Autoware is called AutowareSimulation.</p> <p>In the scene we can distinguish basics components such like <code>MainCamera</code>, <code>ClockPublisher</code>, <code>EventSystem</code> and <code>Canvas</code>. A detailed description of the scene and its components can be found here.</p> <p>Besides the elements mentioned above, the scene contains two more, very important and complex components: <code>Environment</code> and <code>EgoVehicle</code> - described below.</p>"},{"location":"Introduction/AWSIM/#environment","title":"Environment","text":"<p><code>Environment</code> is a component that contains all <code>Visual Elements</code> that simulate the environment in the scene and those that provide control over them. It also contains two components <code>Directional Light</code> and <code>Volume</code>, which ensure suitable lighting for <code>Visual Elements</code> and simulate weather conditions. A detailed description of these components can be found here.</p> <p>In addition to <code>Visual Elements</code> such as buildings or greenery, it contains the entire architecture responsible for traffic. The traffic involves <code>NPCVehicles</code> that are spawned in the simulation by <code>TrafficSimulator</code> - using traffic components. A quick overview of the traffic components is provided below, however, you can read their detailed description here.</p> <p><code>NPCPedestrians</code> are also <code>Environment</code> components, but they are not controlled by <code>TrafficSimulator</code>. They have added scripts that control their movement - you can read more details here.</p>"},{"location":"Introduction/AWSIM/#traffic-components","title":"Traffic Components","text":"<p><code>TrafficLanes</code> and <code>StopLines</code> are elements loaded into <code>Environment</code> from Lanelet2. <code>TrafficLanes</code> have defined cross-references in such a way as to create routes along the traffic lanes. In addition, each <code>TrafficLane</code> present at the intersection has specific conditions for yielding priority. <code>TrafficSimulator</code> uses <code>TrafficLanes</code> to spawn <code>NPCVehicles</code> and ensure their movement along these lanes. If some <code>TrafficLanes</code> ends just before the intersection, then it has a reference to <code>StopLine</code>. Each <code>StopLine</code> at the intersection with <code>TrafficLights</code> has reference to the nearest <code>TrafficLight</code>. <code>TrafficLights</code> belong to one of the visual element groups and provide an interface to control visual elements that simulate traffic light sources (bulbs). A single <code>TrafficIntersection</code> is responsible for controlling all <code>TrafficLights</code> at one intersection. Detailed description of mentioned components is in this section.</p>"},{"location":"Introduction/AWSIM/#egovehicle","title":"EgoVehicle","text":"<p><code>EgoVehicle</code> is a component responsible for simulating an autonomous vehicle moving around the scene. It includes:</p> <ul> <li><code>Models</code> and <code>Reflection Probe</code> components related to its visual appearance.</li> <li><code>Colliders</code> providing collisions and the ability to move on roads.</li> <li><code>Sensors</code> providing data related to the state of the vehicle, including its position and speed in <code>Environment</code> and the state of its surroundings.</li> <li><code>Vehicle</code> component that simulates dynamics, controls <code>**Wheel</code> and is responsible for ensuring their movement.</li> <li><code>Vehicle Ros Input</code> and <code>Vehicle Keyboard Input</code>components that have a reference to the <code>Vehicle</code> object and set control commands in it.</li> <li><code>Vehicle Visual Effect</code> provides an interface for <code>Vehicle</code> to control the lighting.</li> </ul> <p>A detailed description of <code>EgoVehicle</code> and its components mentioned above can be found here. The sensor placement on <code>EgoVehicle</code> used in the default scene is described here. Details about each of the individual sensors are available in the following sections: <code>Pose</code>, <code>GNSS</code>, <code>LiDAR</code>, <code>IMU</code>, <code>Camera</code>, <code>Vehicle Status</code>.</p>"},{"location":"Introduction/AWSIM/#fixedupdate-limitation","title":"FixedUpdate Limitation","text":"<p>In AWSIM, the sensors' publishing methods are triggered from the <code>FixedUpdate</code> function and the output frequency is controlled by:</p> <pre><code>time += Time.deltaTime;\nvar interval = 1.0f / OutputHz;\ninterval -= 0.00001f; // Allow for accuracy errors.\nif (time &lt; interval)\n    return;\ntimer = 0;\n</code></pre> <p>Since this code runs within the FixedUpdate method, it's essential to note that <code>Time.deltaTime</code> is equal to <code>Fixed Timestep</code>, as stated in the Unity Time.deltaTime documentation. Consequently, with each invocation of FixedUpdate, the <code>time</code> variable in the sensor script will increment by a constant value of <code>Fixed Timestep</code>, independent of the actual passage of real-time. Additionally, as outlined in the Unity documentation, the <code>FixedUpdate</code> method might execute multiple times before the <code>Update</code> method is called, resulting in extremely small time intervals between successive FixedUpdate calls. The diagram below illustrates the mechanism of invoking the FixedUpdate event function.\"</p> <p></p> <p>During each frame (game tick) following actions are performed:</p> <ul> <li>first, the <code>Delta Time</code> is calculated as the difference between the current frame and the previous frame,</li> <li>next, the <code>Delta Time</code> is added to the <code>Time</code> (regular time),</li> <li>afterward, a check is made to determine how much <code>Fixed Time</code> (physics time) is behind the <code>Time</code>,</li> <li>if the difference between <code>Time</code> and <code>Fixed Time</code> is equal to or greater then <code>Fixed Timestep</code>, the <code>Fixed Update</code> event function is invoked,</li> <li>if <code>FixedUpdate</code> function were called, the <code>Fixed Timestep</code> is added to the <code>Fixed Time</code>,</li> <li>once again, a check is performed to assess how much <code>Fixed Time</code> is behind the <code>Time</code>,</li> <li>if necessary, the <code>FixedUpdate</code> function is called again,</li> <li>if the difference between the <code>Time</code> and the <code>Fixed Time</code> is smaller than the <code>Fixed Timestep</code>, the <code>Update</code> method is called, followed be scene rendering and other Unity event functions.</li> </ul> <p>As a consequence, this engine feature may result in unexpected behavior when FPS (Frames Per Second) are unstable or under certain combinations of FPS, Fixed Timestep, and sensor OutputHz</p> <p>In case of low frame rates, it is advisable to reduce the <code>Time Scale</code> of the simulation. The Time Scale value impacts simulation time, which refers to the time that is simulated within the model and might or might not progress at the same rate as real-time. Therefore, by reducing the time scale, the progression of simulation time slows down, allowing the simulation more time to perform its tasks.</p>"},{"location":"Introduction/Autoware/","title":"Autoware","text":""},{"location":"Introduction/Autoware/#autoware","title":"Autoware","text":"<p>Autoware is an open-source software platform specifically designed for autonomous driving applications. It was created to provide a comprehensive framework for developing and testing autonomous vehicle systems. Autoware offers a collection of modules and libraries that assist in various tasks related to perception, planning, and control, making it easier for researchers and developers to build autonomous driving systems.</p> <p>The primary purpose of Autoware is to enable the development of self-driving technologies by providing a robust and flexible platform. It aims to accelerate the research and deployment of autonomous vehicles by offering a ready-to-use software stack. Autoware focuses on urban driving scenarios and supports various sensors such as LiDAR, Radars, and Cameras, allowing for perception of the vehicle's surroundings.</p>"},{"location":"Introduction/Autoware/#why-use-awsim-with-autoware","title":"Why use AWSIM with Autoware?","text":"<p>Autoware can be used with a AWSIM for several reasons. Firstly, simulators like AWSIM provide a cost-effective and safe environment for testing and validating autonomous driving algorithms before deploying them on real vehicles. Autoware's integration with a simulator allows developers to evaluate and fine-tune their algorithms without the risk of real-world accidents or damage.</p> <p>Additionally, simulators enable developers to recreate complex driving scenarios, including difficult conditions or rare events, which may be difficult to replicate in real-world testing with such high fidelity. Autoware's compatibility with a AWSIM allows seamless integration between the software and the simulated vehicle, enabling comprehensive testing and validation of autonomous driving capabilities. By utilizing a simulator, Autoware can be extensively tested under various scenarios to ensure its robustness and reliability.</p> <p>Connection with Autoware</p> <p>Introduction about how the connection between AWSIM and Autoware works can be read here.</p>"},{"location":"Introduction/Autoware/#architecture","title":"Architecture","text":"<p>In terms of architecture, Autoware follows a modular approach. It consists of multiple independent modules that communicate with each other through a ROS2. This modular structure allowing users to select and combine different modules based on their specific needs and requirements. The software stack comprises multiple components, including perception, localization, planning, and control modules. Here's a brief overview of each module:</p> <ul> <li>Sensing -  acquires data from sensors different sensors mounted on the autonomous vehicle such as LiDARs, GNSS, IMU and cameras. It pre-processing received data in order to later extract relevant information about the surrounding environment through the Perception module or about vehicle location by the Localization module. More details here.</li> </ul> <ul> <li>Perception - performs advanced processing of sensor data (LiDARs, cameras) to extract meaningful information about the surrounding environment. It performs tasks like object detection (other vehicles, pedestrians), lane detection, and traffic lights recognition. More details here.</li> </ul> <ul> <li>Localization - performs a fusion of data from Sensing module like LiDAR, GNSS, IMU, and odometry sensors to estimate the vehicle's position and orientation accurately. More details here.</li> </ul> <ul> <li>Planning - generates a safe and feasible trajectory for the autonomous vehicle based on the information gathered from Perception and Localization. It also takes into account various factors from Map like traffic rules and road conditions. More details here.</li> </ul> <ul> <li>Control - executes the planned trajectory by sending commands to the vehicle's actuators, such as steering, throttle, and braking. It ensures that the vehicle follows the desired trajectory while maintaining safety and stability. More details here.</li> </ul> <ul> <li>Vehicle Interface - is a crucial component that enables communication and interaction between Autoware software system and a vehicle. It facilitates the exchange of control signals and vehicle information necessary for autonomous driving operations. The vehicle interface ensures that Autoware can send commands to the vehicle, such as acceleration, braking, and steering, while also receiving real-time data from the vehicle, such as speed, position, and sensors data. It acts as a bridge, allowing Autoware to seamlessly interface with the specific characteristics and requirements of the vehicle it is operating with. More details here.</li> </ul> <ul> <li>Map - the map module creates and maintains  a representation of the environment in which the autonomous vehicle operates. It combines data from Lanelet2 (<code>*.osm</code>) and PointCloud (<code>*.pcd</code>) to generate a detailed map. The map contains information about road geometries, lane markings, traffic lights, rules, and other relevant features. Map serves as a crucial reference for planning and decision-making processes. More details here.</li> </ul>"},{"location":"Introduction/CombinationWithAutoware/","title":"CombinationWithAutoware","text":"<p>Autoware is a powerful open-source software platform for autonomous driving. Its modular architecture, including perception, localization, planning, and control modules, provides a comprehensive framework for developing self-driving vehicles. Autoware combined with  AWSIM simulator provides safe testing, validation, and optimization of autonomous driving algorithms in diverse scenarios.</p> <p>Run with Autoware</p> <p>If you would like to know how to run AWSIM with Autoware, we encourage you to read this section.</p>"},{"location":"Introduction/CombinationWithAutoware/#features","title":"Features","text":"<p>The combination of Autoware and AWSIM provides the opportunity to check the correctness of the vehicle's behavior in various traffic situations. Below are presented some typical features provided by this combination. Moreover, examples of detecting several bad behaviors are included.</p>"},{"location":"Introduction/CombinationWithAutoware/#engagement","title":"Engagement","text":"<ul> <li> <p>Driving straight through an intersection with priority</p> <p> </p> </li> </ul> <ul> <li> <p>Turning at the intersection</p> <p> </p> </li> </ul>"},{"location":"Introduction/CombinationWithAutoware/#traffic-light-recognition","title":"Traffic light recognition","text":"<ul> <li> <p>Stopping at a red light</p> <p> </p> </li> </ul> <ul> <li> <p>Driving on a green light</p> <p> </p> </li> </ul> <ul> <li> <p>Stopping at yellow light</p> <p> </p> </li> </ul> <ul> <li> <p>Still driving at yellow light (only when it is too late to stop)</p> <p> </p> </li> </ul>"},{"location":"Introduction/CombinationWithAutoware/#interaction-with-vehicles","title":"Interaction with vehicles","text":"<ul> <li> <p>Yield right-of-way when turning right</p> <p> </p> </li> </ul> <ul> <li> <p>Following the vehicles ahead</p> <p> </p> </li> </ul> <ul> <li> <p>Stopping behind the vehicles ahead</p> <p> </p> </li> </ul> <ul> <li> <p>Cutting-in to a different traffic lane</p> <p> </p> </li> </ul>"},{"location":"Introduction/CombinationWithAutoware/#interaction-with-pedestrians","title":"Interaction with pedestrians","text":"<ul> <li> <p>Giving right of way to a pedestrian crossing at a red light</p> <p> </p> </li> </ul> <ul> <li> <p>Giving way to a pedestrian crossing beyond a crosswalk</p> <p> </p> </li> </ul>"},{"location":"Introduction/CombinationWithAutoware/#detecting-bad-behaviors","title":"Detecting bad behaviors","text":"<ul> <li> <p>Incorrect and dangerous execution of a lane change</p> <p> </p> </li> </ul> <ul> <li> <p>Too late detection of a pedestrian entering the roadway</p> <p> </p> </li> </ul>"},{"location":"Introduction/CombinationWithAutoware/#combination-architecture","title":"Combination Architecture","text":"<p>The combination of AWSIM with Autoware is possible thanks to Vehicle Interface and Sensing modules of Autoware architecture. The component responsible for ensuring connection with these modules from the AWSIM side is <code>EgoVehicle</code>. It has been adapted to the Autoware architecture and provides ROS2 topic-based communication. However, the other essential component is <code>ClockPublisher</code>, which provides simulation time for Autoware - also published on the topic - more details here.</p> <p><code>EgoVehicle</code> component provides the publication of the current vehicle status through a script working within <code>Vehicle Status</code>. It provides real-time information such as: current speed, current steering of the wheels or current states of lights - these are outputs from AWSIM.</p> <p>On the other hand, <code>Vehicle Ros Input</code> is responsible for providing the values of the outputs from Autoware. It subscribes to the current commands related to the given acceleration, gearbox gear or control of the specified lights.  </p> <p>Execution of the received commands is possible thanks to <code>Vehicle</code>, which ensures the setting of appropriate accelerations on the <code>**Wheel</code> and controlling the visual elements of the vehicle.</p> <p>The remaining data delivered from AWSIM to Autoware are sensors data, which provides information about the current state of the surrounding environment and those necessary to accurately estimate <code>EgoVehicle</code> position.</p> <p>More about <code>EgoVehicle</code> and its scripts is described in this section.</p>"},{"location":"Introduction/CombinationWithAutoware/#sequence-diagram","title":"Sequence diagram","text":"<p>Below is a simplified sequential diagram of information exchange in connection between AWSIM and Autoware. As you can see, the first essential information published from AWSIM is <code>Clock</code> - the simulation time. Next, <code>EgoVehicle</code> is spawned and first sensors data are published, which are used in the process of automatic position initialization on Autoware side. At the same time, the simulation on AWSIM side is updated.</p> <p>Next in the diagram is the main information update loop in which:</p> <ul> <li>During each cycle there is a synchronization with the time from the simulation.</li> <li>AWSIM publishes data from sensors available in <code>EgoVehicle</code>, which are taken into account in the processes carried out in Autoware.</li> <li>The control commands from Autoware are subscribed by AWSIM, which are executed on AWSIM side and <code>EgoVehicle</code> update is performed.</li> <li>The current state of the <code>EgoVehicle</code> is published.</li> </ul> <p>The order of information exchange presented in the diagram is a simplification. The exchange of information takes place through the publish-subscribe model and each data is sent with a predefined frequency.</p> <p></p>"},{"location":"Introduction/CombinationWithAutowareAndScenarioSimulator/","title":"Combination with Autoware and Scenario simulator v2","text":""},{"location":"Introduction/CombinationWithAutowareAndScenarioSimulator/#combination-with-autoware-and-scenario-simulator-v2","title":"Combination with Autoware and Scenario simulator v2","text":"<p>Scenario Simulator v2 (SS2) is a scenario testing framework specifically developed for Autoware, an open-source self-driving software platform. It serves as a tool for Autoware developers to conveniently create and execute scenarios across different simulators.</p> <p>The primary goal of SS2 is to provide Autoware developers with an efficient means of writing scenarios once and then executing them in multiple simulators. By offering support for different simulators and scenario description formats, the framework ensures flexibility and compatibility.</p> <p>The default scenario format in this framework is TIER IV Scenario Format version 2.0. The scenario defined on this format is converted by <code>scenario_test_runner</code> to <code>openSCENARIO</code> format, which is then interpreted by <code>openscenario_interpreter</code>. Based on this interpretation, <code>traffic_simulator</code> simulates traffic flow in an urban area. Each NPC has a behavior tree and executes commands from the scenario.</p> <p>The framework uses ZeroMQ Inter-Process communication for seamless interaction between the simulator and the <code>traffic_simulator</code>. To ensure synchronous operation of the simulators, SS2 utilizes the Request/Reply sockets provided by ZeroMQ and exchanges binarized data through Protocol Buffers. This enables the simulators to run in a synchronized manner, enhancing the accuracy and reliability of scenario testing.</p> <p>QuickStart Scenario simulator v2 with Autoware</p> <p>If you would like to see how SS2 works with Autoware using default build-in simulator - <code>simple_sensor_simulator</code> (without running AWSIM) - we encourage you to read this tutorial.</p>"},{"location":"Introduction/CombinationWithAutowareAndScenarioSimulator/#combination-architecture","title":"Combination Architecture","text":"<p>AWSIM scene architecture used in combination with SS2 changes considerably compared to the default scene. Here <code>traffic_simulator</code> from SS2 replaces <code>TrafficSimulator</code> implementation in AWSIM - for this reason it and its <code>StopLines</code>, <code>TrafficLanes</code> and <code>TrafficIntersection</code> components are removed. Also, <code>NPCPedestrian</code> and <code>NPCVehicles</code> are not added as aggregators of NPCs in <code>Environment</code>.</p> <p>Instead, their counterparts are added in <code>ScenarioSimulatorConnector</code> object that is responsible for spawning <code>Entities</code> of the scenario. <code>Entity</code> can be: <code>Pedestrian</code>, <code>Vehicle</code>, <code>MiscObject</code> and <code>Ego</code>. <code>EgoEntity</code> is the equivalent of <code>EgoVehicle</code> - which is also removed from the default scene. However, it has the same components - it still communicates with Autoware as described here. So it can be considered that <code>EgoVehicle</code> has not changed and <code>NPCPedestrians</code> and <code>NPCVehicles</code> are now controlled directly by the SS2.</p> <p>A detailed description of the SS2 architecture is available here. A description of the communication via ROS2 between SS2 and Autoware can be found here.</p>"},{"location":"Introduction/CombinationWithAutowareAndScenarioSimulator/#sequence-diagram","title":"Sequence diagram","text":"<p>In the sequence diagram, the part responsible for AWSIM communication with Autoware also remained unchanged. The description available here is the valid description of the reference shown in the diagram below.</p> <p>Communication between SS2 and AWSIM takes place via Request-Response messages, and is as follows:</p> <ol> <li>Launch - Autoware is started and initialized.</li> <li>Initialize - the environment in AWSIM is initialized, basic parameters are set.</li> <li>opt Ego spawn - optional, <code>EgoEntity</code> (with sensors) is spawned in the configuration defined in the scenario.</li> <li>opt NPC spawn loop - optional, all <code>Entities</code> (NPCs) defined in the scenario are spawned, the scenario may contain any number of each <code>Entity</code> type, it may not contain them at all or it may also be any combination of the available ones.</li> <li>update loop - this is the main loop where scenario commands are executed, first <code>EgoEntity</code> is updated - SS2 gets its status, and then every other <code>Entity</code> is updated - the status of each NPCs is set according to the scenario. Next, the simulation frame is updated - here the communication between Autoware and AWSIM takes place. The last step of the loop is to update the traffic light state.</li> <li>despawn loop - after the end of the scenario, all <code>Entities</code> spawned on the scene are despawned (including <code>EgoEnity</code>)</li> <li>Terminate - Autoware is terminated.</li> </ol> <p>Documentation of the commands used in the sequence is available here.</p> <p></p>"},{"location":"ProjectGuide/Directory/","title":"Directory","text":""},{"location":"ProjectGuide/Directory/#directory","title":"Directory","text":"<p>AWSIM has the following directory structure. Mostly they are grouped by file type.</p> <pre><code>AWSIM       //  root directory.\n \u2502\n \u2502\n \u251c\u2500Assets                           // Unity project Assets directory.\n \u2502  \u2502                               // Place external libraries\n \u2502  \u2502                               // under this directory.\n \u2502  \u2502                               // (e.g. RGLUnityPlugin, ROS2ForUnity, etc..)\n \u2502  \u2502\n \u2502  \u2502\n \u2502  \u251c\u2500AWSIM                         // Includes assets directly related to AWSIM\n \u2502  |                               // (Scripts, Prefabs etc.)\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500Externals                  // Place for large files or\n \u2502  \u2502  |                            // external project dependencies\n \u2502  \u2502  |                            // (e.g. Ninshinjuku map asset).\n \u2502  \u2502  \u2502                            // The directory is added to `.gitignore`\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500HDRPDefaultResources       // Unity HDRP default assets.\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500Materials                  // Materials used commonly in Project.\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500Models                     // 3D models\n \u2502  \u2502  \u2502  \u2502                         // Textures and materials for 3D models\n \u2502  \u2502  \u2502  \u2502                         // are also included.\n \u2502  \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502  \u2514\u2500&lt;3D Model&gt;              // Directory of each 3D model.\n \u2502  \u2502  \u2502     \u2502\n \u2502  \u2502  \u2502     \u2502\n \u2502  \u2502  \u2502     \u251c\u2500Materials            // Materials used in 3D model.\n \u2502  \u2502  \u2502     \u2502\n \u2502  \u2502  \u2502     \u2502\n \u2502  \u2502  \u2502     \u2514\u2500Textures             // Textures used in 3D model.\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500Prefabs                    // Prefabs not dependent on a specific scene.\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u251c\u2500Scenes                     // Scenes\n \u2502  \u2502  \u2502  \u2502                         // Includes scene-specific scripts, etc.\n \u2502  \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502  \u251c\u2500Main                    // Scenes used in the simulation.\n \u2502  \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502  \u2514\u2500Samples                 // Sample Scenes showcasing components.\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u2502\n \u2502  \u2502  \u2514\u2500Scripts                    // C# scripts.\n \u2502  \u2502\n \u2502  \u2502\n \u2502  \u251c\u2500RGLUnityPlugin        // Robotec GPU LiDAR external Library.\n \u2502  \u2502                       // see: https://github.com/RobotecAI/RobotecGPULidar\n \u2502  \u2502\n \u2502  \u2502\n \u2502  \u2514\u2500Ros2ForUnity          // ROS2 communication external Library.\n \u2502                          // see: https://github.com/RobotecAI/ros2-for-unity\n \u2502\n \u251c\u2500Packages         // Unity automatically generated directories.\n \u251c\u2500ProjectSettings  //\n \u251c\u2500UserSettings     //\n \u2502\n \u2502\n \u2514\u2500docs             // AWSIM documentation. Generated using mkdocs.\n                    // see: https://www.mkdocs.org/\n</code></pre>"},{"location":"ProjectGuide/ExternalLibraries/","title":"External Libraries","text":""},{"location":"ProjectGuide/ExternalLibraries/#external-libraries","title":"External Libraries","text":"<p>List of external libraries used in AWSIM.</p> Library Usage URL ros2-for-unity ROS2 communication https://github.com/RobotecAI/ros2-for-unity Robtoec-GPU-LiDAR LiDAR simulation https://github.com/RobotecAI/RobotecGPULidar"},{"location":"ProjectGuide/GitBranch/","title":"Git Branch","text":""},{"location":"ProjectGuide/GitBranch/#git-branch","title":"Git branch","text":"<p>The document presents the rules of branching adopted in the AWSIM development process.</p>"},{"location":"ProjectGuide/GitBranch/#branches","title":"Branches","text":"branch explain main Stable branch. Contains all the latest releases. feature/*** Feature implementation branch created from <code>main</code>. After implementation, it is merged into <code>main</code>. gh-pages Documentation hosted on GitHub pages."},{"location":"ProjectGuide/GitBranch/#branch-flow","title":"Branch flow","text":"<ol> <li>Create <code>feature/***</code> branch from <code>main</code>.</li> <li>Implement in <code>feature/***</code> branch.</li> <li>Create a PR from the <code>feature/***</code> branch to <code>main</code> branch. Merge after review.</li> </ol>"},{"location":"ProjectGuide/HotkeyList/","title":"Hotkey List","text":""},{"location":"ProjectGuide/HotkeyList/#hotkey-list","title":"Hotkey List","text":""},{"location":"ProjectGuide/HotkeyList/#vehiclekeyboardinputcs","title":"VehicleKeyboardInput.cs","text":"Key Feature D Change drive gear. P Parking gear. R Reverse gear. N Neutral gear. 1 Left turn signal. 2 Right turn signal. 3 Hazard. 4 Turn signal off. Up arrow Accelerate. Left arrow Steering (Left). Right arrow Steering (Right). Down arrow Breaking. <p>W,A,S,D keys can also be used to control the vehicle, similar to the arrow keys.</p>"},{"location":"ProjectGuide/HotkeyList/#followcameracs","title":"FollowCamera.cs","text":"Key Feature C Camera rotation on/off toggle. Mouse drag Rotate Camera angle. Mouse wheel Zoom in/out of camera."},{"location":"ProjectGuide/HotkeyList/#hotkeyhandlercs","title":"HotkeyHandler.cs","text":"Key Feature Esc Toggle side bar LeftCtrl + R Reset ego vehicle LeftCtrl + T Toggle traffic visibility"},{"location":"ProjectGuide/HotkeyList/#birdeyeviewcs","title":"BirdEyeView.cs","text":"Key Feature B Toggle BEV camera WASD / Arrow Keys Pan BEV camera Mouse Wheel + Drag Pan BEV camera Mouse Wheel Zoom in/out Space Snap camera to vehicle LeftCtrl + Space Follow ego vehicle on/off"},{"location":"ProjectGuide/HotkeyList/#graphy-asset","title":"Graphy Asset","text":"Key Feature F3 Toggle Graphy Visualizer"},{"location":"ProjectGuide/Scenes/","title":"Scenes","text":"<p>In the AWSIM Unity project there is one main scene (AutowareSimulation) and several additional ones that can be helpful during development. This section describes the purpose of each scene in the project.</p> <p> </p>"},{"location":"ProjectGuide/Scenes/#autowaresimulation","title":"AutowareSimulation","text":"<p>The <code>AutowareSimulation</code> scene is the main scene that is designed to run the AWSIM simulation together with Autoware. It allows for effortless operation, just run this scene, run Autoware with the correct map file and everything should work right out of the box.</p> <p></p>"},{"location":"ProjectGuide/Scenes/#pointcloudmapping","title":"PointCloudMapping","text":"<p>The <code>PointCloudMapping</code> is a scene that is designed to create a point cloud using the Unity world. Using the RGLUnityPlugin and prefab <code>Environment</code> - on which there are models with <code>Meshes</code> - we are able to obtain a <code>*.pcd</code> file of the simulated world.</p>"},{"location":"ProjectGuide/Scenes/#sensorconfig","title":"SensorConfig","text":"<p>Scene <code>SensorConfig</code> was developed to perform a quick test of sensors added to the <code>EgoVehicle</code> prefab. Replace the <code>Lexus</code> prefab with a vehicle prefab you developed and check whether all data that should be published is present, whether it is on the appropriate topics and whether the data is correct.</p>"},{"location":"ProjectGuide/Scenes/#npcvehiclesample","title":"NPCVehicleSample","text":"<p>The <code>NPCVehicleSample</code> was developed to conduct a quick test of the developed vehicle. Replace the taxi prefab with a vehicle prefab you developed (<code>EgoVehicle</code> or <code>NPCVehicle</code>) and check whether the basic things are configured correctly. The description of how to develop your own vehicle and add it to the project is in this section.</p>"},{"location":"ProjectGuide/Scenes/#npcpedestriansample","title":"NPCPedestrianSample","text":"<p>The <code>NPCPedestrianSample</code> was developed to conduct a quick test of the developed pedestrian. Replace the NPC prefab in <code>NPC Pedestrian Test</code> script with a prefab you developed and check whether the basic things are configured correctly.</p>"},{"location":"ProjectGuide/Scenes/#trafficintersectionsample","title":"TrafficIntersectionSample","text":"<p>The <code>TrafficIntersectionSample</code> was developed to conduct a quick test of the developed traffic intersection. Replace the intersection configuration with your own and check whether it works correctly. You can add additional groups of lights and create much larger, more complex sequences. A description of how to configure your own traffic intersection is in this section.</p>"},{"location":"ProjectGuide/Scenes/#trafficlightsample","title":"TrafficLightSample","text":"<p>The <code>TrafficLightSample</code> was developed to conduct a quick test of a developed traffic lights model in cooperation with the script controlling it. Replace the lights and configuration with your own and check whether it works correctly.</p>"},{"location":"ProjectGuide/Scenes/#randomtrafficyielding","title":"RandomTrafficYielding","text":"<p>The <code>RandomTrafficYielding</code> was developed to conduct a tests of a developed yielding rules at the single intersection.</p>"},{"location":"ProjectGuide/Scenes/#randomtrafficyieldingbirdeye","title":"RandomTrafficYieldingBirdEye","text":"<p>The <code>RandomTrafficYielding</code> was developed to conduct a tests of a developed yielding rules with multiple vehicles moving around the entire environment. </p>"},{"location":"ProjectGuide/Scenes/#rgl-test-scenes","title":"RGL test scenes","text":"<p>The scenes described below are used for tests related to the external library <code>RGLUnityPlugin</code> (<code>RGL</code>) - you can read more about it in this section.</p>"},{"location":"ProjectGuide/Scenes/#lidarscenedevelop","title":"LidarSceneDevelop","text":"<p>The scene <code>LidarSceneDevelop</code> can be used as a complete, minimalistic example of how to setup <code>RGL</code>. It contains <code>RGLSceneManager</code> component, four lidars, and an environment composed of floor and walls.</p>"},{"location":"ProjectGuide/Scenes/#lidarskinnedstress","title":"LidarSkinnedStress","text":"<p>The scene <code>LidarSkinnedStress</code> can be used to test the performance of <code>RGL</code>. E.g. how performance is affected when using <code>Regular Meshes</code> compared to <code>Skinned Meshes</code>. The scene contains a large number of animated models that require meshes to be updated every frame, thus requiring more resources (CPU and data exchange with GPU).</p>"},{"location":"ProjectGuide/Scenes/#lidardisablingtest","title":"LidarDisablingTest","text":"<p>The scene <code>LidarDisablingTest</code> can be used to test <code>RGL</code> performance with similar objects but with different configurations. It allows you to check whether <code>RGL</code> works correctly when various components that can be sources of <code>Meshes</code> are disabled (<code>Colliders</code>, <code>Regular Meshes</code>, <code>Skinned Meshes</code>, ...).</p> <p></p>"},{"location":"ProjectGuide/Scenes/#lidarinstancesegmentationdemo","title":"LidarInstanceSegmentationDemo","text":"<p>The <code>LidarInstanceSegmentationDemo</code> is a demo scene for instance segmentation feature. It contains a set of GameObjects with ID assigned and sample lidar that publishes output to the ROS2 topic. The GameObjects are grouped to present different methods to assign IDs.</p> <p></p> <p>To run demo scene:</p> <ol> <li>Open scene: <code>Assets/AWSIM/Scenes/Samples/LidarInstanceSegmentationDemo.unity</code></li> <li>Run simulation</li> <li>Open <code>rviz2</code></li> <li>Setup <code>rviz2</code> as follows:<ul> <li>Fixed frame: <code>world</code>,</li> <li>PointCloud2 topic: <code>lidar/instance_id</code>,</li> <li>Topic <code>QoS</code> as in the screen above.</li> <li>Channel name: <code>enitity_id</code>,</li> <li>To better visualization disable <code>Autocompute</code> intensity and set min to <code>0</code> and max to <code>50</code>.</li> </ul> </li> </ol>"}]}